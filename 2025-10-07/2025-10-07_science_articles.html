
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - SCIENCE Article Summaries - 2025-10-07</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            SCIENCE
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/health/a68884291/immortality-world-leaders-1759857373/'>Vladimir Putin Is Seeking Immortality—And Believes He's Closer Than Ever to Achieving It</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 17:29:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Did the world's most notorious rulers just unlock eternal life? For some communist and authoritarian leaders, “term limits” are more or less dictated by how long they can manage to live. But what if these politicians weren't bound by the constraints of a typical human lifespan? A recent conversation caught on a hot mic between Russian President Vladimir Putin and Chinese leader Xi Jinping suggests these infamous rulers really could achieve immortality—and that it could be right around the corner. “Biotechnology is continuously developing,” Putin's translator said in Chinese at a military parade held in Beijing, China, earlier this week. He later continued, “Human organs can be continuously transplanted. Xi responded, “Some predict that in this century humans may live to 150 years old.” North Korea's Supreme Leader Kim Jong Un accompanied Putin and Xi at the parade, though it's unclear whether the conversation was being translated for him as well. So, is anything stopping these notorious figures from ruling forever? While Putin and Xi make it seem like achieving immortality is closer than ever, the science suggests otherwise. For the same reason, 13 patients on the transplant waitlist die each day on average, according to the U.S. Health Resources & Services Administration, so it's hard to imagine further straining an already overwhelmed system. Despite the transplant pipeline's difficulties, longevity enthusiasts are still optimistic, and conducting new research on how organ transplants—even unconventional ones—could extend the average lifespan beyond what was ever before thought possible. Five years ago, for instance, a team from the Yale School of Medicine managed to revive a slaughterhouse pig's brain after it was deprived of oxygen for four hours. Miraculously, the brain cells began producing proteins, and the neurons displayed signs of metabolic activity, both of which are basic cellular functions that make your body's control center run. Still other researchers are avoiding the need to find organs at all—dead or alive—by making their own. Still, growing spare parts remains somewhat of a sci-fi dream. Ironically, both Putin and Xi are at the ripe age of 72. So it makes sense why the two leaders would begin obsessing over what comes next—even if that means experimental transplant treatments to extend their lives. However, it seems immortality is just out of reach for even the world's most powerful figures. Emma Frederickson graduated from Pace University where she studied communication and media. Prior to her time as an editor, she was a freelance science reporter. She enjoys covering everything from shipwrecks to pimple popping, but her favorite topics include climate change, conspiracy theories, and weird biology. When she's not writing, Emma can be found hopping between coffee shops on the hunt for the world's best oat milk cappuccino.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/archaeology/a68251733/ancient-statue-rare-treasure/'>This Ancient Statue Was Discarded by Raiders. Scientists Think It's a Rare Treasure.</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 12:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The familial statue mixed together different styles of craftsmanship in a way never before seen in ancient Egyptian culture. Archaeologists discovered a 4,300-year-old limestone sculpture may offer unique insight into the artistic history of ancient Egypt. Known as the “family statue,” it was likely initially found by looters centuries ago, only to be tossed aside and eventually buried under layers of sand. It was rediscovered in 2021 during excavations at Gisr El-Mudir in Saqqara, and now, famed archaeologists Zahi Hawass and Sarah Abdoh have analyzed the find, publishing a study on the piece in The Journal of Egyptian Archaeology. That study illustrated for the first time just how crucial the statue could be to understanding ancient Egyptian craftsmanship. Hawass wrote on his website that the Gisr El-Mudir family statue offers a singular example of craftsmanship from the Old Kingdom's fifth dynasty. “Unlike any other extant family statue, all members, except the daughter, are rendered in full three-dimensionality,” he wrote. Adding an extra layer of intrigue to the statue, it wasn't found in a tomb or a chapel, the study authors wrote. This element adds a domestic aspect to the statue that coincides with what's regularly seen drawn on tomb walls of the period. Hawass wrote that the statue offers a fresh perspective on sculpture styling in ancient Egypt—one that requires a new look at how Old Kingdom artisans crafted their work. This once-discarded family portrait might just reshape our understanding of art in the ancient world. Tim Newcomb is a journalist based in the Pacific Northwest. He covers stadiums, sneakers, gear, infrastructure, and more for a variety of publications, including Popular Mechanics. A Metal Detectorist Just Found ‘Devil's Money' This Ancient Furnace May Have Started the Iron Age A Metal Detectorist Stumbled Upon Ancient Coins</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/south-africas-coast-is-rising-and-scientists-have-a-new-explanation-why/'>South Africa's Coast Is Rising—And Scientists Have a New Explanation Why</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 10:45:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>South Africa's Coast Is Rising—And Scientists Have a New Explanation Why Human water management contributes to sinking land across the globe, and it may also be responsible for an unexpected rise Land rising along South Africa's coast may be closely tied to humans' use of water. For decades geologists thought the slow rise of South Africa's southern coast was driven by forces deep below—buoyant plumes of molten rock ascending through Earth's mantle and heaving the crust upward over millions of years. But now satellite data and precise GPS measurements are tilting such assumptions off their axis. A study in the Journal of Geophysical Research: Solid Earth suggests this land rise may have less to do with deep tectonic forces and more to do with missing groundwater just under our feet. Human activity has long been depleting South Africa's groundwater. In 2018, after grappling with severe droughts for years, the country came close to a full-blown water emergency when Cape Town was nearly the world's first major city to literally run out of water—a scenario dubbed “Day Zero.” For several months that year the city's residents faced the very real prospect of having to regularly queue for critically limited water supplies, an outcome staved off only by timely rainfall and intensive water-saving campaigns. The extreme shortage resulted from a combination of climate change and unsustainable water use, which drained surface reservoirs and placed mounting pressure on aquifers across the region. Using GPS and satellite gravity data from between 2000 and 2021, the researchers detected a roughly six-millimeter rise in the land surface—a shift that coincides with humans' depletion of South Africa's water reserves and periods of drought. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. What was once chalked up to the slow churning of Earth's mysterious and inaccessible interior may instead reflect human activity, especially our management—or mismanagement—of water. If molten rock were pushing upward, the motion would be steady, not tied to rainfall cycles. From 1945 to 1970 more than 13,000 square kilometers of California's San Joaquin Valley, once hailed as a “land of milk and honey” for Dust Bowl migrants, sank by at least 30 centimeters—and in some places by nearly nine meters. The San Joaquin sinking has only sped up since then, and parts of the valley drop more than 30 centimeters a year during severe droughts. Something similar is happening to the Chesapeake Bay, which, with its sweeping estuaries and lush tidal wetlands, is one of the U.S. East Coast's most ecologically significant regions. Here land subsidence—driven by both groundwater extraction from aquifers and the lingering effects of ancient glacial shifts—is accelerating flood risk and relative sea-level rise. Satellite data, tide gauge records and projections from the Intergovernmental Panel on Climate Change suggest that by 2100 the combination of subsidence and sea-level rise could inundate up to 1,100 square kilometers of the Chesapeake Bay's coastline. As climate change accelerates, land movements could exacerbate other challenges, especially in coastal areas with rising seas. To monitor such hidden shifts on a global scale, scientists use the GRACE satellite mission (Gravity Recovery and Climate Experiment) to detect changes in Earth's mass by measuring minuscule variations in gravity. Because water has weight, depleting or replenishing groundwater subtly alters the planet's gravitational field, which GRACE can detect from orbit. Knight and other researchers are looking for ways to keep land from shifting on such a vast scale by maintaining a careful balance. “And for water in, the term that's used is ‘recharge. Some recharge happens naturally as rain or snowmelt soaks into the soil, but this precipitation isn't enough to offset decades of groundwater extraction and current demand. That's why places such as California are now turning to managed aquifer recharge: strategically spreading excess surface water (such as winter floodwaters) across land where it can percolate into the ground and rebuild depleted reserves, or injecting water directly into aquifers. Estimates suggest there is space underground for a total amount of water 30 times the volume of California's Shasta Lake, enough to begin reversing the land's descent. As Knight puts it, the solution can't be about just cutting back on groundwater pumping. Avery Schuyler Nunn is an avid surfer, free diver and environmental science journalist based in California. She is a freelance contributor to Scientific American, National Geographic, Smithsonian Magazine, Grist, and more. If you enjoyed this article, I'd like to ask for your support. Scientific American has served as an advocate for science and industry for 180 years, and right now may be the most critical moment in that two-century history. If you subscribe to Scientific American, you help ensure that our coverage is centered on meaningful research and discovery; that we have the resources to report on the decisions that threaten labs across the U.S.; and that we support both budding and working scientists at a time when the value of science itself too often goes unrecognized. In return, you get essential news, captivating podcasts, brilliant infographics, can't-miss newsletters, must-watch videos, challenging games, and the science world's best writing and reporting. There has never been a more important time for us to stand up and show why science matters.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s42255-025-01381-z'>Effect of sweeteners and sweetness enhancers on weight management and gut microbiota composition in individuals with overweight or obesity: the SWEET study</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 10:33:21
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Consumption of sweeteners and sweetness enhancers (S&SEs) is a popular strategy to reduce sugar intake, but the role of S&SEs in body weight regulation and gut microbiota composition remains debated. Here, we show that S&SEs in a healthy diet support weight loss maintenance and beneficial gut microbiota shifts in adults with overweight or obesity. In this multi-centre, randomized, controlled trial, we included 341 adults and 38 children with overweight or obesity. Adults followed a 2-month low-energy diet for ≥5% weight loss, followed by a 10-month healthy ad libitum diet with <10% energy from sugars. Primary outcomes included changes in body weight and gut microbiota composition at 1 year. Secondary outcomes included changes in cardiometabolic parameters. The S&SEs group, compared to the sugar group, maintained greater weight loss at 1 year (1.6 ± 0.7 kg, P = 0.029) and exhibited distinct gut microbiota shifts, with increased short-chain fatty acid and methane-producing taxa (q ≤ 0.05). Overall, our findings indicate that prolonged consumption of S&SEs in a healthy diet is a safe strategy for obesity management. The prevalence of overweight and obesity has increased globally, raising the risk of non-communicable diseases such as type 2 diabetes (T2D) and cardiovascular disease (CVD)1,2. A shift towards a Westernized diet (high in saturated fat and added sugar, low in dietary fibre) has been proposed to be a key contributor to the development of obesity-related cardiometabolic complications3. Added sugar, in particular, increases dietary energy density, which may lead to greater energy intake and the development of obesity. In 2015, the World Health Organization (WHO) strongly recommended that free sugar intake should be less than 10% of total energy intake (E%) and preferably even less than 5 E% as a conditional recommendation4. One common strategy to reduce sugar intake is replacing it with S&SEs. Consequently, the worldwide consumption of foods and beverages containing S&SEs has substantially increased over recent years10. Although S&SEs are generally considered safe, their long-term effects on cardiometabolic health remain debated. Cohort studies have raised concerns about potential risks, prompting the WHO to issue a conditional recommendation against using non-sugar sweeteners for weight control or reducing the risk of non-communicable diseases4,11,12. Nevertheless, observational evidence is not in line with data from short-term studies13,14, and the limited number of long-term randomized controlled trials (RCTs) have shown neutral or beneficial effects, including modest weight loss and no negative impact on T2D or CVD risk markers13,14. Another emerging concern is the potential impact of S&SEs on gut microbiota composition. Some studies suggest that S&SEs may alter the gut microbiota, potentially affecting metabolic health13. A previous study14 demonstrated a link between saccharin-induced alterations in the gut microbiota and glucose intolerance in mice. In addition, in a small post hoc human trial, they showed that supplementation of saccharin (5 mg kg−1 day−1) for 1 week increased glycaemic response, which was associated with microbiota alterations in a small group of study participants clustered as ‘responders', while no response was found in the other participants (‘non-responders')14. The poor glycaemic response in the ‘responders' was replicated in mice upon faecal transplantation. Interestingly, the microbiota composition of responders was distinct before saccharin exposure, suggesting individual variability in response to S&SEs and the potential for the gut microbiome to predict susceptibility. The same group later exhibited person-specific gut microbiome shifts related to altered glycaemic responses following 2 weeks of S&SE supplementation at doses lower than the acceptable daily intake compared to sachet-contained vehicle glucose or no supplement16. However, other studies14 and a crossover RCT found no effect of saccharin, sucralose or aspartame on gut microbiota or glucose regulation after 2 weeks in healthy individuals14,15. A recent cohort study linked sugar-sweetened beverage intake to gut microbiota changes and metabolites associated with diabetes risk17. Controlled, long-term studies are needed to directly assess the impact of replacing sugar with S&SEs on microbiota and metabolic outcomes. Although the inclusion of S&SEs in sugar-reduced diets may assist in sustaining weight loss by improving palatability and adherence, the long-term effects of S&SE intake on the gut microbiota and their potential influence on cardiometabolic health and safety remain to be elucidated. Therefore, the aim of this RCT in the SWEET project (‘Sweeteners and sweetness enhancers: prolonged effects on health, obesity and safety') was to assess the effect of combined and prolonged use of S&SEs (in both foods and drinks)—as part of a healthy sugar-reduced ad libitum diet—on weight loss maintenance, cardiometabolic risk factors and gut microbiota composition in adults with overweight or obesity. In children with overweight or obesity, the trial focused on weight control and cardiometabolic outcomes18. We reasoned that the inclusion of S&SEs in foods and drinks would increase the palatability of the diet and thereby compliance with the recommendations for a healthy sugar-reduced diet, resulting in improved control of body weight and related risk factors, with no effect on gut microbiota or other safety concerns associated with their long-term use compared with a diet excluding S&SEs. The primary outcomes were 1-year changes in body weight and gut microbiota composition in adults. Secondary outcomes included 1-year changes in risk factors for T2D and CVD, body mass index (BMI)-for-age z-score in children, intrahepatic lipid (IHL) content, the occurrence of (serious) adverse events (AEs), gastrointestinal symptoms and use of concomitant medication in adults with overweight or obesity. The most common causes for dropout were either personal reasons or unknown. All explanations for dropout and exclusion can be found in Extended Data Table 1. This trial was conducted during the COVID-19 pandemic; therefore, we encountered obstacles such as disrupted participant follow-up, increased dropout rates and logistical challenges related to travel restrictions and safety protocols. Flow chart of participant enrollment, allocation, and follow-up in the study. A total of 868 individuals were pre-screened, with 379 randomized into two groups: the sugar group (n = 189) and the S&SEs group (n = 190). The figure shows exclusions, dropouts, and the number of adults and children completing each study visit (CID1–CID4) at baseline (M0), weight loss (WL)/weight stability (WS) (M2), mid-weight maintenance (WM) (M6), and after weight maintenance (M12), including the adult microbiota subgroup. The baseline visit (month 0, M0) was completed by 95% of the included adults (median (Q1–Q3) age, 47 years (40–50 years), 71% female; Table 1). with no difference between the groups (Extended Data Table 2). For children, 95% of the included participants completed the baseline visit (median (Q1–Q3) age 10 years (9–11 years), 61% girls; Supplementary Table 1). The 28 children who completed M2 and had a parent who achieved ≥5% weight loss tended to lose weight (−0.5 ± 0.14 kg, P = 0.07) and increase height (1.2 ± 0.8 cm, P < 0.0001), resulting in a 0.19 ± 0.18 reduction in BMI-for-age z-score (P < 0.0001). The changes in body weight (mean ± s.d., sugar group, −0.6 ± 1.6 kg versus S&SEs group, −0.4 ± 1.2 kg), height (sugar group, 1.4 ± 0.8 cm versus S&SEs group, 1.0 ± 0.7 cm) and BMI-for-age z-score (sugar group, −0.22 ± 0.20 versus S&SEs group, −0.17 ± 0.14) did not differ between the groups (P = 0.65, P = 0.31 and P = 0.66, respectively) (Supplementary Table 1). No other changes were observed at M2 (Supplementary Table 1). The adults' body weight over time (in the intention-to-treat (ITT) population) is shown in Fig. The 1-year change in adult body weight (M12 − M0) was −6.4 ± 6.5 kg (mean ± s.d.) A similar lowering of body weight (−6.3 ± 7.2 kg) was observed for completers (Extended Data Fig. For the ITT population (last observation carried forward), the S&SEs group maintained a 1.6 ± 0.7 kg larger weight loss (mean ± s.e.m., P = 0.03) than the sugar group (Table 2). For completers, the difference was similar but not significant (1.7 ± 1.0 kg, P = 0.08) (Table 2). Imputation of missing body weight values for the ITT population based on the same model showed that the S&SEs group maintained a 1.8 ± 0.7 kg larger weight loss (mean ± s.e.m.) (P = 0.01) than the sugar group (Table 2). For the 74 adult participants who dropped out after successful weight loss, missing data are imputed by the last observation carried forward. Body weight was measured in the fasting state except at months 0.5, 1, 4 and 9. Statistical differences between groups were assessed using ANCOVA linear mixed models. Per-protocol analyses were carried out using the compliance scores (Table 2 and Extended Data Table 3). Results for participants scoring at least two points resulted in similar weight differences between groups as the ITT and completers analyses. However, by increased dietary compliance, the weight difference between groups increased (participants scoring three or four points). In both cases, the S&SEs group had the largest weight loss maintenance (Table 2). The clinical characteristics of the subgroup analysed for gut microbiota (n = 137) were comparable to those of the total adult population (n = 341) (Supplementary Table 2). Consistent with the findings in all adults of the total study population, the gut microbiota subgroup showed significantly lower body weight regain in the S&SEs group compared to the sugar group over 1 year (3.4 ± 0.7 kg versus 5.6 ± 0.8 kg, P = 0.016) (Supplementary Table 2). The trends in alpha diversity change over time was not different between groups (Supplementary Fig. We observed a significant interaction between overall microbiota composition change in time and intervention group (PERMANOVA, time × group interaction, P < 0.005) (Fig. A total of 46 taxa exhibited differential trends in relative abundance over time between the groups (Extended Data Fig. The first two constraint axes are plotted, and the amount of variation captured by an axis is displayed in parentheses after the axis name. Black arrows show the direction and relative size of the variables' influence on ordination. The significance of the interaction between time and group was tested with PERMANOVA, and results for each distance are indicated. The distinct shift in microbial communities between groups over time revealed increased overall abundance of multiple short-chain fatty acid (SCFA)-producing genera in the S&SEs group, including Megasphaera, Megamonas, Dialister, Catenibacterium, Eubacterium eligens, Lachnospiraceae ND3007, Prevotella, Alloprevotella, Porphyromonas, Butyricimonas, Oscillospira, Eubacterium siraeum and CAG:56 (Fig. In line with this finding, higher abundances of SCFA-producing families were observed in the S&SEs group for Veillonellaceae, Prevotellaceae and Porophyromonadaceae (Supplementary Fig. Additionally, higher abundances for other SCFA-producing families were found in the S&SEs group compared to the sugar group (Peptococcaceae, Actinomycetaceae, Peptostreptococcales, Tissierellales and Clostridia vadinBB60; Supplementary Fig. Only three genera—Saccharimonadales, Candidatus Competibacter and Clostridium sensu stricto 1—exhibited lower abundance in the S&SEs group compared to the sugar group (Fig. 4k,l,n), corresponding to the lower abundance of Saccharimonadales and Competibacteraceae at the family level (Supplementary Fig. Spaghetti plots depicting change in abundance over time for genera involved in SCFA production. The y axis shows CSS normalized and log-transformed read counts (abundance), and the x axis indicates time (month). Coloured by intervention groups, straight lines indicate the fit of a simple linear regression with corresponding 95% confidence intervals. Statistical importance of differences in trends between groups was tested with linear mixed-effect models as implemented in LinDa; outcomes are indicated above. P values adjusted using false discovery rate. Additionally, among the most strongly affected genera with a higher abundance in the S&SEs group was Methanolobus, a methane (CH4)-producing genus (Fig. This pattern was also observed for Methanosarcinacea at the family level (Supplementary Fig. Several taxa, such as Megasphaera, Catenibacterium and Methanolobus, were different between groups at baseline. Pathway analyses executed by PICRUSt2 MetaCyc pathways showed that in the S&SE group, methanogenesis (METHANOGENESIS-PWY, coenzyme F430 biosynthesis (PWY-5196), methyl-coenzyme M oxidation to CO2 I (PWY-5209), superpathway of methanogenesis (PWY-6830), tetrahydromethanopterin biosynthesis (PWY-6148)) was upregulated, reflecting enhanced CH4-producing potential (Extended Data Fig. In line with the increased abundance of SCFA-producing taxa, SCFA fermentation pathways are potentially upregulated (notably acetate, P341-PWY, PWY-5517, 5532, 6344, 6328, 6185, 5392). Additionally, aromatic compound degradation (PWY-5430, PWY-6185) and l-arabinose degradation (PWY-5517) also show upregulation, suggesting higher breakdown of complex plant-derived compounds and carbohydrates. Finally, compounds related to aromatic amino acids (chorismate), vitamin biosynthesis and cofactor production (PWY-6160, PWY-6165, PWY-5507) are upregulated. Downregulated pathways in the S&SE group highlight a reduction in photosynthesis (chlorophyll biosynthesis), polyunsaturated fatty acid synthesis (for example, linoleate biosynthesis, PWY-5995) and phospholipid remodelling (PWY-7409). Analysis of the microbiota patterns in responders and non-responders was done by random forest analysis. This analysis showed that change in glycated haemoglobin A1c (HbA1c) and body weight regain after weight loss in the S&SEs group but not in the sugar group could be predicted with a degree of confidence (Extended Data Fig. With reasonable accuracy, baseline microbiota composition (M0) classified individuals that did not change or decreased HbA1c during weight maintenance (WM) (responders) versus those that showed an increase in HbA1c (aea under the curve (AUC), 0.76; Fig. Furthermore, microbial composition after weight loss (M2) gave a weaker but reasonable prediction of the extent of body weight regain (AUC, 0.69), indicating the presence of detectable differences in microbiota composition between responders and non-responders. Notably, microbiota composition after weight loss (M2) performed considerably worse in predicting change in HbA1c (AUC, 0.62) than at baseline. Taxa significantly contributing to the classification of HbA1c with microbial composition at baseline included Petrotoga, SH_PL14, Acetobacter, CAG:56, Faecalibacterium and Desulfotomaculales. At M2, the key taxa were Rothia, TMX7 and Flavonifractor (Fig. For body weight classification with microbial composition at M2, important taxa included Turicibacter, Family_XIII_AD3011, Eisenbergiella and Hungatella. a, Receiver operating characteristic (ROC) curves for random forest models in which prediction was considerably better than chance. Facet labels indicate the index used to define responders or non-responders and the CID from which samples were used to build the model. ROC curve colours indicate the nature of the response variable used: green for the original definition of responders or non-responders, and red for a randomly assigned definition. b, Genera that significantly contributed to the classifications shown in a. If a genus was not taxonomically assigned, the lowest assigned taxonomic level was used as the name, preceded by a capital letter indicating the taxonomic rank. Facet labels indicate the classification group (Resp, responders; NonResp, non-responders) for which contribution was measured, or the mean decrease in accuracy (MDA) for the model overall. Bar colour reflects significance (P value); grey bars indicate no significant contribution. P values were estimated with a permutation test as implemented in the rfPermute package with no adjustment for multiple testing. In the total RCT (n = 341), Bristol stool scale (BSS) values were collected on all clinical investigation days (CIDs). No differences in BSS scores were observed between groups at the different time points. Both the S&SEs and sugar groups experienced more constipation or harder lumps after weight loss (M2), based on the decrease in BSS score (P < 0.001 and P = 0.001, respectively) with a shift towards types one and two (Supplementary Fig. During the WM period, BSS scores increased in the S&SEs group (M2 versus M12, P = 0.014), with the most substantial difference observed between M2 (after weight loss) and M6 (P = 0.023) with a shift towards more normal stool types (types three and four) (Supplementary Fig. For the sugar group, no differences in BSS scores were observed during the WM period (Supplementary Fig. For both adults and children, total sugars constituted 15 E% at baseline (M0) (Supplementary Table 4). Large variability was observed for food groups at M0, and intake was not normally distributed, especially for S&SE-containing foods. Data on added sugar intake was only available from the Danish nutritional software (n = 52) and showed that adults in the S&SEs group reduced E% of added sugar by 3.4 ± 1.2 percentage points more than the sugar group (P = 0.05). For all adults, total sugar intake was reduced by 12.0 ± 5.5 g day−1 (P = 0.03) (2.4 ± 0.9 percentage points, P = 0.01) more in the S&SEs group than the sugar group. No other differences were observed for adults or children (Supplementary Table 4). Both adults and children reduced their total intake of sugar-rich products by 142 ± 240 g day−1 (mean ± s.d.) For adults, the S&SEs group had a 107 ± 31 g day−1 larger reduction in sugar-rich products than the sugar group (P = 0.0007), whereas no differences between groups were observed for children (P = 0.36). For S&SE-containing products, adults reduced the total amount of S&SE products in the sugar group and increased it in the S&SE group (difference of 229 ± 36 g day−1, P < 0.0001). For children, the difference of 237 ± 110 g day−1 between groups tended to differ but was not significant (P = 0.07) (Supplementary Table 4). For adults, the differences between groups were mainly explained by consumption of beverages, milk, sugar, honey or jam and candy (Supplementary Table 5). Data for other products can be found in Supplementary Table 5. In the subgroup for microbiota analyses, data on energy intake, S&SE intake, added sugar intake and urinary S&SE excretion patterns reflected the total study population (Supplementary Table 6). Change in urinary nitrogen excretion (reflecting protein intake) at M6 and M12 was similar in both groups. Likewise, there were no differences in excretion of glucose, sucrose and fructose. Biomarkers of S&SE intake, except Steviol acyl glucuronide, increased in the S&SE group and decreased in the sugar group (P < 0.001 at M12). The 1-year change in excretion was generally larger than the change at M6 (Extended Data Table 4). In addition, no differences between groups in a subgroup at Maastricht were found in physical activity, including sedentary time, moderate or vigorous physical activity and step count (all P ≥ 0.1) (Supplementary Table 7). For adult completers, the S&SEs group had a larger reduction in BMI, total cholesterol, low-density lipoprotein cholesterol (LDL-C), high-density lipoprotein cholesterol (HDL-C) and non-HDL-cholesterol (non-HDL-C) at M6 than the sugar group (Table 3). At M12, however, only hip circumference was significantly more reduced in the S&SEs group (1.8 ± 0.8 cm) than the sugar group (P = 0.04), whereas total cholesterol, BMI and waist circumference tended to differ (P ≤ 0.1). Otherwise, no differences were observed (Table 3). Similarly, no differences in risk markers for T2D and CVD were found between groups in the subgroup for the gut microbiota analysis (Supplementary Table 2). Moreover, the 1-year change in IHL content during the WM period, determined in a subgroup at Maastricht (n = 27), remained similar between the S&SEs and sugar groups (1.0 ± 0.8% versus 0.4 ± 0.2%, respectively; P = 0.213) (Table 3). 4), with no differences between groups (sugar group, −0.25 ± 0.38 versus S&SEs group, −0.34 ± 0.41, P = 0.48) (Supplementary Table 8). There were also no differences in other outcomes (all P ≥ 0.18) (Supplementary Table 8). Nine serious AEs were reported during the WM period, of which five were in the sugar group and four were in the S&SEs group (Extended Data Table 5). The serious AEs in the sugar group included a surgical procedure unrelated to the intervention, a shoulder lesion caused by an accident, postoperative ileus unrelated to the intervention and angina pectoris, in which the participants recovered without further consequences. Furthermore, one participant in the sugar group was diagnosed with hypothyroidism. In the S&SEs group, the serious AEs included laparoscopic cholecystectomy, diverticulitis and pulmonary embolism, in which the participants recovered without further consequences. Additionally, one participant in the S&SEs group reported a serious AE, involving an illness of which no specific details were disclosed and was not presumed to be serious. In addition, the number of AEs related to gastrointestinal symptoms was higher in the S&SEs group than in the sugar group (P = 0.026) (Extended Data Table 5). For concomitant medications, the use of S&SEs did not result in a differential use of total concomitant medication (P = 0.775) (Supplementary Table 9). Only for hormonal agents was the amount of reported concomitant medication lower in the S&SEs group, and the S&SEs group was found to be a predictor (P = 0.035). However, when analysing the type of hormonal agents separately, no differences were found between groups in reported glucocorticoids, sex hormones or thyroid drugs (P = 0.649, P = 0.491 and P = 1.000, respectively). A slightly better weight loss maintenance at 1 year was observed in adults consuming S&SE products compared with those who did not (sugar group), both in the context of healthy diets low in added sugars. The improved weight loss maintenance was accompanied by altered gut microbiota composition, with a higher abundance of SCFA-producing and CH4-producing bacterial taxa in the S&SEs group. Furthermore, the S&SE diet led to significantly larger decreases in BMI, total cholesterol, LDL-C, HDL-C and non-HDL-C at 6 months and in hip circumference after 1 year. To our knowledge, this is the first study to demonstrate long-term beneficial health impacts of S&SE intake in adults with overweight or obesity. Both groups maintained a large weight loss after 1 year, with the S&SEs group achieving a 1.6 kg greater weight loss. Participants reduced sugar intake twice as much in the S&SEs group, suggesting that replacing sugar with S&SEs supports WM better. The combined scoring system also demonstrated that the highest level of dietary compliance resulted in the largest weight difference (3.8 kg), suggesting that more consistent adherence could further amplify the observed differences after 1 year, making the results more clinically relevant. Our trial demonstrated almost twice as large a difference in body weight compared to the systematic review and meta-analysis (SRMA) of the WHO, in which an average weight loss of 0.71 kg with S&SEs was reported. The WHO SRMA, however, included studies with different durations and control groups, and the main effect was driven by interventions comparing S&SEs with added sugar as the control, whereas the present trial used a healthy sugar-reduced diet as the control. Consistently, most clinical studies or meta-analyses reported no effects or even beneficial effects of S&SEs on body weight control19,20. Notably, the recommendations of the WHO SRMA were conditional, meaning that the assessment panel was less confident regarding their judgement and that the link between S&SEs and disease outcomes might be confounded by baseline characteristics of participants and complicated patterns of S&SEs use. To support this idea, a recent substitution analysis of prospective cohort studies found that intake of S&SE beverages was associated with a reduction in body weight, incidence of obesity, coronary heart disease and all-cause mortality compared with sugar-sweetened beverages and was no different than water when the influence of reverse causality and residual confounding was mitigated22. As previously mentioned, different conclusions in prior studies and reviews could arise from differences in the choice of comparator, such as a control contributing with (for example, sugar) or without energy (for example, water)23,24. In their SRMA, they found that S&SE-sweetened beverages reduced body weight and cardiometabolic risk factors24 and were associated with reductions in both risk of obesity and CVD outcomes22 when replacing sugar-sweetened beverages. This analysis and the critiques on joint analyses of energy and non-energy comparators led to the second WHO-commissioned SRMA, which included the intended replacement of energy-containing sugars with low and no-energy sweeteners. The present trial aimed to meet the previous critiques both in terms of trial duration (1 year) and choice of comparator26,27,28. In the present trial, significant reductions in some CVD risk markers were observed at M6 in the S&SEs group compared with the sugar group, but these did not last until M12. Fading compliance and/or a decreased number of participants in the last part of the trial might explain this difference. Still, urinary excretion of S&SEs was significantly higher in the S&SEs group versus the sugar group at M12, so the lack of significant differences is probably a result of the lower number of participants. We observed no effects of the S&SE diet on risk markers for T2D and CVD, including IHL content, upon 10 months of S&SE intake. Therefore, our findings from our 1-year intervention study are consistent with those from the WHO SRMA on short-term RCTs29, and together, these data do not support the notion of potential undesirable health effects with long-term use of S&SEs4. Previous short-term RCTs (1–12 weeks) have indicated AEs of S&SEs on glycaemic response, which was driven by alterations in gut microbial composition and functionality; however, the data are not consistent14,16,30. In the current trial, distinct shifts in gut microbiota composition were observed in a subgroup of the S&SEs group, characterized by a higher abundance of taxa associated with SCFA and CH4 production. Additionally, pathway analysis inferred by PICRUSt2 MetaCyc pathways confirmed an increase in methanogenesis, and potentially in fermentation and SCFA production, among other pathways. SCFAs are known to promote beneficial health effects, such as increased energy expenditure through enhanced lipid oxidation and improved satiety by modulating gut–brain signalling through incretins31. Therefore, SCFAs may prevent and/or counteract obesity and associated cardiometabolic risk factors. Additionally, microbial composition at baseline and/or after initial weight loss can classify with reasonable accuracy variation in body weight regain as well as changes in HbA1c. This is in line with previous studies indicating that there may be responders and non-responders to S&SEs intervention driven by their microbiota composition14,16. Overall, our trial indicates a shift towards saccharolytic fermentation (SCFA-producing taxa) in the S&SEs group, which may have contributed to the positive effects on body weight maintenance. Interestingly, the higher abundance in Methanolobus, known for its ability to generate CH4 as a metabolic byproduct, in the S&SEs group was accompanied by more gastrointestinal symptoms. Increased levels of CH4 may contribute to gastrointestinal symptoms by inhibiting gastrointestinal motility, potentially causing slow-transit constipation and related issues like abdominal pain32,33. Additionally, some S&SEs, such as sugar alcohols (for example, sorbitol, xylitol and mannitol), are incompletely absorbed in the small intestine, reaching the colon where they act as osmotic laxatives13,34,35. The osmotic effects, however, vary between different S&SEs, influencing the type and severity of symptoms36. Whether the experienced gastrointestinal symptoms translate into a substantial clinical burden remains to be determined. Additionally, the amount of reported concomitant medication use was not affected by long-term S&SE intake. A strength of the present RCT is the investigation of the long-term (10 months) effects of S&SEs compared to no S&SEs on weight loss maintenance in the context of an ad libitum, low-sugar, healthy diet, whereas most previous studies have investigated single S&SEs37 and only a few included both foods and drinks38,39. Furthermore, longer-term studies (≥6 months) are scarce. In addition, the effect of 10 months of S&SE intake on the human gut microbiome was investigated for the first time in a real-life, controlled setting. Moreover, this multi-centre RCT included participants from northern, central, southern and south-eastern Europe, providing a comprehensive representation across diverse geographic locations in Europe. In addition, this study reflects realistic consumption patterns and dosages of various S&SEs in locally available products. Although participants were recruited from diverse European regions with varying dietary habits, the use of standardized dietary guidance and objective compliance measures (that is, urinary biomarkers) increases confidence that cultural differences had limited impact on adherence and outcomes. The dropout rate was higher than expected (40% versus 30%), which resulted in a lower number of completers than required for 90% power. However, with 203 completers, the power for a 1-year change in weight loss was 86%, which can be deemed satisfactory. Furthermore, results on energy intake should be interpreted cautiously owing to underreporting, with baseline intake about 25% lower than the estimated energy need, as previously observed40. The same caveat applies to the children's results, which should be interpreted with caution because of the limited sample size and suboptimal compliance. Moreover, the absence of direct measurements of SCFAs may have limited our ability to fully interpret the metabolic implications of the observed microbial shifts. Further detailed analysis of the functionality of the microbiota by metagenomic analysis or analysis of microbial metabolites, in addition to the current taxonomic classification and prediction of functionality, as done by PICRUSt2 MetaCyc pathways, would have provided more mechanistic insights into the link between changes in gut microbiota compositions and changes in clinical parameters. Future research is warranted to investigate the effects of long-term intake of S&SEs on human gut microbial functionality, allowing for a more comprehensive assessment of long-term physiological effects in humans. In conclusion, the present RCT showed that compared to adults consuming drinks and foods without S&SEs, those who included S&SEs in a healthy, ad libitum, sugar-reduced diet exhibited improved 1-year weight loss maintenance and gut microbiota composition (in terms of a higher abundance of SCFA-producing and CH4-producing bacterial taxa) without affecting cardiometabolic health markers. Moreover, based on initial microbial composition or composition directly after weight loss, we can classify with reasonable accuracy the extent of body weight regain or change in HbA1c, suggesting a significant contribution of the altered microbiota composition to weight loss maintenance. The personalized microbiota composition-related effect on HbA1c and body weight during WM warrants further mechanistic deepening to better understand the clinical implications. This trial has been registered at ClinicalTrials.gov (NCT04226911), was approved by national ethical committees (protocols and amendments) and was conducted in accordance with the Declaration of Helsinki. The study was monitored for Good Clinical Practice compliance by the European Clinical Research Infrastructure Network, as described previously18. All participants provided written informed consent. The SWEET trial was a two-armed parallel group RCT conducted at four intervention sites: Athens (Harokopio University of Athens, Greece), Copenhagen (University of Copenhagen, Denmark), Maastricht (Maastricht University, the Netherlands) and Pamplona (University of Navarra, Spain), effectively covering northern, central, southern and south-eastern Europe. The 1-year RCT consisted of an initial 2-month weight loss period followed by a 10-month, randomized, two-armed parallel WM period. For adults, the goal was first to achieve a weight loss of ≥5% of the initial weight and second, to maintain their new body weight. For children, the first goal was to achieve weight stability and second, to maintain their BMI-for-age z-score. The analysis of gut microbiota composition was done in a subgroup of 137 adult completers. Participants were enrolled between June 2020 and October 2021. In brief, 18–65-year-old men and women (self-reported sex) with BMI ≥ 25 kg m−2 and 6–12-year-old boys and girls (self-reported sex) with a BMI-for-age of >85th percentile were included. Children were included in a family setting with at least one recruited parent. Participants were required to have a regular consumption of sugar-containing or sugar-sweetened products. Adult participants were excluded at screening if, for example, they had been surgically treated for obesity, were taking medication affecting body weight, had been diagnosed with diabetes, had a fasting glucose >7.0 mmol l−1 or had systolic blood pressure of >160 mmHg and/or diastolic blood pressure of >100 mmHg. Children were excluded if, for example, they performed >10 h of intensive physical training per week, had self-reported eating disorders, were diagnosed with diabetes or used medication that affected their body weight. All adults received low-energy diet products free of charge. Participants in Copenhagen, Pamplona and Athens did not receive reimbursement for their participation; travel expenses and financial compensation were provided for participants in Maastricht. The trial lasted 1 year for each participant. The first CID (M0) was on 24 August 2020, and the last participant's final visit (M12) was on 6 October 2022. For children, weight stability should be achieved by following the dietary recommendations of the American Academy of Pediatrics on the prevention, assessment and treatment of overweight and obesity18. All participants were randomly allocated to one of two diet groups in a 1:1 ratio by a site-specific, computerized randomization list created by a person in Copenhagen not involved in the RCT. Stratification was done by sex, age (<40 or ≥40 years) and BMI (<30 or ≥30 kg m−2) in blocks of four. If there was more than one eligible child per household, it was still the randomization group of the oldest adult participant that determined the child allocation. Although randomization was done after inclusion (at screening), it was not revealed to the participants before completion of the 2-month weight loss or weight stability period. The two ad libitum intervention diets were a healthy diet with <10 E% added sugar, allowing foods and drinks with all types of S&SE products commercially available (S&SEs group), and a healthy diet with <10 E% added sugar, not allowing S&SE products (sugar group). The maximum allowed sugar intake was calculated individually at M2 and recalculated at M6 and then converted to a simple trial-specific unit system (one unit = 10 g sugar). The participants received their maximum unit intake and lists with sugar-rich products, including the unit content. Lists were divided into different categories; for example, drinks, breakfast or desserts. The lists also provided a corresponding product with S&SEs (similar in weight or volume; details described elsewhere18). For the S&SEs group, the aim was to replace as many sugar-containing products as possible with S&SEs products, whereas S&SEs products were not allowed in the sugar group. S&SEs included high-potency sweeteners (for example, aspartame, acesulfame-K, saccharin, thaumatin, neotame, stevia glycosides), polyols (for example, erythritol, sorbitol, mannitol, isomalt, maltitol, lactitol, xylitol), slowly digestible carbohydrates (for example, sucromalt, isomaltulose) and sweet fibres or oligosaccharides (inulin-type oligosaccharides). Owing to the characteristics of the trial, blinding was not possible, but all efforts to blind trial staff taking measurements and doing statistical analyses were made. During the intervention period, participants were supervised by dieticians at least every third month. The goal for adults was to maintain weight loss, and for children, the goal was to maintain BMI-for-age z-score. Further reduction in body weight or BMI-for-age z-score was allowed if the participant was compliant with the intervention. Data were collected according to common standard operating procedures, which were used in all intervention sites. Most data were collected at the CIDs after ≥10 h of overnight fast. All data were stored in a central data hub in Copenhagen, from which pseudo-anonymized data can be requested until 2032 through a data-sharing contract. As of 2032, fully anonymized data can be transferred. Body weight was measured to the nearest 0.1 kg on a digital scale, with participants wearing underwear or light clothes. Fasting body weight was measured at screening and CIDs, but fasting was not required at other visits. Participants were selected based on whether they completed the full intervention and whether faecal spot samples were available at all time points. The participants were stratified based on age, sex and centre. Faecal spot samples were collected and immediately frozen (−20 °C) at home by the participants before all CIDs. Barcoded amplicons from the V3–V4 region of 16S rRNA genes (341 F, 5′-CCTACGGGNGGCWGCAG-3′; 785 R, 5′-GACTACHVGGGTATCTAATCC-3′) were generated using the Illumina two-step PCR protocol, and sequencing on an Illumina MiSeq with the paired-end (2×) 300 bp protocol (Nextera XT index kit). After each PCR step, the products were purified (QIAquick PCR Purification Kit), the size of the PCR products was checked on a fragment analyzer (Advanced Analytical), and concentration was quantified by fluorometric analysis (Qubit dsDNA HS Assay Kit). All outcomes were determined at M0, M2, M6 and M12. Stool consistency was assessed using BSS for each sample41. Secondary outcomes included changes in anthropometry and body composition, risk factors for T2D and CVD, IHL content, the occurrence of (serious) AEs, gastrointestinal symptoms and use of concomitant medication in adults with overweight or obesity. The methods for measuring anthropometry (BMI, waist and hip circumference) and body composition (fat percentage, fat mass, fat-free mass and computed visceral adipose tissue (CoreScan software; Encore v.17.0) using Dual-energy X-ray absorptiometry) have been extensively described elsewhere18. Risk factors for T2D include elevated levels of glucose, insulin and HbA1c; for CVD, risk factors include high cholesterol, elevated triglycerides and high blood pressure. During CIDs, fasting venous blood samples were drawn. Whole blood samples were collected in EDTA tubes for HbA1c analysis and immediately stored locally at −80 °C. Blood samples were collected in serum separator tubes for analyses of lipids (triglycerides, total cholesterol, LDL-C and HDL-C) and insulin. Whole blood samples with fluoride were collected for glucose analysis, with EDTA for CCK, GLP-1 and EDTA plus aprotinin for ghrelin analysis. Supernatant (serum or plasma) samples were aliquoted and stored locally at −80 °C until shipment to the central lab at Bioiatriki (Athens). Glucose concentration was measured by the enzymatic Hexokinase/G-6-PD method (Alinity c Glucose Reagent Kit 07P55, Abbott) using an Alinity c analyser (a clinical chemistry analyser from Abbott). A conversion factor for glucose, from mg dl−1 to mmol l−1, was applied by multiplying by 0.0555. Insulin concentration was measured by a chemiluminescent microparticle immunoassay (Alinity i Insulin Reagent kit, Abbott) using an Alinity i analyser (immunoassay analyser from Abbott). A conversion factor for insulin, from µIU ml−1 to pmol l−1, was applied by multiplying by 6. HbA1c was measured by an enzymatic assay (Alinity c Haemoglobin A1c Reagent kit, Abbott) using an Alinity c analyser, clinical chemistry (Abbott). One participant had results below the lower detection limit during two CIDs. These latter results were reported as 5.0%. Total cholesterol was measured by an enzymatic assay (Alinity c Cholesterol Reagent kit (Abbott)), and LDL-C and HDL-C were measured by liquid (Alinity c Direct LDL Reagent kit) and accelerator (Alinity c Ultra HDL Reagent kit) selective detergent methods, respectively (both Abbott). All cholesterol concentrations were analysed using an Alinity c analyser. A conversion factor for cholesterol (total, LDL and HDL), from mg dl−1 to mmol l−1, was applied by multiplying by 0.02586. Triglycerides were measured by a glycerol phosphate oxidase method (Alinity c Triglyceride Reagent kit, Abbott) using an Alinity c analyser. A conversion factor for triglycerides, from mg dl−1 to mmol l−1, was applied by multiplying by 0.01129. CCK was measured by a competitive enzyme immunoassay method (RayBio Human CCK Enzyme Immunoassay kit, RayBiotech) using a BioTek Quant analyser (BioTek Instruments). Two participants had results above the range of quantification (4,000 pg ml−1) during two CIDs, and their CCK results were reported as 4,000 pg ml−1. Proton-magnetic resonance spectroscopy (1H-MRS) was performed using a 3T MR system (Achieva 3T-X Philips Healthcare) at M0, M2 and M12. A 32-channel sense cardiac/torso coil (Philips Healthcare) was used, and a 30 × 30 × 30 mm voxel was placed in the lower hepatic lobe. The STEAM (repetition time, 4,500 ms; echo time, 20 ms; number of signal averages, 128) sequence was used, as described previously42,43. VAPOR water suppression was applied, and an additional water reference scan was obtained (number of signal averages, 16). Phasing, frequency alignment and eddy current correction were all performed on spectra before signal averaging. Any experienced (serious) AEs or changes in medication use were registered during the CIDs. During the WM period, the participants were asked, regardless of intervention, directly about AEs potentially related to the consumption of S&SEs, including gastrointestinal symptoms and headache. Subjective appetite sensations over the last 7 days were reported in a questionnaire delivery platform on-site or at home within ±7 days of each CID. The participants answered five questions: (1) how strong was your desire to eat savoury foods; (2) how strong was your desire to eat sweet foods; (3) how satiated have you felt; (4) how hungry have you felt; and (5) how full have you felt? All questions were developed in English and translated into the local language at each site. A visual analogue scale with extremes anchored at each end (0, not at all; 100, extremely) was used. Depending on the device that was used, lines were not necessarily 10 cm long, but the rating was presented as a percentage; for example, a mark at 4 cm on an 8 cm line would correspond to 50%. Only records of a minimum of 3 days were deemed valid and used for further analysis. Daily average intake of energy and macronutrients was calculated by national dietary software at the four intervention sites. In Copenhagen, all information from food records was manually entered into the software programme DankostPro44. This software is based on the official Danish national food composition database (v.4) developed by the National Food Institute at the Technical University of Denmark45. In Maastricht, food intake data were analysed by the Eetmeter food diary and analysis tool (Voedingscentrum). In Harokopio, food intake data were analysed with Nutritionist V diet analysis software (v.2.1, 1999; First Databank), extensively amended to include traditional Greek foods and recipes, as described in the Food Composition Tables and Composition of Greek Cooked Food and Dishes46. Furthermore, the database was updated with nutritional information of processed foods provided by independent research institutes, food companies and fast-food chains. In Pamplona, food records were manually entered in the online application Nutrium (Nutrium.com), which is based on two food databases (BEDCA & CESNID from Spain and the US Department of Agriculture). Furthermore, intake (g) of products with sugar and S&SEs and the corresponding units were estimated. As an objective measure, 24 h urine samples were collected at M0, M6 and M12. The urine samples were weighed and volumes registered (Maastricht, Harokopio and Pamplona) or calculated from the urinary density (Copenhagen). Urinary biomarkers of S&SEs (acesulfame-K, saccharin, sucralose, cyclamate and steviol glucuronide) as well as glucose, fructose and sucrose were analysed by ultra-pressure liquid chromatography coupled to tandem mass spectrometry47. After correction for dilution, urinary concentrations (ng ml−1) were multiplied by 24 h urine volume and converted to daily excretions (mg day−1). Urinary urea concentration was analysed locally and converted to daily urinary nitrogen excretion (g day−1) by multiplying urea excretion (g day−1) by 0.4664. At Copenhagen and Maastricht, urea was measured by an enzymatic UV test (colourimetry) (ABX Pentra Urea CP, Horiba ABX) using an ABX Pentra 400. At Harokopio, urea was measured by an enzymatic colourimetric (Urease) Alinity c Urea Nitrogen Reagent kit (Abbott Laboratories). At Pamplona, urea was measured by an enzymatic kinetic test (COBAS 8000, Roche Diagnostics). For the per-protocol population, participants' compliance was estimated using points (minimum zero points and maximum four points) in relation to four criteria: intake of sugar units and S&SE units and urinary excretion of S&SEs at M6 and at M12. Physical activity was measured in a subgroup at Maastricht for seven consecutive days at M0, M6 and M12 using a triaxial accelerometer (activPAL 3TM micro, PAL Technologies). The activPAL was attached to the anterior thigh of the participants and measured posture allocation, step count and 24 h physical activity, distinguishing between sleeping time, low-to-moderate physical activity (time spent standing or walking <100 steps per min), moderate-to-vigorous physical activity (time spent walking ≥100 steps per min or cycling) and sedentary time (time spent sitting or lying down). Sample size calculation was based on body weight results from a previous trial48. With an estimated dropout of 30%, a minimum of 330 adult participants should be included (approximately 25% per intervention site). The sample size for the 1-year change in gut microbiota required a minimum of 100 participants (n = 50 per intervention group) and was based on a calculation taking into account ~10% change in 20 of the 50 most abundant operational taxonomic units with an alpha of <0.05%. According to this calculation, a total of 40 participants would be enough to detect compositional changes. Furthermore, considering previous work49 in which n = 75 was used to determine differences in beta-diversity, the estimated number of n = 100 participants in our statistical power calculation would indeed be sufficient. No power calculation was performed for the children; hence, the analyses of BMI z-scores were exploratory only. Statistical analyses were conducted in R (v.4.3.1). Baseline characteristics before (M0) and after weight loss or weight stability (M2) are presented as medians (Q1–Q3), and changes at M2 and M12 are presented as unadjusted mean ± s.d. Differences between groups are presented as adjusted mean ± s.e.m. Given that randomization was completed at M0 (that is, 2 months before the 10-month intervention period was initiated), differences in the participant characteristics between groups after weight loss or weight stability (M2) and in changes during the 2-month period (M2–M0) were analysed by analysis of covariance (ANCOVA) adjusted for sex, age, baseline body weight and site. Sex was included as a covariate, given that we know from previous studies that different outcomes can vary between men and women50. The trial was not powered to analyse data from men and women separately. Change in body weight was calculated as the difference between M0 and M12 (M12 − M0). For the ITT analysis, missing data (that is, body weight at M12) were imputed as the last observation carried forward. The analysis of differences between the groups was conducted using an ANCOVA linear mixed model, with individual change in body weight as the response, and intervention group, baseline body weight, age, sex and site as fixed effects. Additionally, a complete-case analysis (all dropouts omitted) and a per-protocol analysis (only compliant participants defined by the point scoring system) were analysed with the same ANCOVA model. Finally, adult body weight was analysed with the inclusion of all visits (time points) as a linear mixed-effect model with repeated measurements. This analysis had body weight as the response variable and included the fixed effects time-intervention interaction, age, sex and baseline body weight. Site and participants were included as random effects. If there was a significant interaction, post hoc tests were conducted with the R extension package ‘emmeans' to calculate the estimated marginal mean and s.e.m. at each time point for comparison of the groups. This model was also used to impute missing body weight values for post hoc ITT analyses. For secondary outcomes on continuous data, the main analysis compared the 6-month and 12-month mean changes between the treatment groups by use of the ANCOVA linear mixed model defined above, without imputation of missing values (that is, complete-case analyses). All models were graphically checked by residual plots and quantile–quantile plots to assess model assumptions, mainly the normality assumption, and when relevant, transformed (for example, by logarithm). Furthermore, Poisson regression analysis was used to analyse the predictive effect of group intervention on reported AEs and concomitant medications. The complete microbiota data processing and analysis pipeline is available at https://github.com/AlexanderUm/SWEET_microbiome. In short, raw reads were preprocessed with the CASAVA pipeline (v.1.8.3) and the Quantitative Insights Into Microbial Ecology 2 (QIIME2; v.2023.9.1) platform. Demultiplexed reads were de-noised into amplicon sequence variants (ASVs) with the DADA2 plug-in51 and were taxonomically annotated with the Naive Bayes classifier trained on the SILVA (v.138) database52. A phylogenetic tree was constructed using the FastTree algorithm and MAFFT alignment. 53) was used to infer metabolic pathways using default settings, and the unstratified MetaCyc pathways abundance table was used for further pathways analysis. Phylogenetic tree, taxonomic, pathways abundance and ASV tables were imported in the R statistical and programming environment (v.4.3.2)54 using the qiime2R package55. Before further analysis, ASVs with fewer than 50 reads across all samples and those taxonomically assigned to mitochondria or chloroplasts or to kingdoms other than Bacteria or Archaea, as well as those not assigned to any phylum, were removed from the dataset. An appropriate normalization of ASV counts, as specified below, was applied in correspondence with the performed analysis or visualization. Microbial alpha diversity metrics (Chao1, observed species, Shannon and Simpson indexes) were calculated, and a linear mixed-effects regression analysis (LMM) was performed as implemented in the MicrobiomeStat63 R package for all indexes except observed species. For observed species, a generalized linear mixed-effect model (GLMM) for a Poisson distribution (log link function) was performed using the lme4 package64. Alpha diversity metrics were calculated using the unfiltered ASV table rarefied at even depth (40k). The LMM and GLMM used intervention group (main effect), time of sampling (M0, M2, M6 and M12) (time variable), intervention centre (adjustment variable) as fixed effects and subject ID as the random effect. Differences in overall microbial composition between intervention groups were assessed using distance-based redundancy analysis (dbRDA) followed by an ANOVA-like permutation test (PERMANOVA) as implemented in the vegan65 R package. The following model was used for dbRDA: DistanceMatrix ~ Time × IntervetionGroup + Condition(Country). The PERMANOVA was performed with 999 permutations and by model terms. Dissimilarity distances (Jaccard, Bray–Curtis, unweighted and weighted UniFrac) for dbRDA were calculated as implemented in the phyloseq66 R package from the ASV count table with total-sum scaling and log2-transformed count. Differences in individual taxa and metabolic pathway abundance trends over time between intervention groups were tested using the LinDA method as implemented in the MicrobiomeStat R package. Differential abundance was assessed at the genus and family taxonomic levels; in addition, inferred MetaCyc pathways and default count normalization implemented in LinDA was used. Before differential abundance analysis, features with prevalence less than 50% were removed from the dataset. The P values calculated for the LMM were adjusted for multiple testing with false discovery rate correction, and q values less than or equal to 0.1 were considered significant. The LMM model used for differential abundance analysis was identical to the model used for analysis of alpha diversity trends. Microbial composition at each CID was then used to predict the response with a random forest. For classification, genera with a minimum prevalence of 50% in any treatment group were normalized using total-sum-scaling and then log2-transformed. Classification accuracy was assessed with 25 times repeated fivefold cross-validation as implemented in the caret package, and AUC was estimated. This process was repeated ten times, and the results were compared with the original model. The pROC70 package was used to build ROC curves. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Any additional information required to reanalyse the data reported in this paper is available from the lead contacts upon request. Upon approval of a synopsis with a research idea (within 4 weeks of submission), individual de-identified data will be shared. In principle, all reasonable requests will be approved. Raw fastq files of 16S rRNA gene amplicon sequences are available from NCBI BioProject under accession number PRJNA1276528. Source data are provided with this paper. Full microbiome analysis is available at https://github.com/AlexanderUm/SWEET_microbiome. Fruh, S. Obesity: risk factors, complications, and strategies for sustainable long-term weight management. & Melgar, S. The impact of Western diet and nutrients on the microbiota and immune response at mucosal interfaces. Use of Non-Sugar Sweeteners: WHO Guideline (2023). & Feskens, E. Total, free, and added sugar consumption and adherence to guidelines: the Dutch National Food Consumption Survey 2007–2010. Ruiz, E. et al. Dietary intake of individual (free and intrinsic) sugars and food sources in the Spanish population: findings from the ANIBES Study. Prolepsis Institute of Preventive Medicine Environmental and Occupational Health. National Dietary Guidelines for Greek Adults and Children (Ministry of Health, 2014). Guideline: Sugar Intake for Adults and Children (2015). Pedersen, A. N. et al. Dietary habits in Denmark 2011–2013 (DTU Fødevareinstituttet, 2015). & Rother, K. Trends in the consumption of low-calorie sweeteners. Artificial sweeteners and risk of cardiovascular diseases: results from the prospective NutriNet-Santé cohort. Artificial sweeteners and risk of type 2 diabetes in the prospective NutriNet-Santé Cohort. Effects of sweeteners on the gut microbiota: a review of experimental studies and clinical trials. Artificial sweeteners induce glucose intolerance by altering the gut microbiota. Normand, M., Ritz, C., Mela, D. & Raben, A. Low-energy sweeteners and body weight: a citation network analysis. Suez, J. et al. Personalized microbiome-driven effects of non-nutritive sweeteners on human glucose tolerance. Zhang, Y. et al. Sugar-sweetened beverage intake, gut microbiota, circulating metabolites, and diabetes risk in Hispanic Community Health Study/Study of Latinos. Kjølbæk, L. et al. Protocol for a multicentre, parallel, randomised, controlled trial on the effect of sweeteners and sweetness enhancers on health, obesity and safety in overweight adults and children. & Koster-Rasmussen, R. Substitution of sugar-sweetened beverages with non-caloric alternatives and weight change: a systematic review of randomized trials and meta-analysis. Pang, M. D., Goossens, G. H. & Blaak, E. E. The impact of artificial sweeteners on body weight control and glucose homeostasis. Khan, T. A. et al. WHO guideline on the use of non-sugar sweeteners: a need for reconsideration. Relation of change or substitution of low- and no-calorie sweetened beverages with cardiometabolic outcomes: a systematic review and meta-analysis of prospective cohort studies. Sievenpiper, J. L., Khan, T. A., Ha, V., Viguiliouk, E. & Auyeung, R. The importance of study design in the assessment of nonnutritive sweeteners and cardiometabolic health. McGlynn, N. D. et al. Association of low- and no-calorie sweetened beverages as a replacement for sugar-sweetened beverages with body weight and cardiometabolic risk: a systematic review and meta-analysis. Aas, A. et al. Evidence-based European recommendations for the dietary management of diabetes. Perspective: standards for research and reporting on low-energy (“artificial”) sweeteners. Expert consensus on low-calorie sweeteners: facts, research gaps and suggested actions. Health Effects of the use of Non-Sugar Sweeteners: A Systematic Review and Meta-Analysis (World Health Organization, 2022). Serrano, J. et al. High-dose saccharin supplementation does not induce gut microbiota changes or glucose intolerance in healthy humans and mice. & Blaak, E. Short-chain fatty acids in control of body weight and insulin sensitivity. Kalantar-Zadeh, K., Berean, K. J., Burgell, R. E., Muir, J. G. & Gibson, P. R. Intestinal gases: influence on gut disorders and the role of dietary manipulations. & Pimentel, M. The degree of breath methane production in IBS correlates with the severity of constipation. Bauditz, J., Norman, K., Biering, H., Lochs, H. & Pirlich, M. Severe weight loss caused by chewing gum. Liauw, S. & Saibil, F. Sorbitol: often forgotten cause of osmotic diarrhea. & Chey, W. A systematic review of the effects of polyols on gastrointestinal health and irritable bowel syndrome. & Appleton, K. M. The effects of low-calorie sweeteners on energy intake and body weight: a systematic review and meta-analyses of sustained intervention studies. Blackburn, G. L., Kanders, B. S., Lavin, P. T., Keller, S. D. & Whatley, J. Raben, A., Vasilaras, T. H., Moller, A. C. & Astrup, A. Sucrose compared with artificial sweeteners: different effects on ad libitum food intake and body weight after 10 wk of supplementation in overweight subjects. Associations of changes in reported and estimated protein and energy intake with changes in insulin resistance, glycated hemoglobin, and BMI during the PREVIEW lifestyle intervention study. Lewis, S. & Heaton, K. Stool form scale as a useful guide to intestinal transit time. Roumans, K. et al. Hepatic saturated fatty acid fraction is associated with de novo lipogenesis and hepatic insulin resistance. & Georga K. Composition Tables of Foods and Greek Dishes, 3rd edn (Parisianou Publications, 2004). Diepeveen-de Bruin, M. et al. Development and validation of a UPLC–MS/MS method for the quantification of sugars and non-nutritive sweeteners in human urine. The effects of water and non-nutritive sweetened beverages on weight loss during a 12-week weight loss treatment program. Christensen, P. et al. Men and women respond differently to rapid weight loss: metabolic outcomes of a multi-centre intervention study after a low-energy diet in 2500 overweight, individuals with pre-diabetes (PREVIEW). Callahan, B. J. et al. DADA2: high-resolution sample inference from Illumina amplicon data. The SILVA ribosomal RNA gene database project: improved data processing and web-based tools. Douglas, G. M. et al. PICRUSt2 for prediction of metagenome functions. R: A Language and Environment for Statistical Computing (R Foundation for Statistical Computing, 2018). Bisanz J. qiime2R: Importing QIIME2 artifacts and associated data into R sessions. Simpson G. & Oksanen J. ggvegan: ‘ggplot2' plots for the ‘vegan' package. & Couch S. broom: convert statistical objects into Tidy Tibbles, version 1.0.10; https://CRAN.R-project.org/package=broom (2023). Gu, Z., Eils, R. & Schlesner, M. Complex heatmaps reveal patterns and correlations in multidimensional genomic data. Neuwirth E. RColorBrewer: ColorBrewer palettes, version 1.1-3; https://CRAN.R-project.org/package=RColorBrewer (2022). & Zhang, X. LinDA: linear models for differential abundance analysis of microbiome compositional data. & Walker, S. Fitting linear mixed-effects models using lme4. Oksanen J. et al. vegan: community ecology package, version 2.7-1; https://CRAN.R-project.org/package=vegan (2022). & Holmes, S. phyloseq: an R package for reproducible interactive analysis and graphics of microbiome census data. Variation in extracellular matrix genes is associated with weight regain after weight loss in a sex-specific manner. & Wiener, M. Classification and regression by randomForest. Kuhn, M. Building predictive models in R using the caret package. Archer E. rfPermute: estimate permutation p-values for random forest importance metrics. The SWEET consortium thanks all participants for their time and commitment. Furthermore, we thank the scientific advisory board (I. Macdonald and D. Mela), all involved scientists (PhD students K. Apergi and E. Botsi), staff members (dieticians A. M. Raabyemagle, M. Hernandez Ruiz de Eguilaz, B. Martinez de Morentin, G. Castelnuovo and V. Karmiri; dietician secretary A. Karayannis; lab technicians S. Skov Frost, J. Guldborg Jørgensen, M. Zabala, V. Ciaurri, P. Vassilaki, M. Serbou, N. Konstantinakou and N. Spyrou; nurses S. Pérez Diez, P. Lemonia and V. Iro), data hub managers (L. Ove Dragsted, J. Stanstrup, K. Nowak and M. Bo Johansen), statistician C. Ritz and all students for their dedication and valuable contribution to the trial. The trial is funded by the Horizon 2020 programme: ‘Sweeteners and sweetness enhancers: Impact on health, obesity, safety and sustainability' (SWEET; grant no. 774293), covering salary for project personnel, supplies, remuneration and dissemination of results. The amount is deposited into a project account that is subject to audits and public review. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. These authors contributed equally: Michelle D. Pang, Louise Kjølbæk, Jacco J. A. J. Bastings, Sabina Stoffer Hjorth Andersen, Ellen E. Blaak, Anne Raben. Department of Human Biology, Institute of Nutrition and Translational Research in Metabolism (NUTRIM), Maastricht University Medical Centre+, Maastricht, The Netherlands Louise Kjølbæk, Sabina Stoffer Hjorth Andersen & Anne Raben Center for Nutrition Research, University of Navarra, Pamplona, Spain Navarra Institute for Health Research, Pamplona, Spain Food, Consumer Behaviour and Health Research Centre, School of Psychology, University of Surrey, Guildford, UK European Clinical Research Infrastructure Network, Paris, France International Reference Laboratory Services, Bioiatriki, Athens, Greece Department of Nutrition & Movement Sciences, NUTRIM, School of Nutrition and Translational Research in Metabolism, Maastricht University, Maastricht, The Netherlands Institute of Agri-food and Life Sciences, Hellenic Mediterranean University Research Centre, Heraklion, Greece Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar The SWEET EU-project was initiated by J.C.G.H., A.R. are principal investigators at the four intervention sites, where M.D.P., J.J.A.J.B., M.M.S., A.U., L.K., S.S.H.A., S.N.-C., K.R., T.C.A. were responsible for specific methods, platforms or analyses, and M.dA. is responsible for monitoring of the trial. Correspondence to Ellen E. Blaak or Anne Raben. has received honoraria from Nestlé, Unilever and the International Sweeteners Association and is currently employed by Novo Nordisk. have received project funds from the American Beverage Association. works for a company (NetUnion) that has no conflict of interest in the trial outcome. 's research centre provides consultancy to, and has received travel funds to present research results from, organizations supported by food and drink companies. The remaining authors declare no competing interests. Nature Metabolism thanks John Sievenpiper, Louis Aronne and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editors: Jean Nakhle and Ashley Castellanos Jankiewicz, in collaboration with the Nature Metabolism team Peer reviewer reports are available. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. a, Violin plots of body weight regain, fasting glucose, and HbA1c indices during the weight maintenance phase (CID4 – CID2), with participants in the Sugar and S&SEs groups classified as responders or non-responders. For body weight regain (left panel) and fasting glucose (middle panel), participants were split into five tertiles within both S&SEs and Sugar group. For HbA1c, participants with a shift above zero were considered non-responders; the rest were considered responders (right panel). Abbreviations: S&SEs, sweeteners and sweetness enhancers. Heat map of microbial genera (rows) CSS normalized and log2 transformed abundance per sample with significant different abundance trends over time between the intervention groups. Colored by intervention groups straight lines indicate the fit of a simple linear regression with corresponding 95% confidence intervals: red lines for the S&SEs group and blue lines for the Sugar group. The black lines signify the mean of each group, with dashed lines and squares denoting the S&SEs group and solid lines and dots for the Sugar group. Statistical importance of differences in trends between groups was tested with linear mixed effect models as implemented in LinDa and outcomes are indicated above. P-values adjusted using False Discovery Rate. (A) Receiver operating characteristic (ROC) curves based on Random Forest classification of responders and non-responders, (B) Dot plots of the Area under the curve (AUC) based on Random Forest classification of responders and non-responders Responder and non-responders were defined by changes in HbA1c, fasting glucose, or weight maintenance index (as indicated in grid row names), (A)Treatment group and CID are indicated in the grid column names (B) CID is indicated in the grid columns names and treatment group in the x-axis labels The colour of ROC curves or dots corresponds to the order of the response variable used to build the prediction model: green - original definition of the responders/non-responders; red – definition of the responders/non-responders is random. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. et al. Effect of sweeteners and sweetness enhancers on weight management and gut microbiota composition in individuals with overweight or obesity: the SWEET study. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/adventure/outdoor-gear/a68156721/prime-big-deal-days-pocket-knife-deals-2025/'>Pocket Knife Deals Are Hotter Than Ever During Prime Big Deal Days—Our Experts Recommend These Smart Buys</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 10:07:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>We may earn commission if you buy from a link. Amazon Big Deal Days only runs two days, and there is an overwhelming amount of sales to shop if you're getting a jump on holiday shopping (whether that's for others or for yourself). Amazon offers price cuts on pocket knives from our favorite blade brands, including Smith & Wesson, Civivi, Case, and more. I've scoured pocket knife deals and checked the specs against the knives we've tested and recommended to offer a selection that doesn't skimp on craftsmanship and save you more than just a couple of bucks. The Civivi Mini Axis is a popular EDC knife that's easy to sharpen and handles camping and outdoor pursuits just as well. Gerber's Paraframe is an absolute workhorse of a serrated blade for those who cut rope or fabric daily, and is now available for under $20. Personally, I love the CRKT CEO pocket knife: It's sleek, has lightning-fast deployment, and isn't much larger than a pen. Recently, he was coordinator of partnership content at another product journalism outlet. Prior to that, he was a buyer for an independent men's shop in Houston, Texas, where he learned all about what makes great products great. He enjoys thrifting for 90s Broadway tees and vintage pajama sets. His spare time is occupied by watching movies and running to impress strangers on Strava. What Kind of Steel Is Best For a Pocket Knife?</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-03196-0'>Stop treating code like an afterthought: record, share and value it</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 09:45:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). Roberto Di Cosmo is a computer scientist at INRIA Paris, France. Sabrina Granger is an open-science community manager at INRIA Lyon, France. Konrad Hinsen is a molecular biophysics researcher at the French National Centre for Scientific Research (CNRS) in Orléans, France. Nicolas Jullien is an economist at IMT Atlantique, Brest, France. Daniel Le Berre is a computer scientist at Artois University, Lens, France. Violaine Louvet is a research engineer in scientific computing at the Jean Kuntzmann Laboratory, Grenoble, France. Camille Maumet is a neuroinformatics researcher at INRIA, University of Rennes, France. Clémentine Maurice is a security researcher at CNRS, University of Lille, France. Raphaël Monat is a computer scientist at INRIA, University of Lille, France. Nicolas P. Rougier is a computational neuroscientist and open-source developer at INRIA, University of Bordeaux, France. From short scripts to vast simulations of Earth's climate, protein structures or even the cosmos, it is hard to imagine scientific research without software. Why it's worth making computational methods easy to use Why it's worth making computational methods easy to use Scholars, librarians, research institutions and funding agencies are wrestling with how to reconcile these two requirements. Recent efforts to do this1 have focused on adapting a set of principles initially developed2 to make research data findable, accessible, interoperable and reusable (FAIR). But this approach relies on tracking data, archiving them and making metadata available. Imagine, for example, maintaining a software package that has tens of contributors (which is not rare). Each release and version requires a new upload to an archive, with updates to the metadata, author list, dependencies (any other software required for programs to run), interoperability (which other programs it can work with) and more. Some programs have a weekly or even daily release cycle, making the FAIR approach impractical. Researchers must be able to publish a piece of software without the need for lengthy bureaucratic procedures to identify rights holders, choose an open licence and protect intellectual property. As researchers and engineers with expertise in software development in various scientific domains — ranging from computer science to neuroscience, physics and chemistry — we have recently proposed an approach called ‘CODE beyond FAIR' that outlines how software can be better handled, shared and maintained. Here, we outline recommendations for two groups: the scholars who develop software (see Supplementary information (SI), Table S1), and the research institutions, funders, libraries and publishers that use it (see SI, Table S2). Our recommendations are based on our collaborative experience in developing open-source software, but also draw from the work of free and open-source software (FOSS) communities. These have long experience of project governance, funding, recognizing individual contributions and training future contributors. Sharing the code that has been used to reach a paper's conclusions is important for research integrity and reproducibility, but practices vary widely among research communities. Permissive licences, which let others use and modify software with few restrictions, are increasingly common, particularly in computer science, mathematics and physics, yet most software is still not published at all. Platforms exist to share code, such as GitHub or GitLab, and to archive it, such as in the repositories Zenodo or Software Heritage, which can capture the whole history of a project3. All researchers should know how to share and deposit code. Finding the right level of expertise to ensure that researchers know how to document, share and archive code in their field is crucial. Why NASA and federal agencies are declaring this the Year of Open Science Why NASA and federal agencies are declaring this the Year of Open Science One way to improve matters is to train all PhD students from all scientific disciplines in the basics of software engineering during the first year or so of their postgraduate research careers. International training organizations exist that teach data and computational skills to wide audiences with basic or no knowledge of software development. For example, the Neuromatch Academy for global neuroscience education — co-founded by computational neuroscientists Dan Goodman and Konrad Körding during the COVID-19 pandemic4 — reported having supported more than 2,000 students from more than 100 countries through online training in 2024. And The Carpentries, founded in 1998 by Greg Wilson to improve the computational skills of researchers5, has facilitated or organized almost 4,800 workshops in more than 70 countries so far. These courses range from basic computational skills (such as in the programming languages Shell, R and Python, and version control) to advanced concepts (statistics and machine learning) as well as discipline-specific advanced skills. GitHub, now owned by Microsoft, has become the de facto international hub for code sharing. Institutions should support efforts to connect portals to ensure adequate cross-referencing between different projects and versions, such as those in the European Open Science Cloud research-support platform. Why it's worth making computational methods easy to use Why NASA and federal agencies are declaring this the Year of Open Science Customizable AI systems that anyone can adapt bring big opportunities — and even bigger risks AI is dreaming up millions of new materials. ‘Shake it off': Taylor Swift's changing voice shows how our accents evolve Journals infiltrated with ‘copycat' papers that can be written by AI Six questions to ask before jumping into a spreadsheet Seeking exceptional Senior/Junior PIs, Postdocs, and Core Specialists globally year-round The Department of Ophthalmology at the University of Texas Medical Branch (UTMB) in Galveston, Texas, invites applications for a research position ... Why it's worth making computational methods easy to use Why NASA and federal agencies are declaring this the Year of Open Science An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41467-025-64259-4'>Data-driven fine-grained region discovery in the mouse brain with transformers</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 09:09:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Spatial transcriptomics offers unique opportunities to define the spatial organization of tissues and organs, such as the mouse brain. We address a key bottleneck in the analysis of organ-scale spatial transcriptomic data by establishing a workflow for self-supervised spatial domain detection that is scalable to multimillion-cell datasets. This workflow uses a self-supervised framework for learning latent representations of tissue spatial domains or niches. We use an encoder-decoder architecture, which we named CellTransformer, to hierarchically learn higher-order tissue features from lower-level cellular and molecular statistical patterns. Coupling our representation learning workflow with minibatched GPU-accelerated clustering algorithms allows us to scale to multi-million cell MERFISH datasets where other methods cannot. CellTransformer is effective at integrating cells across tissue sections, identifying domains highly similar to ones in existing ontologies such as Allen Mouse Brain Common Coordinate Framework (CCF) while allowing discovery of hundreds of uncataloged areas with minimal loss of domain spatial coherence. CellTransformer domains recapitulate previous neuroanatomical studies of areas in the subiculum and superior colliculus and characterize putatively uncataloged subregions in subcortical areas, which currently lack subregion annotation. CellTransformer is also capable of domain discovery in whole-brain Slide-seqV2 datasets. Our workflows enable complex multi-animal analyses, achieving nearly perfect consistency of up to 100 spatial domains in a dataset of four individual mice with nine million cells across more than 200 tissue sections. CellTransformer advances the state of the art for spatial transcriptomics by providing a performant solution for the detection of fine-grained tissue domains from spatial transcriptomics data. Hierarchical spatial organization is ubiquitous in tissue and organ biology. Systematic, high-dimensional phenotypic measurements of this organization, generated through experimental tools such as spatial transcriptomics, multiplex immunofluorescence, and electron microscopy, are also becoming increasingly available as large, open datasets. However, transforming this abundance of data into a useful representation can be difficult, even for fields with a wealth of prior knowledge, such as neuroanatomy. Datasets such as the Allen Brain Cell Whole Mouse Brain (ABC-WMB) Atlas1,2,3, a multi-million cell single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics (MERFISH) atlas, provide unprecedented opportunities to investigate whether computational tools can help biologists understand spatial cellular and molecular organization. However, the size of these datasets presents computational challenges for existing methods. Existing methods for spatial niche or spatial domain detection often operate on the entire dataset at once, for example, a tissue-section-wide cell by gene matrix. This precludes scale-up to large multi-section datasets as most systems do not have the GPU memory required to load multiple sections of data or store intermediary representations such as pairwise distance matrices4,5,6, particularly as datasets scale into the millions or tens of millions of cells. Some methods rely on Gaussian processes, which feature a costly cubic computational scaling in the number of observations7. Other more scalable methods are limited in capturing granular structure, integration across tissue sections, or require significant neuroanatomical prior knowledge to manually audit, cluster, and hyperparameter tune for domain discovery workflows8,9,10. Our method, CellTransformer, implements a robust representation learning and clustering workflow to discover spatial niches at scale by representing not tissue sections but subgraphs that represent individual cellular neighborhoods. We describe an innovative strategy to induce the encoder of an encoder-decoder transformer to aggregate useful information into a neighborhood representation token. This occurs by training the model to condition cell-type specific gene expression predictions using this neighborhood context token. This representation allows for recovery of important anatomically plausible spatial domains while remaining computationally efficient. We evaluate CellTransformer on using the ABC-WMB dataset (3.9 million cells collected with a 500 gene MERFISH panel)1 demonstrating its effectiveness in producing completely data-driven spatial domains of the mouse brain by comparing the results to the Allen Mouse Brain Common Coordinate Framework version 3 (CCFv3)11. CellTransformer domains reproduce known regional architecture observed in targeted studies of the subiculum and in the superior colliculus superficial layers. Beyond the 670 regions currently annotated in ABC-WMB, we show our workflow produces meaningful data-driven domains in regions that currently lack subregion annotation. As examples, we focus on data-driven subdomains we define in superior colliculus and midbrain reticular nucleus. We also demonstrate CellTransformer's strength in integrating domains across animals, leveraging a separate whole-brain dataset within ABC-WMB12 comprising 6.5 million cells distributed across four animals and 239 sections and with a separate gene panel with 1129 genes. We find that CellTransformer produces consistent subregions across all 5 animals (1 coronal and 4 sagittal), suggesting a successful integration across animals with heterogeneous measurements. Notably, we also find that identified domains are highly consistent across animals. This work demonstrates that large-scale data-driven discovery of domains at CCF-like resolution can be based on spatial transcriptomics data. Finally, we show that our framework can perform domain detection in a different spatial transcriptomics modality, Slide-seqV2, using a whole-brain dataset of cellularly deconvoluted results13. CellTransformer is a graph transformer14 neural network that is trained to learn latent representations of cell neighborhoods by conditioning single-cell gene expression predictions on neighborhood spatial context. As input, our model requires the gene expression profiles and cell type classifications for cells in a neighborhood and outputs a latent variable representation for that neighborhood. One of the principal operations in a transformer is the self-attention operation, which computes a feature update based on pairwise interactions between elements in a sequence, which are referred to as tokens (here, cells). Accordingly, one interpretation of our model is of learning an arbitrary and dynamic pairwise interaction graph among cells. Restricting this graph to a small neighborhood subgraph of the whole-tissue-section graph has benefits for both computational resource usage and biological interpretability. Truncating neighborhoods using a fixed spatial threshold instead of choosing a fixed number of neighbors also allows the network to account for the varying density of cells in space. We designed a self-supervised training scheme requiring only cell-type labels, which many large-scale studies make available via scRNA-seq atlas reference mapping1,12. We model cellular neighborhoods as sets of cell tokens that are within a box of fixed size centered around a reference cell and use them to predict the observed gene expression of the cell at the center of the neighborhood. Cell tokens are generated by composing cell-type and gene expression information (Methods). After encoding with a series of transformer layers (where cells are only allowed to attend to each other if they are in the same neighborhood), these tokens are then aggregated using a learned pooling operation to produce a single token representation of the entire tissue context. The model receives a new mask token representing the reference cell's type, which is used to predict its gene expression following the operation of several transformer decoder layers (Fig. 1b; note that the point cloud displayed is for illustration purposes only). Importantly, during this process, only the mask token and the neighborhood representation can attend to each other. This operation captures a hierarchical encoding and decoding process where low-level information (gene and cell type) is produced at the cell token level and aggregated into a high-level representation. This high-level representation is then used to conduct the reverse decoding process (prediction of gene expression from cell type and tissue context information). However, unlike masked prediction models such as NCEM17, we aggregate information across tokens (cells) in a cellular neighborhood using a learned pooling, which strongly bottlenecks the information distributed across the tokens prior to masked cell prediction. a During training, a single cell is drawn (we denote this the reference cell, highlighted in red). We extract the reference cell's spatial neighbors and partition the group into a masked reference cell and its observed spatial neighbors. b Initially, the model encoder receives information about each cell and projects those features to d-dimensional latent variable space. Note that the latent variable point cloud shown is merely for illustration purposes and does not correspond to the actual CellTransformer latent space. Features interact across cells (tokens) through the self-attention mechanism. These per-cell representations and an extra token acting as a register token are then aggregated into a single vector representation, which we refer to as the neighborhood representation. This representation is concatenated to a mask token which is cell type-specific and chosen to represent the type of the reference cell. A shallow transformer decoder (dotted lines) further refines these representations and then a linear projection is used to output parameters of a negative binomial distribution modeling of the MERFISH probe counts for the reference cell. c Once the model is trained, we compute embeddings (one for each neighborhood/reference-cell pairing) and concatenate these embeddings within the tissue section datasets and across tissue sections. Concatenating embeddings across tissue sections produces region discovery at organ level. We then cluster these embeddings using k-means to discover tissue domains across sections. At test time, we extract this neighborhood representation for each cell and use k-means clustering to identify discrete spatial domains (Fig. We emphasize that the input embedding matrix for k-means is constructed by concatenating all cells across the dataset across tissue sections. Since minibatching is used during training (unlike methods such as STAligner and GraphST), for generating embeddings, and during k-means (using cuml for GPU-acceleration), overall computational costs of our algorithm are limited in principle only by the memory required for storage of cellular neighborhoods rather than entire sections or datasets. The ABC-WMB spatial transcriptomics dataset contains data from five mouse brains1,12. One animal was processed by the Allen Institute for Brain Science with a 500 gene MERFISH panel and 53 coronal sections (Yao et al.)1 The remaining four other animals, generated in Zhang et al.11. The other two animals in the dataset (“Zhuang 3”, 23 sections; and “Zhuang 4”, 3 sections) were sampled sagittally. We first trained CellTransformer on the Allen 1 dataset. We then clustered these embeddings using k-means. We emphasize that to generate spatial domains across the brain, all k-means clustering in this paper was performed by concatenating cells in the dataset across tissue sections. All further references to visualizations of domains, including those only visualized for a subset of domains, were fit at a given number of domains across the entire dataset. We observed an isolated case where a small number of spatial clusters only in the striatum displayed a non-homogenous spatial patterning that strongly resembled a predominantly astrocytic, Crym+ population described in Ollivier et al. (2024) and have similar gene expression patterns (particularly Crym expression) and spatial organization (See Supplementary Note 1 for a discussion on biological plausibility of these domains and the effect of smoothing, as well as an extensive series of visualizations of the domains at high resolution with and without smoothing). Despite this plausibility, we introduced an optional smoothing step (see Methods) to remove this population to concord with neuroanatomical convention, which typically analyzes spatially contiguous regions. We also visualized the first three principal components (~19.5% variance explained, Supplementary Fig. 2a–c), demonstrating the CellTransformer embeddings produce neuroanatomically reasonable spatial patterns. We generated domains at k = 25, 354, and 670, to match the division, structure, and substructure annotations' resolution in CCFv3, displaying domains for four consecutive tissue sections (Fig. We also provide representative images of spatial clusters across the brain (28/53 sections) at different k in Supplementary Figs. Low domain numbers such as k = 25 broadly divide the brain into neuroanatomically plausible patterns, with subregions of striatum (dorsal and ventral marked in Fig. A comparison of cortical layers across these sections shows that CellTransformer domains at k = 25 are well matched to CCF (Supplementary Fig. In particular, we point out the excellent correspondence of domains across tissue sections at k = 25 across the entire dataset (Supplementary Fig. This suggested that our neighborhood representation method was robust enough to enable integration without modeling of batch or tissue-level covariates. a Four sequential tissue sections (the inter-section distance is 200 µm) from anterior (first row, corresponding to section 50) to posterior (bottom row, section 47). In the first three columns, each dot is a cell, colored by spatial domain identified by CellTransformer when clustering was conducted with k = 25, 354, and 670 domains (the CCF division, structure, and substructure domain resolutions). Fourth column shows CCF region registration to the same tissue section. Select regions are annotated with CCF labels. b Single hemisphere images of same tissue sections in (a) domains fit at k = 670, zoomed in on cortical layers of motor cortex (MO) and somatosensory cortex (SS). CCF boundaries are shown in semi-transparent lines, with the boundary between SS and MO outlined in larger black dotted lines. d Proportion of discrete domains by model as compared to CCF at the same resolution. Single-cell level spatial smoothness was averaged over each domain instead of averaged over the entire dataset as in (c). For each CCF annotation level we computed a threshold as the 20th percentile of per-domain spatial smoothness values. e Average Pearson correlation (averaging over number of domains and method) of the maximum Pearson correlation between the cell type composition (at subclass level, 338 types) vectors of data-driven regions with CCF ones. f Average Pearson correlation (averaging over number of domains and method) of optimal matched pairs between data-driven and CCF regions, where CCF regions are only allowed to pair with one data-driven region per comparison. g Region-by-region Pearson correlation matrix comparing cell type composition vectors from 670 CCF regions (at substructure level) with 670 spatial domains from CellTransformer. i Cell type (cluster level) by region matrix for 670 CellTransformer regions. Colors along x-axis in both (g, h) show cell class annotations from ABC-WMB cell type taxonomy at class level to allow for visualization of composition in terms of known types. Rows of both (h, i) are grouped using clustering to produce approximately similar structure. At k = 354, anterior-posterior subdivisions emerge such as the presence of layer 4 in the motor cortex (Fig. Historically, the mouse motor cortex was thought to lack a granular layer 418, however recently, MERFISH, transcriptomic and epigenomic studies have confirmed its existence1,18,19. At k = 100 and k = 354, we find a domain corresponding to Layer 4 in the somatosensory cortex which clearly extends to layer 4 in the motor cortex. We also provide a UMAP visualization of the CellTransformer embedding space with cluster labels from k = 354 clustering (Supplementary Fig. 7), demonstrating the observed domain spatial organization can also be recovered in a low-dimensional representation. At k = 670, the cortical layers identified at lower resolution are further partitioned into superficial, intermediate, and deep strata within several layers. We visualize cortical layers across sections in depth (Fig. 2b), showing CellTransformer not only identifies fine superficial-deep structure within cortical layers but also preserves the boundary between somatosensory and motor cortex (marked in thick black dotted lines in Fig. Taken together these results showed that CellTransformer robustly describes previously known anatomical structures. We also examined the caudoputamen at various choices of k. At k = 25, the caudoputamen is one domain, which separates into broad spatially contiguous domains at k = 100. 8) that strongly resembles the Voronoi parcellation established in Hintiryan et al.20 through systematic projection mapping to caudoputamen. 8), which Hintiryan et al. attributed to the differences in subnetwork reorganization. The correspondence of our transcriptomic domains to the Hintiryan et al. results, which are exclusively based on projection mapping (non-transcriptomic data), suggests the biological relevance of our representation learning workflow. We compared CellTransformer to several other workflows to capture spatial coherency and multiresolution neuroanatomical annotations in CCF at the division, structure, and substructure levels. For comparison, we used two recent methods, CellCharter21 and SPIRAL22 that are scalable to millions of cells as benchmarks. CellCharter builds spatially informed embeddings for domain detection by concatenating the embeddings across scales, followed by dimensionality reduction and batch correction, while SPIRAL uses graph neural networks for batch effect correction and integration across scales. Additionally, we implemented two machine learning baselines. Gene-expression based domain detection is employed for spatial transcriptomics data analysis and has been shown to be highly successful for brain parecllation23,24,25. Therefore, we conducted k-means clustering on the single-cell MERFISH probe counts. We also employed k-means clustering on cellular neighborhoods (represented as cell type count vectors). Many other GPU-accelerated methods, such as scENVI4, STACI26, spaGCN5, STAligner6, STAGATE27, or GraphST28, cannot be run on datasets that contain millions of cells due to computational constraints (see Methods). One reason is that several of these methods require the instantiation of a dataset-wide pairwise distance matrix between all cells either on GPU or in RAM, which is a prohibitively large matrix (~60TB for ~4 M cells). For each cell, we identified its nearest 100 spatial neighbors. Ideally, we would expect a high proportion of neighbor cells to be in the same spatial domain as the starting cell. In this comparison of neighborhood spatial smoothness, CellTransformer outperforms CellCharter (58.2% better spatial coherence at 670 domains) and SPIRAL (4091.2%). CellTransformer also outperforms the machine learning baseline based on k-means clustering on cellular neighborhoods (61.9% better spatial coherence at k = 670), as well as the gene expression baseline (419.2% better spatial coherence at k = 670). We also compared versions of CellTransformer domains with and without the optional smoothing step. Both versions perform very similarly, indicating the CellTransformer representation efficiently suppresses high-frequency spatial features and changes relatively little after smoothing with a small bandwidth. For reference, we include the CCF parcellation (dashed purple line) in this comparison to provide an upper bound. To understand the impact of this choice, we trained two additional variants of CellTransformer, one without cell type information in the decoder and one without cell type information in the encoder and the decoder (only expression). We generated spatial domains from these models' embeddings with smoothing, performed identically as in the base model. These variants perform competitively with the base CellTransformer version. The base model domains were 3.0% more similar to CCF than the model without cell type decoding, and 5.4% more similar than the model without cell type in either the encoder or decoder (Supplementary Fig. The smoothed and unsmoothed CellTransformer domains were also very similarly spatially smooth (Supplementary Fig. 9b), even when the number of domains was extended to 2000. CellTransformer variants also produced relatively little decrease in spatial smoothness at greater than 1000 domains, whereas CellCharter smoothness sharply declined. Taken together, these results indicate that cell type conditioning is an important component of the model that increases similarity to CCF and spatial uniformity. This may be because the cell types, which were fit using whole-transcriptome scRNA-seq profiling, implicitly comprise a whole-transcriptome imputation step. However, we also note that the CellTransformer variant without any cell type information still performs better than CellCharter with respect to similarity to CCF and spatial coherence. In addition to dataset-wide spatial smoothness, we also quantified the proportion of discrete domains (Fig. To classify a given domain as spatially discrete, we first averaged the single-cell level smoothness values over domains. We then investigated the distribution of per-domain average smoothness (Supplementary Fig. CellTransformer's regional smoothness distribution is more similar to CCF than CellCharter, with relatively more highly smooth regions. Additionally, CellTransformer domains with smoothing are more similar to CCF than without. For a given CCF annotation resolution, we then computed a threshold based on the 20th percentile of per-domain averaged CCF smoothness values, which we used to classify a given data-driven domain as discrete or not. CellTransformer domains fit with and without smoothing both perform well, while for CellCharter, SPIRAL, and the gene expression baselines, discreteness declines significantly at 354 and 670 domains (Fig. We also computed the proportion of spatially discrete domains for resolutions from 700 to 2000 data-driven domains using the 20th percentile cutoff from the k = 670 CCF annotation level. For CellCharter, the proportion of discrete domains significantly diminishes (Supplementary Fig. The unsmoothed CellTransformer embedding workflow was most performant; we reasoned that the isotropic Gaussian smoothing employed may have eroded fine laminar boundaries, despite removing the isolated non-uniform domains discovered in striatum (Supplementary Note 1). We visualized a series of sections comparing domains from the smoothed and unsmoothed CellTransformer embeddings, finding a slight erosion of fine lamina in the cortex (Supplementary Fig. To quantify the similarity of detected domains with CCF annotations, we compared the cell type composition of domains using cell type calls from the ABC-WMB taxonomy. We again chose the subclass cell type level, extracting for each domain and for each method a 338-long cell-type vector. We calculated the Pearson correlation of cell type composition vectors computed using the CCF regional annotations at division (25), structure (354) and substructure (670) levels against those of the various methods at the corresponding number of spatial domains. First, for each data-driven domain, we computed the maximum correlation to any CCF domain at the same CCF annotation resolution averaging these numbers across domains. CellTransformer outperforms other methods at mid-granularity and fine-granularity (Fig. In this comparison, several data-driven regions can match the same CCF region, which in the worst case could provide an overly optimistic picture of the correspondence between data-driven domains and CCF. To address this, we conducted a second analysis where only one CCF region could be matched to a given data-driven one. We used linear programming to compute an optimal 1:1 pairing of data-driven regions to CCF ones based on their Pearson correlation. We then averaged correlations across regions (Fig. CellTransformer is highly performant, showing that increase in correlation is not due to redundant matches to a single area in CCF. Visualization of spatial clusters from CellCharter (Supplementary Figs. 13–14) at k = 670 domains across the brain and in midbrain shows lack of spatial coherence in cortical layers and midbrain, with detected domains distributed in a what appear to be non-biological patterns. This is particularly evident in thalamus and across cortical layers. In contrast, CellTransformer identified spatially coherent domains and uncovered plausible neuroanatomical structures. Next, we directly compared CCF annotation domains with data-driven domains using normalized mutual information (NMI) and the adjusted Rand index (ARI) (Supplementary Fig. Registering spatial transcriptomics data to CCF's dense MRIs is significantly challenging. We therefore attribute the overall low magnitude of the ARI and NMI, to potential errors in registration. To further characterize the similarity of CellTransformer domains with CCF, we plotted the Pearson correlation matrix (Fig. 2g) between cell type composition vectors generated at 670 domains (substructure level in CCF). Block structures with very high correlations (>0.9, shown in bright green) in the matrix clearly show that CellTransformer is able to identify regions that are highly similar with known ones without any labels. We also investigated the correspondence of cell type composition with more granular single-cell annotations, employing the lowest-level single-cell annotations (the “cluster” level, with 5274 cell types, as opposed to the “subclass” level with 338) from ABC-WMB. We observed high similarity between the “substructure” CCF domain set (Fig. This shows that the high correspondence of CCF and CellTransformer is robust to cell type resolution at which comparison occurs. CellTransformer identified an increase in the number of domains containing the 09 CNU-LGE GABA class (striatal/pallidal GABAergic neurons from lateral ganglionic eminence compared with the 670 CCF substructures, shown in light purple box in Fig. 2h, i), potentially suggesting the presence of uncharacterized developmental populations. We implemented a previously published strategy29 for Drosophila embryonic spatial gene expression and30 for 3D spatial gene expression in the adult mouse brain to determine the optimal number of domains using a stability criterion. We reasoned that the optimal choice of spatial domain number would feature minimal variability across clustering runs. In brief, we computed 20 clustering instances with different random initializations for a large range k values (100–2000) and quantified their variability over these initializations (see Methods). Interestingly, stability increased with increasing k (Supplementary Fig. To facilitate the choice of a particular resolution for analysis, we also computed the inertia (sum of squared errors in embedding space) for each clustering solution. Low stability at small numbers of domains may partially explain subpar results for CellTransformer in the k = 25 CCF evaluations. We averaged the inertia curve and instability and computed the point of second derivative crossing to identify k = 1300 as our resolution for analysis (crossing point shown with red dot in Supplementary Fig. CellTransformer is the only method out of the three deep learning methods we implemented (including six other pipelines, which were unable to cope with the size of the ABC-WMB dataset) to allow discovery of spatially coherent divisions at greater than CCF resolution. This study shows that spatial transcriptomics data can be used to identify brain regions at resolutions finer than previously defined in the CCF. CellTransformer domains are more spatially smooth than comparator deep learning methods, as well as clustering on either gene expression or cell type composition, both at the single-cell level and at the domain level. Additionally, CellTransformer domains are more similar to CCF, both at the single-cell level, using NMI and ARI, and at the regional composition level. CellTransformer can also be used with and without cell type labels, although performance is unsurprisingly better when using cell type information. Crucially, CellTransformer is highly performant at discovering a high number of domains, whereas methods such as CellCharter experience a significant loss of spatial coherence (Supplementary Fig. Encouraged by these findings, we next sought to establish correspondence of particular domains at k = 1300 to known neuroanatomy. We focused on this area because it is well characterized with respect to both connectivity31 and transcriptomic composition32,33. These structures were investigated in Ding et al.34, where the authors performed consensus clustering of glutamatergic neurons and subsequent ISH experiments were used to comprehensively map domains in dorsal subiculum (SUBd) and dorsal and ventral prosubiculum (PSd and PSv). We qualitatively compared spatial domains discovered by CellTransformer with k = 1300 to the anatomical borders identified in Ding et al. (Fig. Figure 3a shows a diagram of SUB and PS regions based on Ding et al. with the pyramidal and polymorphic layers of SUB and PS annotated in bold black text. Figure 3b shows discovered spatial domains at k = 1300 across four sequential sections corresponding to those in Ding et al.34. A subset of domains corresponding to SUB and PS are shown in Fig. CellTransformer identifies a three-layer organization in the dorsal subiculum corresponding to that in Ding et al. labeled SUBd-py (light green), SUBd-po (gold), and SUBd-mo (gray-blue). CellTransformer also correctly splits the SUBd and PSd shown with black dotted lines on the image of section 32. Three-layer strata are also observed in PSd, although notably the pyramidal layer domain extends caudally, consistent with transcriptomic studies31,32,33 of SUB architecture. For instance, our PSd-po domains (sections 31 and 30) strongly resemble the HGEA layer 4 found in Bienkowski et al.31. Note that differences may arise between panels in Fig. 3a, c because of sectioning variability and lack of exact match between sections in ABC-WMB and the Ding et al. study. In addition to the aforementioned regions we also observe high agreement in areas such as in the hippocampus-amygdaloid transition area (HA) and ventral prosubiculum (PSv). a Representative images reproduced from Ding et al. of region boundaries in prosubiculum (PS), subiculum (SUB), and hippocampal-amygdala (HA) particularly along the dorsal-ventral axis. b Images from hippocampal formation across 4 sequential tissue sections (anterior to posterior) roughly aligned to sections presented in Ding et al. Each dot is a cell colored using domain labels with k = 1300. c Same as (b) but only showing cells inside PS, SUB, and HA. Putative regional annotations are indicated and grouped by dorsal or ventral region within PS and SUB. d Gene expression patterns visualized at the corresponding tissue section, where only cells within PS/SUB/HA are shown. e Gene expression heatmap of identified subregions, with putative anatomical annotation. Dendrogram from hierarchical clustering in gene expression space is shown to the right. Genes visualized in (d) are bolded and denoted with a pink asterisk. Two genes per domain are shown and each gene is expressed with at least log-fold change greater than 1 relative to the other domains. Ding et al.34 observed differential projection topology in dorsal subiculum versus ventral prosubiculum. Correspondingly, genes were found to form opposing gradients across the length of subicular areas. Since CellTransformer domains appeared to correspond well with literature results, we explored gene expression patterns across domains to verify whether dorsal-ventral and medial-laterally varying gene patterns could be observed. We conducted differential expression analysis across our subicular domains (Fig. Many genes expressed in SUB and PS traverse their long axis as reported previously32. The identification of spatial domains that subdivided specific layers of PS and SUB, similar to the results in Ding et al. and featured similar types of gene expression gradients as existing literature, suggests that our pipeline was successful in learning neuroanatomically useful information. Importantly, while results in Ding et al. and related works were enabled by significant neuroanatomical and experimental expertise, CellTransformer allows identification of granular tissue structure in a data-driven fashion. Encouraged by this result, we continued our investigation of CellTransformer correspondence with known literature with a comparison in superior colliculus. Recent studies using systematic mapping of cortico-tectal fibers in the superior colliculus (SC) have identified distinct laminar and columnar structure38, suggestive of the complex role SC plays in the integration of sensory information and the coordination of signals. Therefore, SC presented an excellent opportunity to identify transcriptomic and cellular correlates of connectomic variation. We observed a strong correspondence of three of our spatial clusters (k = 1300) in the Allen 1 dataset with known layers of superior colliculus, sensory area, particularly the zonal (zo), superficial gray (sg), and optic (op) layers across a set of tissue sections spanning ~600 µm from anterior to posterior (rows of Fig. CellCharter was unable to identify these structures (Supplementary Fig. 14) at k = 670 (demonstrating a significant drop in spatial coherence at higher k) and only identifies two layers in SC, which does not conform with existing understanding. a Putative subregions of sensory layers of superior colliculus in tissue section 32 identified with k = 1300 CellTransformer domains. CCF registration is in the first column, with zonal (zo), superficial gray (sg), and optic (op) layers labeled by the color of their CellTransformer domain in third, fourth, and fifth columns. The second column shows all cells with color labels from their spatial domain from CellTransformer at k = 1300. The third, fourth, and fifth columns show the putative zo (gray-green), sg (purple), and op (red) domains. These columns also show the spatial distribution of one supertype level cell type in yellow across the section. b Sequential tissue sections (32: anterior, 31: posterior) shown similarly to (a) but visualizing subregions of the intermediate gray and intermediate white layers, which are indicated with black arrows in the CCF registered annotation image. c Proportions of different supertype level cell types for top-ten most abundant types in different spatial domains. Colors refer to the same spatial domain label in (a, b). Cell types visualized in (b) are denoted with a yellow asterisk. e Number of unique cell types (at supertype level) found in each domain, grouped by neurotransmitter class. By visualizing the cell type composition within the top-ten most abundant types for these three spatial domains (Supplementary Fig. 17a, b), we were able to identify cell types that were highly selective for our data-driven SC layers: types 0873 SCsg Gabrr2 Gaba_2, 0861 SCs Pax7 Nfia Gaba_3, and 0788 SCop Sln Glut_1. Crucially, the cell types, which have already been annotated as being associated with one of the zonal, optic, or superficial gray, are identified automatically by CellTransformer. We chose the supertype level to allow inspection of abundant cell types without being difficult to visualize. Supertype-level visualizations also show that even with granular cell types (1201 types in Yao et al.) CellTransformer domains are often marked by spatially specific cell type patterning; we note that we do not filter cells outside of our putative superior colliculus layers for visualization. Next we visualized the percentage of cells in each domain (Supplementary Fig. 17c), grouping them by neurotransmitter class (GABA-ergic, glutamatergic, and non-neuronal). To further explore these relationships, we calculated the number of distinct cell types (supertype level) within each neurotransmitter class and domain. A clear dorsal-ventral organization was evident (Supplementary Fig. 17d) with the number of GABA-ergic and glutamatergic neuron types increasing with layer depth, suggesting CellTransformer's ability in capturing complex patterns of cellular spatial organization. Encouraged by these findings, we also investigated subregions of the intermediate gray and intermediate white areas of the motor-related areas in SC (Fig. 4a, b), where we identify consistent regions across two consecutive sections that are not annotated in the CCF (rows of Fig. We define subregions of intermediate gray (ig) and white (iw), noting a medial-lateral structure similar to that in Benavidez et al.38, which exhaustively cataloged projection zones in superior colliculus. Notably unlike in superior colliculus sensory, a significant number of non-neuronal cell types are found in very similar proportions across the intermediate white and gray layers (Fig. 4c), and instead differences in regions may be attributable to varying proportions of rare cell types. Encouragingly, even in these fine-grained areas, cell types that are highly specific for our data-driven layers can be readily identified (columns of Fig. The identification of Pitx2-expressing neurons also supports our assertions that CellTransformer identifies biologically relevant domains, with previous studies using Pitx2 expression specifically as an intermediate layer marker in superior colliculus19,35. We observed complex cell type abundance gradients when visualizing the percentage of cells in a given domain by their neurotransmitter type (Fig. We used supertype level to confirm that spatially-varying cell distribution patterns persisted when using more granular cell type annotations. Lateral domains such as intermediate gray, lateral (shown in dark blue) and intermediate white, lateral (shown in light blue) featured a smaller proportion of GABA-ergic neurons than medial domains but were enriched for glutamatergic neurons and non-neurons (Fig. Next, we investigated the midbrain reticular nucleus (MRN), a subcortical structure with few anatomical annotations in CCF. MRN is highly enriched for interneurons and appears to play a complex role in movement initiation and release36,37. CellTransformer identifies four subregions of the MRN, which are not included in the existing CCF annotation (Fig. Plotting cell type proportions across the MRN, we identified cell types that are enriched for these putatively uncharacterized areas, although all domains were predominantly glial (e.g., 1184 MOL NN_4 supertype is abundant in all regions, Fig. Interestingly, several neuronal types found in these subregions were originally annotated as belonging in inferior colliculus (i.e., 0811 IC Six3 En2 Gaba_5 and 0809 IC En2 Gaba_3)1. However, the fine-grained region derivation by CellTransformer prompted us to perform additional visual inspection (see CCF annotation of MRN identified in Fig. 5a with black arrows) of the MERFISH data and conclude that these neurons are clearly located in MRN. In addition, by visualizing differentially expressed genes across the domains (Fig. Hierarchical clustering showed that the two dorsal domains (purple and brown) group together with the two ventral ones (gold and gray). a Sequential tissue sections (26 and 25, anterior to posterior). Note that registration is not exact and can differ across hemispheres. Second column: all cells in field of view, with color from spatial domains determined with k = 1300. b Supertype level cell type proportions for top fifteen most abundant types across the MRN subregions. Cell types visualized in (a) are denoted with green asterisks. c Selected 4 differentially expressed genes across regions. Each gene is expressed at least log fold change greater than 1 relative to the other domains. MERFISH probe distributions for select genes indicated with pink asterisks are shown in (d). d Gene expression gradients across tissue section 25 for Bnc2, Six3, and Pax5, showing specificity for each of the putative MRN subregions. Intensity of color is 0–1 normalized after log scaling raw probe counts. We show only cells within the subregions to make it visually easier to distinguish the relevant cells. e Bar plot of the percentage of cells for a given neurotransmitter type found in each domain, (GABA GABAergic, Glut Glutamatergic, NN non-neuronal). f Number of unique cell types (at supertype level) found in each domain, grouped by neurotransmitter class. We again visualized the neurotransmitter composition and the number of unique cell types of given neurotransmitter classes. 5e) featured the highest proportion of glutamatergic neurons, and the proportion of glutamatergic neurons in MRN domains decreased with increasing depth. Interestingly, MRN domains composed of a higher proportion of glutamatergic neurons were also the ones with the greatest number of glutamatergic neuron types, also following the dorsal-ventral gradient (Pearson correlation r = 0.89). This relationship was observed for nonneuronal cells (Pearson correlation r = 0.81, Fig. 5f), but not for GABA-ergic neurons (Pearson correlation r = −0.64). This suggests that CellTransformer can identify plausible structures even in historically difficult to characterize areas. In order to investigate CellTransformer's ability to integrate across animals, we trained a new model from scratch on the Zhang et al.12 MERFISH data, which uses an 1129 gene panel and is split over four animals, with both coronal (Zhuang 1 and 2) and sagittal sections (Zhuang 3 and 4). We computed embeddings for each neighborhood as in the previous analysis and performed k-means clustering, concatenating representations for all mice and sections. This provided an opportunity to examine whether CellTransformer could adapt to a multi-animal case in addition to finding spatial domains across tissue sections of the same animal. Spatial domains in sequential tissue sections appeared highly concordant across all four mice (Fig. We used 50 domains to facilitate clear visualization of the domains across animals with relatively few colors. Coronal and sagittal sections across mice clearly corresponded anatomically. Cortical layers were highly consistent across animal and section orientation. Despite a relatively low number of cells in mouse 4 (162,579 cells versus more than 1.0 million for each of the other animals), nearly all spatial domains observed for Zhuang 4 are present in other animals. a Representative images of all four mice arranged by column. Note that Zhuang 4 only had three sagittal sections, which only correspond to one hemisphere of the brain. For each image, each dot is a cell neighborhood and colors come from a spatial clustering with k = 50 (number of CCF regions at structure level), fit by concatenating embeddings across mice. b Quantification of number of per-mouse specific spatial clusters, computed by clustering at different k and computing the number of clusters found for all mice (4 animals) and for the three mice with the most cells per mouse (Zhuang 1, 2, and 3). Note that because serial sections were collected at a higher frequency (100 µm versus 200 µm), different areas of the brain will have marginally higher coverage in one brain or another. This is particularly relevant for Zhuang 4, for which sections only correspond to one hemisphere of the brain and therefore will have limited correspondence with domains in the other animals, which either cover the entire brain anterior-posterior (Zhuang 1 and 2) or across the sagittal axis (Zhuang 3). In contrast, Zhuang 3 does not fully cover a single hemisphere. c Average correlation of the cell type composition of brain regions computed CellTransformer to CCF regions, computed using the linear-sum assignment matching algorithm (exclusively matching regions from one set to the other). Dotted lines with “o” marker indicate results when fitting using all three mice with >1 M cells together. Solid lines with “x” marker indicate results when computing spatial clustering on each mouse in isolation. d Quantification of subject-level information present in embeddings using linear regression. The median absolute prediction error (x-axis) quantifies accuracy in predicting the (x, y, z) coordinates of a neighborhood from its embeddings. The y-axis quantifies accuracy when predicting mouse identity from embeddings using logistic regression. Values are averaged across cells per mouse. e Results of domain discovery (k = 50) on a Slide-SeqV2 whole mouse brain (Macosko 1) dataset13. Two sets of three sequential sections are shown. We quantified the robustness of CellTransformer domains in a multi-animal context across and within Zhuang 1-4 datasets. We ran clustering and identified domains at the three values of k: 25, 333, 630. These k values correspond to three CCF resolution levels reported by registration in Zhang et al. (note the number of domains differs due to registration differences). For each k value, we counted the number of domains observed in all four animals. We also repeated this analysis without data for Zhuang 4, which contains far fewer cells than the datasets from other animals (Fig. To verify that domain consistency across animals was not related to loss of domain spatial coherence, we repeated the neighborhood smoothness analysis we developed for analysis of the Allen 1 dataset on the combined Zhang et al. data. Spatial smoothness was similar to that of Allen 1 (Supplementary Fig. 18), indicating CellTransformer can discover spatially coherent domains that are robustly integrated across animals. We next quantified the similarity of CellTransformer domains to CCF regions. Similarly to our analysis of the Yao et al. dataset, we computed average similarity of cell type composition vectors from CCF and CellTransformer. In domain discovery across all animals, we found cell-type composition vectors that correspond strongly to CCF (Pearson correlation = 0.805, red line in Fig. We also evaluated whether clustering only on embeddings from one animal would significantly affect similarity to CCF. Correspondence between CellTransformer domains and CCF is high even when domains are fit with a subset of the dataset (Pearson correlations >0.7 for all comparisons across resolutions and domain source, Fig. This demonstrates CellTransformer can reproduce a consistent neuroanatomical structure even with a small number of observations. Results were highly similar overall to CCF in the Zhang dataset and Yao dataset (Pearson correlations greater than 0.6 for all comparisons), indicating CellTransformer's robustness to changes in gene panel and preprocessing choices. To further investigate how donor metadata was encoded in the embeddings, we employed linear probing strategies commonly used in interpretation of deep learning model embeddings. We regressed CCF-registered (x, y, z) coordinate position across all embeddings and used logistic regression to classify animal identities. Neighborhood level prediction of donor identity was very accurate (>94% for all animals, Fig. 6d) and median absolute prediction error was accurate within 151 µm. The observation that mouse donor identity is easily predicted from per-neighborhood embeddings while still maintaining cross-animal and cross-section coherence is another demonstration of the richness of the representation learned by our approach. Finally to demonstrate the applicability of our strategy to a different spatial transcriptomics modality, we analyzed a whole mouse brain Slide-SeqV2 dataset (“Macosko 1”), collected in Langlieb et al.13. Slide-SeqV2 provides whole transcriptome coverage in a spatial context by tiling tissue slices with 10 µm by 10 µm squares. As each square may contain more than one cell or a partial cell, we fit our model to the deconvoluted single-cell data computed using the RCTD39 method provided. We also filtered the dataset for low-quality cells and infrequently expressed genes (see Methods). We found that increasing the size of our model (from 4 encoder layers to 10) was necessary to identify spatially coherent domains, perhaps driven by the much larger number of genes detected (5019 versus 500 or 1129 in the two MERFISH datasets; see Methods). We plot three sequential sections from domain discovery at k = 50 (Fig. We show that CellTransformer robustly identifies cortical layers across sections and in known structures such as the midbrain and the piriform areas. Domain discovery with greater than 50 regions did not produce adequate integration across sections, possibly because of variable cellular density and single-cell read depth across sections. In this study, we present a transformer-based pipeline to combine scRNA-seq and spatially resolved transcriptomic atlases to perform accurate organ-level domain discovery. We developed a custom representation learning workflow and implemented a computationally efficient pipeline that readily allows scaling to multi-million cell, multi-animal datasets. The representations learned in our model can be clustered to identify progressively finer-scale spatial domains directly from local cellular and molecular information alone, without predefined spatial labels. We show these regions can be interpreted at the gene or cell level and recapitulate a variety of existing findings in the neuroscience literature, where many existing methods cannot. Our pipeline allows the extraction of a very high number of domains, which retain high correspondence to the existing brain region ontology (CCF). These domains are also highly spatially consistent both within and across tissue sections and even over multiple animals. Not only can CellTransformer discover this fine structure, but it can reliably find it across animals, even with hundreds of regions. This capability is intrinsic to our model and is learned despite any conditional modeling for donor or section-level covariates, indicating the robustness of learned features. We note that our objective is not to suggest that the domains discovered by our method are a definitive, normative set of brain regions, and we did not conduct any further validation studies to support these claims. Nor is it to strongly assert that the brain is composed of discrete brain regions, as opposed to a composition of gene expression gradients. Our objective was to develop a tool that would operationally identify plausible brain structures and substructures in a data-driven way and to satisfy a neuroanatomical convention for discrete domains. To support this goal, we used k-means clustering due to its capability to be mini-batched during fitting and because it produced biologically plausible domains; a probabilistic framework such as Gaussian mixture modeling may also have worked well, and possibly would have produced similarly plausible domains. Indeed, the fact that no finite number of domains was identified via our stability criterion suggests the possibility of a significantly larger number of domains. We demonstrated the robustness of our model in uncovering biologically relevant models and characterized our pipeline's ability to reproduce known neuroanatomy in the hippocampal formation and superior colliculus. Detected domains were concordant with previous comprehensive transcriptomic and connectivity studies of these areas, but were identified in a scalable and data-driven way. Not only were we able to detect known and novel regions, we found that CellTransformer domains can recapitulate and extend known spatial cell type enrichment patterns and gene expression gradients. We highlight several advantages of our architecture and approach. Although the use of graph-structured architecture or self-attention to model cells in a neighborhood graph is not novel17,27,40, our approach is distinguished by a self-supervised training objective based on spatial correlation between a cell and its neighbors that facilitates learning of a fixed representation for a cellular neighborhood (Supplementary Note 2). The intersection of graph neural networks, transformers, and representation learning research is a rich and rapidly moving research area. Methods for spatial-graph structured data such as CellTransformer will benefit immensely from implementing more effective ways of encoding the data and its metadata such as better position encoding mechanisms41, rotationally-invariant architectures42, or arbitrary numbers of genes43. There are also significant opportunities to extend CellTransformer's local representation framework to include other data modalities. Using a transformer rather than a graph neural network facilitates the inclusion of arbitrary contextual data, such as cell-level (e.g., neurophysiology44,45) and pixel-level data (e.g., mesoscale axonal connectivity46, or magnetic resonance imaging47), which can be tokenized and included in our framework. The stability of detected spatial domains at a given radius or k poses an interesting future angle from which to study anatomical hierarchies in the brain. Users must also have access to GPUs (to allow for timely model fitting), which reduces overall accessibility, although the hardware requirements are still much less intensive than for many existing models such as spaGCN and scENVI4. CellTransformer advances the state of the art for automated domain detection by facilitating the identification of granular and biologically relevant spatial domains that are extensible to very large, multi-animal spatial transcriptomic datasets. As spatially resolved transcriptomic and multi-omics studies of the brain become more prevalent, tools such as CellTransformer provide avenues to transform data into refined anatomical maps of the brain and other complex organs and pave the way towards tissue-level structure-function mapping. We downloaded the log-transformed MERFISH probe counts and metadata for the Allen Institute for Brain Science animal (“Allen 1”) from the Allen Institute public release (https://alleninstitute.github.io/abc_atlas_access/intro.html) access for ABC-WMB. The Allen 1 dataset is composed of 53 coronal sections. Serial sections were collected at 200 μm intervals. We transformed the (x, y) coordinates of each cell into microns instead of mm as provided. Otherwise, the dataset was used as-is for neural network training. Serial sections for Zhuang 1 (female) were collected at 100 μm intervals, while serial sections for the other animals (all male) were collected at 200 μm intervals. We transformed the (x, y) coordinates of each cell into microns instead of mm as provided. Otherwise, the data were used as-is for neural network training. We consider cells in the same neighborhood as a reference cell if the distance between them is within a box of fixed size. For all MERFISH datasets, we used a box width of 85 μm. We construct a CellTransformer to generate a latent representation from a cellular neighborhood where this representation is composed of both molecular and cell type information. We represent cells as nodes in an undirected graph, \(G=(V,E)\) where \(V\) indexes the nodes in the graph (cells) and we add an edge \(({x}_{i},{x}_{j})\) to the edge set \(E\) if \({r > d}_{i,j\,}\), with \(r\) a user-specified distance in microns. We assume also that for each node we have access to \({x}_{i}\in {R}^{g}\), a \(g\)-dimensional vector of MERFISH probe or cell deconvoluted transcript counts. We also assume we are given class labels \({{\bf{c}}}={\left\{{c}_{i}\in \left\{1,\ldots,C\right\}\right\}}_{i=1}^{N}\) for each of the \(N\) cells. The user must also specify an embedding dimension and number of transformer encoder and decoder layers; in all experiments in this paper, using MERFISH data, we use an embedding dimension of 384, 4 encoder layers, and 4 decoder layers. Its first degree neighbors are extracted from \(G\). We first apply a shallow encoder (two layer perceptron with GELU nonlinearity) function \({f}_{\theta }:{R}^{g}\to {R}^{192}\) which maps the gene expression into embedding space. We likewise construct and apply the function \({g}_{\theta }:{R}^{C}\to {R}^{192}\) to map one-hot encoded cell type labels to an embedding space, here a simple lookup into a learnable embedding table. These representations are concatenated into a single 384-dimensional representation. We note that at this point, all operations have been performed per cell without interactions. In addition to these cell tokens, we also instantiate for each neighborhood a register token, which we use to accumulate global information across the neighborhood. We then apply a transformer encoder to the cells, only allowing cells within the same neighborhood and their <cls > -like tokens to attend to each other. We use 8 attention heads with GELU activations and layer norm prior to attention and MLP projection. We note that including a bias term in the key, query, and value MLPs is important to stabilize training, while not noting any significant differences in models fit with and without bias terms for the rest of the encoder and decoder layer MLPs. Following the transformer encoder, we use attention pooling to aggregate the cell and <cls> representations for each neighborhood into a single token with embedding dimension 384. We refer to these as the neighborhood representations. We then instantiate a new token from each reference cell that is a learned embedding for each cell type (separately from the encoder cell type embedding). These are concatenated to the neighborhood representations. We then apply a transformer decoder to the tokens, allowing only the neighborhood token and masked cell embedding to attend to each other if they are from the same cellular neighborhood. This decoder embedding dimension was 384 with 8 attention heads. During training, we extract only the masked reference cell tokens. We then use separate linear projections to output the mean, dispersion, scale, and zero inflation logit parameters for zero-inflated negative binomial regression. We optimize the model by minimizing the log likelihood of a negative binomial distribution using observed cells' MERFISH probe counts. We trained all versions of CellTransformer on a system with 2 NVIDIA A6000 GPUs with an effective batch size of 256. Once trained, we apply CellTransformer to a given dataset and instead of extracting reference cell tokens we extract the neighborhood representation. We then cluster this representation using k-means. We use the cuml library to perform this operation on GPU (cuml.KMeans), with arguments n_init=3, oversampling_factor=3, and max_iter=1000. We observe that spatial domains are spatially smooth. However, in the case that there is a high-frequency signal that the end-user would like to filter, we optionally introduce a step prior to k-means where we smooth the embeddings using a Gaussian filter. For all comparisons except those in Supplementary Fig. 12, smoothing was performed with a Gaussian filter with a 40 μm full-width at half maxima (sigma of 12.01 μm). We used an 80–20% train-test split proportion (random splitting across the entire dataset) and the ADAM optimizer over 40 epochs. We perform a linear warmup for 500 steps to a peak learning rate of 0.001 and use an inverse-square root learning rate scheduler to decay the learning rate continuously. We perform training from scratch without transfer. We follow Wu et al. in using an Amari-type distance to compare clustering solutions. Briefly, we compute several replicates (20 in this work) of k-means at a given choice of k with different random seeds \({D}_{k,i}\), with i indexing the different centroids for a given solution. to identify the choice of k which is most stable. To quantify the overall similarity of regions extracted using CellTransformer with CCF, we first extract cell type composition vectors for each region at a given level of the hierarchy. 2, we use the subclass level (338 cell types), resulting in k-region by 338 matrices. For each region derived from one of the tested models, we compute two quantities: the best match (maximum value of Pearson correlation, non-exclusively) to any CCF (Fig. 2d) or an exclusive match (using the linear sum assignment algorithm) to pair the regions from either set one-to-one (Fig. We use scipy.optimize's implementation to solve the linear sum problem. To run CellCharter, we first generated scVI embeddings using the default settings for depth and width of the network and with the tissue section labels as conditional batch variables. We trained for 50 epochs using the early_stopping=True setting. We then aggregated across 3 (default settings), 6, 9 layers using the cellcharter.gr.aggregate_neighbors function. We then applied CellCharter's Gaussian mixture model implementation at various choices of the number of Gaussians. We could not run the mixture model with our hardware (A6000, 48GB GPU memory) for more than 9 layers, which was also the number that produced the highest correspondence with CCF and is reported in Fig. SPIRAL requires supervision on single-cell types so for this we use the subclass cell type levels. We trained models across neighborhood sizes for 1 epoch and then chose the neighborhood size with best performance (170 μm) and trained this model to saturation (10 epochs). SPIRAL uses four objective functions so to assess saturation we averaged them. We note that SPIRAL does not use a training and testing set split in their training, making it difficult to assess an optimal stopping point. For the k = 354 and k = 670 domain discovery analyses the SPIRAL clustering pipeline produced an out-of-memory error and we instead used our own pipeline with k-means on SPIRAL embeddings. These proportions are averaged across all cells and tissues. First, for a given domain at a given resolution (choice of k), we average the nearest-neighbor smoothness values over each domain. This process was performed for each compared method and for the CCF domains at 25 domains (matching the CCF division annotations), 354 domains (matching the CCF structure annotations) and 670 domains (matching the CCF substructure annotations). Then, for a given resolution, we used the per-domain average smoothness distribution from the CCF domain sets to derive a per-resolution cutoff. We then applied the cutoffs for each resolution to each of the corresponding data-driven domain sets. To extend the discreteness analysis to domain sets larger than 670, we used the 20th percentile value for the substructure annotations without adjusting them for different values of k. First, we regress these embeddings on the (x, y, z) coordinates. We also fit a multi-class logistic regression using the mouse donor identity. For the logistic regression, we use cuml.LogisticRegression with default settings in cuml. For the cell position regression we fit simple least squares using PyTorch via QR decomposition. To do this, we compute a simple baseline model that predicts average gene expression (computed across the entire Allen Institute for Brain Science mouse dataset) for each cell. We then compute a Pearson correlation between each cell's observed gene expression and the CellTransformer predictions, averaging similarly across instances of a given cell type. The difference between the baseline and model predictions is displayed, per cell type, and grouped across neurotransmitter types in Supplementary Fig. We contrast two methods of extracting spatial domains from the four animals in the Zhuang lab dataset12. We first fix k, the number of desired spatial domains. Then we fit one k-means model on all of the neighborhood embeddings for all four (Zhuang 1, 2, 3, and 4) mice together. We then compute the similarity of these region sets using the same method used to quantify differences between CellCharter and CellTransformer by comparing their regional cell type composition vectors. Initial results with a direct transfer of hyperparameters to the Langlieb et al.13 dataset13 did not produce spatially coherent domains. We therefore implemented two quality control procedures on the raw data. At the cellular level, we identified cells with >20% mitochondrial genes and those within the 10th percentile of read depth across each section. We noted that a successful segmentation in the Langlieb et al. dataset required a larger model than the MERFISH ones, using 10 encoder layers rather than 4, which we attributed to the 10X higher number of genes in this dataset versus the 500 in the ABC-WMB Allen 1 dataset. We used a neighborhood size of 50 μm to reduce memory footprint, reasoning the higher cell density in this dataset and higher number of genes would provide enough information for representational richness. Principal software used in this work includes PyTorch48, numpy49, scikit-learn50, scipy51, scanpy52, cuml53, matplotlib54, and seaborn55. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. All MERFISH data used for this publication is available either at the Allen Institute ABC-WMB data portal (https://portal.brain-map.org/atlases-and-data/bkp/abc-atlas; both Allen 1 and Zhuang 1, 2, 3 and 4 datasets), the CZI cellxgene portal (https://cellxgene.cziscience.com/datasets; Zhuang 1, 2, 3, and 4 datasets). Slide-Seq data are available through the BrainCellData website at https://www.braincelldata.org/. Source data are provided with this paper. Yao, Z. et al. A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain. Functional connectomics spanning multiple areas of mouse visual cortex. Chen, X. et al. Whole-cortex in situ sequencing reveals input-dependent area identity. The covariance environment defines cellular niches for spatial inference. Hu, J. et al. SpaGCN: Integrating gene expression, spatial location and histology to identify spatial domains and spatially variable genes by graph convolutional network. Zhou, X., Dong, K. & Zhang, S. Integrating spatial transcriptomics data across different conditions, technologies and developmental stages. Large-scale characterization of cell niches in spatial atlases using bio-inspired graph learning. Shi, H. et al. Spatial atlas of the mouse central nervous system at molecular resolution. Towards a universal spatial molecular atlas of the mouse brain. The Allen Mouse Brain Common Coordinate Framework: a 3D reference Atlas. Zhang, M. et al. Molecularly defined and spatially resolved cell atlas of the whole mouse brain. Devlin, J. et al. Bert: Pre-training of deep bidirectional transformers for language understanding. He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2022). Modeling intercellular communication in tissues using spatial graphs of cells. Yao, Z. et al. A taxonomy of transcriptomic cell types across the isocortex and hippocampal formation. Comparative cellular analysis of motor cortex in human, marmoset and mouse. Guo, T. et al. SPIRAL: integrating and aligning spatially resolved transcriptomics data across different experiments, conditions, and technologies. Automated identification of the mouse brain's spatial compartments from in situ sequencing data. Maher, K. et al. Mitigating autocorrelation during spatially resolved transcriptomics data analysis. Ortiz, C. et al. Molecular atlas of the adult mouse brain. Zhang, X., Wang, X., Shivashankar, G. V. & Uhler, C. Graph-based autoencoder integrates spatial transcriptomics with chromatin images and identifies joint biomarkers for Alzheimer's disease. Dong, K. & Zhang, S. Deciphering spatial domains from spatially resolved transcriptomics with an adaptive graph attention auto-encoder. Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST | Nat. Wu, S. et al. Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks. Cahill, R. et al. et al. Unsupervised pattern identification in spatial gene expression atlas reveals mouse brain regions beyond established ontology. The subiculum is a patchwork of discrete subregions. Bienkowski, M. S. et al. Homologous laminar organization of the mouse and human subiculum. Distinct Transcriptomic cell types and neural circuits of the subiculum and prosubiculum along the dorsal-ventral axis. Genetically defined neuron types underlying visuomotor transformation in the superior colliculus. Inagaki, H. K. et al. A midbrain-thalamus-cortex circuit reorganizes cortical dynamics to initiate movement. Specific populations of basal ganglia output neurons target distinct brain stem areas while collateralizing throughout the diencephalon. Robust decomposition of cell type mixtures in spatial transcriptomics. Brbić, M. et al. Annotation of spatially resolved single-cell data with STELLAR. Maskey, S. et al. Generalized Laplacian positional encoding for graph representation learning. Burgess, J., Nirschl, J. J., Zanellati, M.-C., Cohen, S. & Yeung, S. Learning orientation-invariant representations enables accurate and robust morphologic profiling of cells and organelles. B. et al. A brain-wide map of neural activity during complex behaviour. de Vries, S. E., Siegle, J. H. & Koch, C. Sharing neurophysiology data from the Allen Brain Observatory. Oh, S. W. et al. A mesoscale connectome of the mouse brain. Hike, D. et al. High-resolution awake mouse fMRI at 14 Tesla. Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. Pedregosa, F. et al. Scikit-learn: machine learning in Python. Virtanen, P. et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Wolf, F. A., Angerer, P. & Theis, F. J. SCANPY: large-scale single-cell gene expression data analysis. & Nolet, C. Machine Learning in Python: main developments and technology trends in data science, machine learning, and artificial intelligence. The authors would like to acknowledge Forrest Collman for help with Neuroglancer data visualization of spatial domains. We acknowledge Patrick Xian for helpful discussions. AJL acknowledges Katharine Z. Yu, Chang Kim, Parker Grosjean, Lee Rao, and Tom Nowakowski for feedback on neuroscientific and machine learning contexts of paper. would like to acknowledge support from the Weill Neurohub through the Weill Neurohub's Next Great Ideas Award. R.A. would like to acknowledge support from the National Institute of Mental Health of the National Institutes of Health under award number RF1MH128672 and Sandler Program for Breakthrough Biomedical Research, which is partially funded by the Sandler Foundation. Sharing of Allen Institute for Brain Science data through Allen Brain Cell Atlas (and related tools) and registration of the AIBS MERFISH brain to the CCFv3 was funded through 1U24MH130918-01 to L.N. UCSF Weill Institute for Neurosciences, San Francisco, CA, USA Allen Institute for Brain Science, Seattle, WA, USA Michael Kunst, Shenqin Yao, Nicholas Lusk, Lydia Ng, Hongkui Zeng & Bosiljka Tasic Nature Communications thanks the anonymous reviewer(s) for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Lee, A.J., Dubuc, A., Kunst, M. et al. Data-driven fine-grained region discovery in the mouse brain with transformers. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41467-025-64574-w'>SafeTraffic Copilot: adapting large language models for trustworthy traffic safety assessments and decision interventions</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 09:07:48
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Predicting expected traffic crashes and designing targeted interventions are highly challenging due to the inherent complexity of crash data and persistent concerns over the prediction trustworthiness. We introduce SafeTraffic Copilot that adapts Large Language Models (LLMs) to perform expected crash prediction as a text-reasoning task, then attribute critical features for targeted safety interventions. Within the Copilot, SafeTraffic LLM is customized then fine-tuned on the textualized SafeTraffic Event dataset, which consists of 66,205 real-world crash cases with 14.5 million words from five U.S. states. Across multiple prediction tasks including crash type, severity, and number of injuries, SafeTraffic LLM demonstrates a 33.3% to 45.8% improvement in average F1-score over existing works. To interpret these results and inform safety interventions, we introduce SafeTraffic Attribution, a sentence-level feature-attribution framework enabling conditional “what-if" risk analysis. Findings reveal that alcohol-impaired driving is the leading factor for severe crashes, with impairment-related and aggressive behaviors contributing nearly three times more risk than other behaviors. Furthermore, SafeTraffic Attribution identifies critical features during fine-tuning, guiding crash data collection strategies for continual improvement. SafeTraffic Copilot enables prediction and reasoning of conditional crash risks through foundation models, thereby supporting traffic safety improvements and offering clear advantages in generalization, adaptation, and trustworthiness. Road traffic injury remains a stubborn public health crisis in the United States. In 2022 alone, 42,795 people were killed on U.S. roads—one of the highest per capita fatality rates in the developed world1. Despite decades of counter-measures, the fatality curve continues to rise, especially in the United States (as shown in Fig. 1a), underscoring an urgent need for new data-driven techniques that can uncover the mechanisms of crashes and inform decisive policy action. Expected crash prediction models (hereafter referred to as crash prediction) offer a principled way to learn from historical data and isolate the factors that most strongly elevate risk2. a The U.S. faces one of the highest crash risks among developed countries, with a rising trend. However, analyzing and addressing this issue is challenging due to the heterogeneous factors involved in crash events, including traffic conditions, human behavior, environmental impacts, and driver characteristics. To tackle this, we propose SafeTraffic Copilot, a framework designed for two key tasks: (1) Predicting crash outcomes and (2) Attributing crash factors for conditional risk analysis. By addressing questions such as why crashes occur and how to mitigate crash risks, SafeTraffic Copilot seeks to deliver optimal policies for safety improvement. b The SafeTraffic Copilot workflow incorporates multi-modal data, including driver behavior, vehicle details, infrastructure, and environmental conditions, represented through textual reports, satellite imagery, and other formats. Leveraging an AI-expert cooperative method, the crash data is transformed into textual prompts, resulting in the SafeTraffic Event dataset comprising 66,205 cases. SafeTraffic LLM is created with accurate and trustworthy forecasting abilities for further analysis. Building on this pipeline, SafeTraffic Attribution operates across three dimensions: (1) Event-level risk analysis to identify feature contributions, (2) Conditional risk analysis to assess state-level risks under varying conditions, and (3) Data collection guidance to optimize the data acquisition process. The results of SafeTraffic Attribution provide actionable insights to enhance data analysis and collection, fostering a more comprehensive understanding of crash data and events. The current approaches to crash prediction are broadly categorized into macroscopic, statistical-level analyses, and microscopic, event-level investigations. Macroscopic models offer a general understanding of safety performance, identifying high-risk areas and temporal trends, but they lack the granularity to explain the specific circumstances of a crash—the who, what, and why3,4,5,6. While microscopic models, often employing machine learning, aim to predict crash consequences under specific traffic conditions, they have struggled with precision and generalization4,7,8,9,10. A fundamental challenge lies in effectively integrating the multi-modal data associated with a crash event, spanning textual narratives, numerical data, images, and driver histories, and interpreting the complex interplay of contributing factors, thereby limiting their utility in designing effective safety policies. The recent emergence of foundation models, especially Large Language Models (LLMs), presents a transformative opportunity to mitigate these enduring challenges by leveraging their advanced capabilities in processing and reasoning from complex, multi-modal information11,12,13,14,15. These models can synthesize and interpret vast, unstructured data, such as the narrative descriptions in crash reports, and align them with structured data like roadway characteristics and driver histories, offering a more holistic understanding than was previously possible. However, adapting these powerful generative models for the discriminative task of crash outcome prediction introduces its own set of technical hurdles. The primary challenge is methodological: generative LLMs with extensive output vocabularies must be re-engineered to reliably predict outcomes within a set of well-defined, finite categories (e.g., crash severity levels)16. This adaptation raises major concerns about the trustworthiness and calibration of their predictions, which is crucial for high-stakes applications like public safety17. Furthermore, the inherent “black box" nature of these models poses a major obstacle to achieving the interpretability required for targeted safety improvements18,19,20. While initial studies have explored LLMs for traffic safety, they have been limited to prompt engineering and have not addressed the crucial interpretability gap, which is essential for robust decision support and for answering the critical why and how questions of crash causation21,22,23. In this research, we introduce SafeTraffic Copilot, a LLM-driven framework that shifts the paradigm from aggregate-level statistics to granular, event-level crash prediction and understanding (see Fig. By reframing crash prediction as a text-based reasoning task, SafeTraffic Copilot is designed to address the key challenges of data integration, model generalization, and feature attribution. The framework consists of three integrated components: SafeTraffic Event dataset, for unifying multi-modal crash data; SafeTraffic LLM, for accurate outcome prediction; and SafeTraffic Attribution, for conditional risk analysis. This approach allows us to not only forecast the when, where, who, and what of a crash but also to provide a deep, interpretable understanding of why it occurred and how similar risks can be mitigated, offering a unified approach for targeted and effective data-driven safety interventions. This study shifts traffic safety analysis from aggregate-level to event-level crash prediction by developing SafeTraffic Copilot, a customized LLM framework that integrates multi-modal crash data into the broader semantic context to forecast consequences and attribute features with interpretability. Using SafeTraffic Event dataset (66,205 textual prompts; over 14 million words) and framing outcome prediction as token generation, SafeTraffic LLM delivers a 33.3% to 45.8% average F1 improvement over competitive baselines across multiple crash-consequence tasks. By embedding traffic-safety priors and explicitly targeting the number of injuries, crash severity, and crash type via special tokens, SafeTraffic Copilot yields accurate and trustworthy predictions, where the accuracy increases with confidence, achieving over 70% accuracy when the confidence score exceeds 60%, and 95% precision for fatal-crash predictions at the same threshold. Our proposed textual feature-attribution module provides event- and state-level insight—it simultaneously uncovers what drives risk in a specific crash, enabling conditional intervention, and what information drives model quality at scale, guiding strategic data-collection policies for long-term accuracy. Specifically, the proposed sentence-level Shapley scores identify high-risk scenarios (such as “alcohol-impaired,” “work-zone,” “inappropriate behaviors,” etc.) For example, combining alcohol impairment with a work-zone setting nearly doubles the likelihood of a severe crash compared with sober driving under identical conditions. Further, summing Shapley contributions across the entire fine-tuning dataset ranks data fields by their marginal impact on prediction accuracy and confidence score, thereby guiding first-responders toward the details that matter and streamlining continuous model updates. For example, unit information (driver and vehicle attributes) accounts for more than 40% of the contribution to crash severity prediction, highlighting these fields as priority entries for future crash documentation. In our research, we define crash prediction as expected (conditional) crash prediction, a definition consistent with the agencies and established literature4,24,25,26. Officially, expected crash prediction is defined as the task of estimating expected crash characteristics, including crash type, severity, number of injuries, and their likelihood of occurrence, under specified conditions. These estimations are based on expected traffic conditions and relevant contextual information, such as roadway attributes, environmental conditions, traffic volumes, and driver behaviors. The prediction targets consist of three variables with belonged confidence score: Number of Injury, Severity, and Crash Type (see Fig. Specifically, Number of Injury task predicts the number of people injured in the given crash event. We define the number of injuries as the total number of non-fatal injuries, including possible, minor, and serious injuries, obtained by subtracting fatalities from the total number of injured people in a crash. Number of Injury task is treated as a classification task with four categories: zero, one, two, and three or more than three, where crashes involving more than two injured people are grouped into a single category due to the limited number of such cases. The Severity task assesses the level of injury severity in a crash, classified into five levels from no apparent injury to fatal. Type task predicts the type of crash, such as the rear-end collision or collision with object, with 14 crash type categories in the Washington dataset and 16 in the Illinois dataset. Detailed information on the defined targets and confidence scores is available in “SafeTraffic Event dataset construction" and “Expected crash prediction confidence score calculation" in the “Methods” section. Multi-modal crash data is collected and organized into textual prompts through an AI-expert cooperative process. The Highway Safety Information System (HSIS) crash data, satellite images, and infrastructure data are used to extract general and infrastructure information, including the crash time, location, the road level, and so on. The vehicle data and person data are converted into the event information and the unit information, including vehicle movements, driver characteristics (e.g., age, gender, alcohol use), vehicle attributes (e.g., manufacture year), and so on. SafeTraffic Event dataset is created with three prediction targets: Number of Injury, Severity, and Type. To reframe the crash outcomes prediction from a classification task to a language inference task, SafeTraffic LLM is fine-tuned by adding prediction targets as special tokens in its vocabulary and adjusting parameters using Low-Rank Adaptations (LoRA)37, a lightweight fine-tuning technique that injects trainable rank-decomposed matrices into each layer without updating the full model. Our cleaned dataset comprises crash data from Washington State in 2022, totaling 16,188 records, and from Illinois in 2022, totaling 42,715 records, after excluding cases with missing key attributes related to vehicle or crash object status. Primary sources include the Highway Safety Information System (HSIS) crash data29, the state crash report, and satellite images30. HSIS is a multistate database that contains crash, roadway inventory, and traffic volume data for a select group of States. Crash data provides detailed descriptions of crashes, such as location, time, and injury severity. Infrastructure data includes information about road layouts and traffic characteristics, such as road level and speed limits. Vehicle data contains details such as manufacturing year and reported defects of the involved vehicles, while person data captures demographic and other relevant details about drivers and passengers, such as age and gender. Satellite images complement the HSIS data by providing additional visual context, including information about lanes, intersections, and other roadway attributes. Further information on raw data formats and types is available in “Raw data” in the “Methods” section. In addition to the Washington and Illinois datasets, we also collect and process 2250 crash cases from Maine, 2250 from Ohio, and 2802 from North Carolina to evaluate the model's zero-shot generalization. Their raw data sources align with those used for Illinois and Washington. Using the Illinois State prompt template (due to its closer data format), we converted the records into prompt format, keeping shared features and setting unmatched ones to null. Differences in data representation (e.g., driver alcohol levels as a numeric value in Illinois versus a textual descriptor in Maine) introduce unseen values and distributions, enabling evaluation of the model's zero-shot generalization. To leverage the multi-modal crash data described above for crash prediction, we developed the SafeTraffic Copilot crash outcomes prediction pipeline, which transforms crash outcomes prediction into a text-based reasoning task. To achieve this, the raw crash data is organized into the textual SafeTraffic Event dataset, which is then used to fine-tune the SafeTraffic LLM. Figure 2 presents an overview of the proposed pipeline. The SafeTraffic Event dataset is created through an AI-expert cooperative textualization process, organizing multi-modal raw data for effective crash prediction. 2, the constructed prompts are divided into five parts: one system prompt and four content parts, with each content part containing approximately 100 words. System prompt: provides an introduction and task-specific instructions. Infrastructure information: describes road infrastructure, encompassing static features like the number of lanes and speed limits, as well as dynamic elements such as work zones, lighting, and road surface conditions. Unit information: provides vehicle and individual details relevant for crash prediction, such as airbag status and the driver's age. To organize and merge the mentioned information for each crash event, we perform feature engineering and textualization, structure the textualized data as input, and process labels corresponding to the three targeted crash prediction tasks from real-world reports. The complete prompt examples are presented in Fig. Ultimately, after filtering out data items with missing information, the SafeTraffic Event dataset merges the complementary information from multi-modal data sources and contains 66,205 crash records with approximately 14.5 million words. These records are split into training, validation, and test sets in a 7:1.5:1.5 ratio for Washington and Illinois datasets, while the remaining datasets from three other states are used exclusively for cross-region training-free generalization evaluation. An example prompt of an expected crash prediction event from the dataset collected in Washington State. With SafeTraffic Event dataset, we can adapt LLMs for expected crash prediction. Although vanilla LLMs like Llama 313 possess broad general knowledge and strong text-reasoning capabilities, they demonstrate limited effectiveness on crash prediction tasks without the fine-tuning process (see Supplementary Section 3.1). To address this, we developed SafeTraffic LLM, a specialized model fine-tuned on the processed SafeTraffic Event dataset. This fine-tuning process enhances the LLM's comprehension of crash events and enables accurate outcome prediction. Specifically, additional special tokens are introduced into the LLM vocabulary as prediction targets (Number of Injury, Severity, and Crash Type), fine-tuning the model to generate these tokens during prediction. We evaluate the performance of SafeTraffic LLM and compare its performance with other baselines (see “Adopted baselines" in the “Methods” section). The fine-tuning process is based on two vanilla LLMs with different sizes: Llama 3.1 8B and Llama 3.1 70B. SafeTraffic LLM provides the highest accurate and reliable prediction results across all crash types, severity, and injury counts, even in zero-shot scenarios. Table 1 compares the performances of SafeTraffic LLM and adopted baselines. The results show that the SafeTraffic LLM outperforms all the baselines in each task setting with an average F1-score improvement of 33.3%–45.8% across multiple tasks. SafeTraffic LLM performs well on both the Washington and Illinois datasets, demonstrating its stability across diverse geographical regions. 4a, b, beyond aggregated metrics, SafeTraffic LLM demonstrates a more balanced prediction distribution and achieves higher accuracy across individual categories. 4c, d, existing machine learning models tend to predict the dominant categories (e.g., zero under Number of Injury prediction task, no apparent injury under Severity prediction task, and the complete confusion matrix is shown in Supplementary Fig. In contrast, baseline models tend to predict the most frequent category across both the c Washington and d Illinois datasets (we show baseline models with the best F1-score. The performances for other baseline models can be found in Supplementary Fig. Meanwhile, SafeTraffic LLM produces trustworthy predictions for both the e Washington and f Illinois datasets. Higher confidence levels in the model's predictions correspond to an increased likelihood of accuracy. Furthermore, g The SafeTraffic LLM achieves higher precision for fatal-crash predictions. h Fatal-crash predictions also exhibit higher confidence in the Illinois dataset. The Washington dataset is not shown due to limited fatal cases. i For fatal crashes, the SafeTraffic LLM achieves near-perfect precision (97.61%) when the confidence score exceeds 0.6, indicating that the SafeTraffic LLM is highly accurate and trustworthy for fatal crashes. j The 3-year temporal comparison of monthly prediction accuracy across tasks (2019–2021). The used SafeTraffic LLM was fine-tuned on the 2022 Washington dataset and evaluated on the 2019–2021 Washington datasets to assess its temporal generalization capability. SafeTraffic LLM demonstrates stable performance at the county level for fine-tuning tasks in Illinois (IL) and Washington (WA), as well as zero-shot tasks in Maine (ME), North Carolina (NC), and Ohio (OH). States evaluated in zero-shot settings are highlighted with a gray background. SafeTraffic LLM provides trustworthy crash predictions, where a higher confidence score links to higher accuracy. SafeTraffic LLM tailors LLMs for discriminative crash outcomes prediction tasks, generating predictions accompanied by confidence scores that represent the probabilities associated with specific special tokens (see “Expected crash prediction confidence score calculation" in the “Methods” section for the calculation details of the confidence score). Figure 4e, f illustrates the trend of accuracy in relation to the confidence scores of SafeTraffic LLM 's predictions for the Washington and Illinois datasets. The results indicate that our model achieves greater accuracy at higher confidence levels. This relationship is even more pronounced for fatal-crash predictions (see Fig. By providing reliable confidence scores alongside predictions, the framework empowers informed decision-making in real-world applications. SafeTraffic LLM exhibits reliable spatial and temporal generalization capabilities. For spatial generalization, we evaluated SafeTraffic LLM by training on the Illinois dataset and testing on three unseen states: Maine, North Carolina, and Ohio. Beyond state-level evaluation, we also assessed generalization at the county level. 4j, most counties exhibit an accuracy variation within 10%–20%, demonstrating the model's stable performance across regions. For temporal generalization, we evaluate the model fine-tuned on 2022 Washington data using data from 2019 to 2021 in the same state. 4k, the model maintains stable performance across months for all three tasks: Number of Injury, Severity, and Type prediction, with over 75% of the months falling within a  ±10% accuracy range. Aggregated yearly performance is presented in Supplementary Section 3.3. Understanding how SafeTraffic LLM generates accurate predictions and how various components of the input prompt influence the outcomes is fundamental to enabling evidence-based decision-making. In our analysis, we focus exclusively on severe crashes (i.e., fatal and serious injury crashes) to identify the contributing factors behind these events. As discussed above, the SafeTraffic LLM 's confidence score strongly correlates with its predictive accuracy for severe crashes. Consequently, the confidence score associated with severe crash predictions (hereafter referred to as the confidence score) can be used as an indicator of crash risk level: a higher confidence score corresponds to greater prediction accuracy for severe crashes, which in turn reflects a higher likelihood that the crash is severe (rather than minor or no apparent injury) in the real world. Notably, the SafeTraffic LLM 's confidence scores tend to be lower than their corresponding accuracy values, indicating that using the confidence score is a conservative estimate of risk. Within the SafeTraffic Attribution framework, a sentence-based feature contributions calculation method was proposed to identify how each sentence contributes to the LLM's outputs based on Shapley theory, which is recognized as a systematic and equitable method for attributing the contribution of each feature to a model's output31,32, thereby revealing crash-related factors at the event level (see “SafeTraffic Attribution” in the “Methods” section for details). In essence, each feature's contribution represents its share of responsibility for the model's confidence in a particular prediction. Figure 5 illustrates sentence-level feature contributions for the severity of individual crash events, using one crash from Washington and one from Illinois as examples. In the Washington crash example (Fig. 5a), Driver Behavior (e.g., reckless driving or speeding) is the primary factor contributing to serious injury crashes, with the feature contribution of 0.258. Person Info (e.g., no seatbelt use) also shows a substantial impact with the feature contribution of 0.149. By contrast, Dynamic Info (daylight and dry roads) lowers the probability of a crash with serious injuries with a negative feature contribution of −0.009. While in the Illinois example (Fig. More additional sentence-level feature-attribution analysis can be found in Supplementary Sections 4.1 and 4.2. The following sections utilize SafeTraffic Attribution framework to examine feature importance from two perspectives: (1) at the inference stage, to identify key factors influencing crash predictions under various conditions and high-risk scenarios, and (2) at the fine-tuning stage, for which data are more critical for model learning. The left part displays the full prompt from a Washington and b Illinois, with different colors representing various semantic text sequences. Positive contributions signify a supportive role in the model's prediction, whereas negative contributions indicate a detracting influence. Conditional analysis evaluates crash outcomes across various scenarios, such as driving with or without alcohol consumption, to quantify the risk factors associated with each scenario. Severe crashes (serious injuries and fatal crashes) were prioritized in the conditional analysis due to their critical importance for traffic safety. These crashes, particularly fatal ones, were predicted accurately and reliably by SafeTraffic LLM (see Fig. Five key contributing factors were identified for this conditional analysis: Driver BAC (BAC = 0 mg/dL or not offered/BAC < 80 mg/dL/BAC ≥ 80 mg/dL), Roadway Type (Highway/not highway), Work Zone (Work zone/not work zone), User Type (Pedalcyclist or pedestrian/not pedalcyclist or pedestrian), and Driver Behavior (Aggressive driving / impairment-related behavior/traffic rules violations/improper driving/others). Collectively, these factors accounted for an average of 79.33% of the model's overall attribution in predicting serious and fatal crashes (see Fig. A summary of key findings is provided: The BAC record emerges as a critical determinant in predicting serious and fatal crashes. Among all contributing factors, BAC accounts for 25.26% of the total contribution to serious and fatal-crash prediction (see Fig. Notably, its contribution substantially increases when a driver consumes alcohol, irrespective of the amount. When drivers are under the influence of alcohol even if their BAC does not exceed the legal intoxication limit of 80 mg/dL33,34, this factor's feature contribution still reaches around 0.45, surpassing that of most other factors in many cases (see Fig. Conversely, when a driver's BAC is recorded as “zero or not offered,” its contribution approaches zero, indicating minimal impact on the model's predictions. Driving in a work zone is already risky under sober conditions, but alcohol consumption greatly increases the danger, making it one of the most hazardous scenarios for severe injury crashes. 6a, driving in a work zone while sober (“Work Zone-Yes” and “BAC = 0 or not offered”) contributes little to severe crash outcomes, with an average feature contribution of 0.03. However, after consuming alcohol (whether “BAC is higher than 80 mg/dL” or “BAC less than 80 mg/dL”), the work zone feature contribution rises more than seven times to an average of 0.22. Furthermore, the overall crash risk increases substantially when driving in a work zone after drinking, as indicated by an average risk level of 0.78, compared to 0.44 under sober conditions. These findings indicate that work zones become especially hazardous when alcohol consumption is involved, creating one of the highest-risk scenarios for severe crash outcomes. Potential drunk driving warnings and risk mitigation strategies shall be closely linked with work-zone areas. Aggressive and impairment-related behaviors pose nearly three times the risk for severe crash outcomes compared to other driver behaviors. 6c, aggressive driving emerges as the most important contributor among driver behaviors, with a median feature contribution of 0.195. Impairment-related behavior, including driving under the influence of alcohol or drugs, also has a substantial influence, with a median feature contribution of 0.154. In comparison, other improper driver behaviors, such as traffic rule violations (median feature contribution of 0.055) and distractions like mobile phone use (categorized under improper driving, with median feature contribution of 0.015), show below-average contributions to serious and fatal crashes. The “other" category, which includes normal driving and unknown behaviors, has the smallest impact, with a feature contribution of 0.007. 6a, c, our analysis reveals a strong correlation between the number of risk factors present in a crash and the expected risk level for severe crash outcomes. High-risk factors are defined as those with a 75th percentile contribution exceeding 0.2 in Fig. 6c, including driving after drinking (BAC < 0 mg/dL), driving in work zones, pedestrian-involved crashes, and high-risk driver behaviors (aggressive or impairment-related). When no risk factor is involved, the average risk level for severe crash outcomes is estimated at 0.47. This value increases to 0.59 with one risk factor, rises to 0.68 with two, and reaches 0.73 when three risk factors co-occur. These findings underscore the need for transportation agencies to implement comprehensive, multi-dimensional interventions, particularly in scenarios characterized by overlapping high-risk conditions. Higher confidence scores in SafeTraffic LLM 's predictions correspond to greater accuracy, allowing the confidence score (calculated as the sum of feature contributions for all data components) to serve as an indicator of risk level for serious and fatal crashes. a The aggregated average crash risk level under different feature combinations. Cases are grouped based on their combinations of conditions (indicated by dark dots). For each group, the average crash risk level and the average contributions of five key features are calculated and visualized, including driver behavior, user type, whether driving occurred within a work zone, roadway type, and Blood Alcohol Content (BAC, in mg/dL). We use 80 mg/dL as a key threshold, based on the legal limit for ethanol concentration33,34. The detailed data is provided in Supplementary Table 11. b The overall average contribution of each feature. Taking BAC as an example, each case yields a BAC contribution value, and the mean of these values across all cases represents the overall contribution of BAC. c Average feature contribution for each factor under specific values. Boxes are colored pink if the median exceeds the overall median for that factor, and blue otherwise. Each box is annotated with its median value. d Feature contributions of different data components during the training stage for the Washington and Illinois datasets. Event information and unit information are the most important components for the model training. While feature contributions at the inference stage reveal which features drive critical crash outcomes, understanding feature contributions during training provides deeper insights into which data components most effectively enhance model accuracy. 6d, the feature contributions of each component in the Washington and Illinois datasets are shown, demonstrating their impact on the model's performance during training (see Supplementary Table 10 for detailed results and “SafeTraffic Attribution” in the “Methods” section for calculation details). Transforming crash prediction into a text-reasoning task unlocks the full richness of multi-modal safety data. By integrating crash narratives, satellite and incident images, and infrastructure attributes into a unified textual prompt, foundational language models can jointly reason over behavioral cues (“alcohol-impaired,” “work-zone”), pre-crash trajectories, and environmental context instead of treating them as disconnected numbers. Our AI-expert co-designed prompts let LLMs outperform conventional baselines while attribution scores reveal which factor combinations, e.g., impairment plus work zones, are the most elevated risk, guiding both targeted interventions and smarter data-collection priorities. This multi-modal-to-text paradigm therefore signals a powerful solution: integrating diverse crash information streams through foundational models not only boosts predictive accuracy but also yields transparent, actionable insights for continuous safety improvements. Integrating rich data with a powerful foundation-model engine shifts prediction from simple distribution fitting to situational-aware reasoning, yielding transparent and trustworthy insights that generalize across regions. In both Washington and Illinois, accuracy exceeds 70% once confidence surpasses 60%, with a near-linear rise in precision thereafter—giving practitioners a clear, quantifiable handle on uncertainty, as shown in Fig. 4e, f. Furthermore, the proposed SafeTraffic Attribution components turn this trust into action: it ranks textual, visual, and categorical cues by their contribution to confidence, highlighting the levers that most elevate risk. Notably, alcohol-impaired driving raises the severe-crash confidence score by 0.47, underscoring its critical policy relevance. The SafeTraffic LLM 's conditional attribution engine pinpoints which factor combinations truly drive crash risk with trust, ranking scenarios by danger and revealing actionable “what-ifs.” In data-rich settings, it reliably surfaces high-risk pairings, e.g., alcohol use in work zones or aggressive driving under impairment, guiding focused counter-measures such as on-site BAC checks or behavior-targeted education. Crucially, its probabilistic confidence signals rise with accuracy, lending quantifiable trustworthiness to every recommendation. When data are sparse, the same framework generalizes through simulation: analysts can toggle rarely observed variables (pedestrian presence, freeway geometry, etc.) and still obtain credible risk shifts. This blend of calibrated confidence and flexible what-if analysis empowers agencies to design precise, evidence-based traffic-safety interventions before crashes happen. Aggregated data-attribution analysis pinpoints the crash-record elements that matter most, offering a generalizable blueprint for smarter, future-proof data collection and quality control. Currently, each state designs its own crash-report form, preventing a common standard and stifling national-scale analytics. During the fine-tuning stage, unit-level details, such as driver behavior, vehicle attributes, and event-level cues, such as vehicle movement, weather, and roadway conditions, emerged as the strongest predictors of injury severity. Prioritizing complete, high-resolution capture of alcohol use, vehicle defects, vulnerable-user status, and road context, therefore, maximizes model payoff, whereas gaps in these fields quickly erode performance. Moreover, the aggregated attribution analysis also provides a quantitative basis for assessing data quality, showing how missing or incomplete values in key components impair model performance. Feeding these insights back into template design lets agencies standardize richer, more consistent reporting protocols that fuel continual model refinement and transferability across regions, accelerating broad safety transformations without sacrificing generalizability. Limitations and future work of the SafeTraffic Copilot. A primary limitation relates to the handling of multi-modal data. In the SafeTraffic Copilot, satellite images were processed into textual descriptions and incorporated into prompts. While this approach offers flexibility, advancements in multi-modal foundation models and increasing research on integrating multi-modal data with LLMs present promising alternatives35. Leveraging specialized image encoders or utilizing multi-modal foundation models for processing image data are compelling directions. Another potential limitation lies in the efficiency of model training and attribution. Although we employed LoRA fine-tuning and a stratified sampling technique to enhance efficiency36, implementing the complete framework still demands substantial resources. This poses certain limitations when resources are scarce or in situations demanding rapid model deployment. The raw crash data used in this study were obtained from the HSIS29 and Google Maps30. Data from the HSIS, sourced from multiple systems, encompasses a variety of formats, including categorical, numerical, and textual. In total, four main datasets were used: This dataset captures the essential spatio-temporal and contextual attributes of each crash. It includes crash date, time, day of the week, and month, along with location details such as route number, milepost, and the surrounding area's classification (e.g., rural or urban). Higher-level planning attributes (e.g., roadway and functional classifications, intersection-related indicators) are also recorded. In addition, it documents the dynamic circumstances leading up to the event, including the number of vehicles and pedestrians involved, vehicle travel directions (increasing or decreasing milepost), and any maneuvers performed (e.g., lane changes, straight-line movement). Key elements include the type of road surface (e.g., asphalt or concrete), average annual daily traffic (AADT), posted speed limits, and access control mechanisms. It also encompasses dimensions such as total road width, right and left shoulder widths, and median width (including median barriers if present), as well as road surface conditions (e.g., dry or wet) and ambient lighting at the time of the crash (e.g., daylight or dusk). This dataset consolidates information on the vehicles involved in each crash, including vehicle type (e.g., passenger car or truck), intended use (e.g., commercial or private), mechanical condition (e.g., defects), and relevant driver actions (e.g., lane changes or stopping). Additional information on airbag deployment and occupant ejection status provides further granularity. This dataset compiles information about individuals involved in the crash, detailing demographic characteristics such as age, gender, and seating position. It also includes the use of safety equipment (e.g., seat belts or helmets) and any contributing factors, such as driver distraction or impairment. The satellite images obtained from Google Maps serve as a supplementary data source to complement the HSIS dataset. Overall, we collected 16,188 crash event data from Washington State and 42,715 events from Illinois State for further analysis. To adapt the raw data for LLMs' fine-tuning process, we employ the feature engineering and textualization process to generate textual inputs (see Fig. We followed the following process to generate a textual prompt from raw data entry: The route ID and milepost were used to identify the specific road segment where the crash occurred, allowing us to gather related road and environment information from infrastructure data. Four raw datasets from HSIS (crash, infrastructure, vehicle, and person data) are used to construct a prompt through four steps. (2) Satellite image textualization: Retrieve satellite images via GPS coordinates using the Google Maps API, then employ GPT-4o to extract text-based information. (3) Dimensionality reduction: Combine targets with similar values using GPT-4o. (4) Prompt generation: Use the processed data from the previous steps to generate a prompt for each part. An example of the infrastructure information part of an event case in the Washington dataset is shown. The HSIS datasets provide GPS coordinates for crash locations in Washington and Illinois. To address missing information, such as the number of road lanes, high-resolution satellite images (512 × 512 pixels at a zoom level of 19) were retrieved using these GPS coordinates via the Google Maps API. These images supplement the crash dataset with crucial infrastructure and environmental context. Descriptive textual annotations were generated from the satellite images using GPT-4, filling key gaps in the original dataset. Image-related information enhances the model's performance; see Fig. a Examples of prompt modifications with image-derived information removed. b Performance comparison for expected crash prediction on Num. of Injury, Severity, and Crash Type prediction tasks on the Illinois and Washington datasets, using T + I (Text + Image) and T (Text-only) input modalities. c Average contribution of image-derived information at the inference stage. Raw data include abundant attributes with rich and varied descriptions. However, some features suffer from insufficient distinction between attribute values due to the original classification's complexity. To address this, we performed dimensionality reduction on these attributes by combining domain experts' insights with GPT-4o clustering results. For example, similar classifications like “pedalcyclist struck by vehicle" and “pedalcyclist strikes vehicle” were clustered under a broader category such as “pedalcyclist collisions.” This process generalized the data and reduced redundancy. See Supplementary Table 6 for detailed information. Prompt generation using an AI-expert textualization method. To generate logically coherent and continuous textual data suitable for LLM training, we transformed each category of data into text format using GPT-4o12. All data are organized as key-value pairs, and we get four parts of the key-value pairs for each event case. Then GPT-4o is used to generate the text prompt for each section of the key-value pairs individually. For each part, we apply a straightforward prompt to GPT-4o, such as “Please translate a python dictionary to paragraph, act as a crash data interpreter.” The text content is extracted from GPT-4o's response for each part, consisting of approximately 100 words. By linking four parts of the text, we obtain a comprehensive textual description for each crash event case. The detailed process is shown in Fig. We select three variables as the prediction targets: Injury, Severity, and crash Type. The three targets are defined as: The Severity\({s}_{i}^{{{{\mathcal{D}}}}}\in \{{S}_{k}| k =1,2, \cdots \, \}\), where Sk is the kth level of crash severity. We utilize these three variables to describe the crash result \({{{{\rm{CR}}}}}_{i}^{{{{\mathcal{D}}}}}\). For numerical variables, the function f(l) describes the number of people injured in a crash as follows: “zero" if l = 0, “one” if l = 1, “two" if l = 2, and “three and more than three" if l ≥ 3, the values for Sk and \({T}_{k}^{{{{\mathcal{D}}}}}\) are provided in the Supplementary Table 4 and Supplementary Table 5. We fine-tune SafeTraffic LLM by adapting LLaMa 3.113 to crash prediction tasks to enhance the LLMs' capabilities in interpreting crash data, identifying critical factors, and conducting feature-attribution analysis to offer insights for crash prevention. In this section, we will introduce detailed information on the fine-tuning process. The user prompt comprises the four content parts detailed in “SafeTraffic Event dataset construction" section for each case. Examples of these prompts are shown in Fig. We tokenize the text inputs using LLaMA 3.1's tokenizer. To adapt the LLM as a crash classifier, additional tokens have been incorporated into the tokenizer's vocabulary, and the detailed crash attribute categories are listed in Supplementary Table 4 and Supplementary Table 5. Specifically, for predicting the number of people Injuries of Washington dataset and Illinois dataset, we have introduced four special tokens: <ZERO>, <ONE>, <TWO>, and <THREE AND MORE THAN THREE>. Similarly, for predicting the Crash Severity of the Washington dataset and the Illinois dataset, we use five additional tokens: Sk, where 1 ≤ k ≤ 5, corresponding to different levels of severity. For Washington datasets, we utilize 14 special tokens: \({T}_{k}^{{{{\mathcal{W}}}}}\), where 1 ≤ k ≤ 14, each representing a specific crash type. During the fine-tuning phase, the traffic forecasting task is framed as a next-token generation task. Given an input prompt xi and its prediction target yi, we construct the full prompt as Ti = concat(xi, yi), where concat( ⋅ ) denotes the concatenation operation that appends the target label yi to the input xi as a special token. The next-token generation process can be described as: By maximizing the likelihood \({p}_{\theta }(T)={\prod }_{i=1}^{N}{p}_{\theta }({T}_{i})\), the LLM's parameters are learned. Both the system prompt and the user prompt are masked for loss computation during training. We also used a uniform data sampling strategy during the training process to facilitate the convergence of SafeTraffic LLM 16. Through this process, the model learns to make predictions for a traffic crash. The confidence score is a critical component that links model predictions to interpretability within the SafeTraffic Copilot. The confidence score quantifies the model's certainty in its prediction for a given input. Since we incorporate target labels as special tokens in the LLM's vocabulary and fine-tune the model to generate only these tokens as outputs, we define the confidence score based on the predicted token's probability. Specifically, given a textual input xi and its corresponding label yi, the confidence score C(xi) is defined as: where \({{{\mathcal{Y}}}}\) denotes the set of all possible labels (e.g., fatal, serious injury, etc., for crash severity prediction). For a given threshold t, let Nt denote the number of samples with confidence scores greater than t. Among these, Rt samples are correctly classified. The accuracy at threshold t is then given by By computing the accuracy at different thresholds t, we can plot the relationship between accuracy Acct and the threshold t, as shown in Fig. In our experiments, we follow LoRA37 to fine-tune LLaMA 3.1 models. Specifically, we update only the input and output layers directly, while all remaining layers are frozen and trained through LoRA. We use the AdamW optimizer38 with a learning rate of 3e-4 and a batch size of 32 (with gradient accumulation over 8 steps). The models are trained on 8 NVIDIA A100 GPUs (80GB memory each) using DeepSpeed39 for efficient distributed training. We split the Washington and Illinois dataset into training, validation, and test sets in a 7:1.5:1.5 ratio. Since the Washington dataset contains relatively few crash events per year, we utilized as many reports as possible to ensure sufficient training data. However, the data distribution across different classes is highly imbalanced. The imbalanced data distribution presents a great challenge for the model's training and evaluation. During the fine-tuning, we used a uniform sampling strategy to train the model on this unbalanced data. Similarly, to facilitate the model's evaluation, for the validation set and test set, we removed most of the data with a crash severity category of S1. To balance the validation and test set for better evaluation, we removed 1428 S1 data and used 1000 remaining data for the validation set and test set separately. Compared with Washington state, more crash records can be used in Illinois state to generate a dataset. As a result, we were able to balance all subsets, including the training, validation, and test sets. In evaluating the model performance as a classification task, we employ weighted accuracy, precision, and F1-score as metrics. In the context of a classification task, we have four notations: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). Using these notations, we can represent the metrics as follows: Accuracy is one of the most commonly used measures for the classification performance, and it is defined as a ratio between the correctly classified samples to the total number of samples as follows: F1-score combines results on precision and recall. It is the harmonic mean of precision and recall, which can be calculated using the formula: We follow recent studies40 and adopt machine learning models, including XGBoost41, Random Forest (RF)42, Decision Trees (DT)43, Adaptive Boosting (AdaBoost)44, Logistic Regression (LR)45, and Categorical Boosting (CatBoost)46. We also include deep learning models such as BERT47 and TabNet48. In addition, we consider the National Average49, which predicts crash severity distributions using calibrated Severity Distribution Functions. For these models, the Bayesian optimization method (BayesSearchCV) is used to facilitate the identification of optimal hyperparameters, such as max_depth and learning_rate. To ensure a fair comparison across baseline models, we retained the original architecture and design of each model, modifying only the input data format when necessary. Detailed information on hyperparameter settings and input data preprocessing for all baseline models is provided in Supplementary Section 1.2. The experiments on the North Carolina, Maine, and Ohio datasets are conducted under a zero-shot setting, where the model is fine-tuned only on the Illinois dataset and has never seen data from North Carolina, Maine, and Ohio during training. Traditional machine learning models perform poorly in this context due to their limited ability to adapt. Therefore, to ensure a fair comparison under the same conditions, we introduce two baseline methods: Leveraging its pre-training on large corpora, BERT possesses a certain degree of generalization capability. In our experiments, we fine-tune BERT using prompts from the Illinois dataset and evaluate its zero-shot performance on the North Carolina and Maine datasets. Chain-of-thought (CoT) reasoning enables language models to perform multi-step inference by generating intermediate reasoning steps before arriving at a final answer. Zhen et al.23 explored the use of CoT for zero-shot crash severity prediction and reported improved performance over standard LLM prompting. Following their approach, we apply CoT prompting to evaluate zero-shot performance on the North Carolina and Maine datasets. Shapley value is a concept from cooperative game theory that has been widely adopted in machine learning to interpret model predictions51. It provides a way to fairly allocate the contribution of each feature to the outcome of a predictive model. In essence, the Shapley value quantifies how much each feature contributes to a prediction by considering all possible combinations of features. Formally, the Shapley value φ of a feature (or player) i in a cooperative game is defined as: where N = {1, 2, …, n} is the index set of n features, S is a subset of N, and v(S) is the utility of the subset S, which represents a measurable value, such as accuracy or prediction score, achieved by the model using only the subset S of features. The Shapley value is utilized in both the training and inference stages in SafeTraffic Copilot. As outlined in “Developing SafeTraffic LLM for predicting crashes” in the “Results” section, the jth prompt Tj in the dataset P is divided into five parts: c0: system prompt (i.e., “You are a helpful assistant designed to predict the severity of a traffic crash..."), c1: general information, c2: infrastructure information, c3: event information, and c4: unit information. Referring to Equation (7), the contribution of part ci at training, \({\varphi }_{i}^{\,{\mbox{train}}\,}\), is where N = {1, 2, 3, 4} indexes the four content parts, and v(P(S)) is a performance metric (e.g., accuracy) obtained after retraining the model only on prompts in P(S). Unlike traditional machine learning models that primarily handle fixed-length feature vectors, LLMs process variable-length text sequences as input52. This characteristic makes commonly used Shapley value approximation methods, such as KernelSHAP53 and DeepSHAP, less applicable to LLMs. Recent approaches like TokenSHAP54 and TransSHAP55 have been proposed to address this by decomposing input text into tokens and computing Shapley values at the token level. However, applying token-level Shapley value computation to SafeTraffic LLM introduces two primary challenges: (1) Computational limitations. In our SafeTraffic LLM, with an input size of approximately 500 tokens, the large-scale computation of token-level Shapley values for crash data becomes impractical. Decomposing the prompt at the token level disregards inter-token dependencies, and the arbitrary masking or replacement of tokens can lead to semantic ambiguity and contextual shifts. These issues hinder a precise understanding of how individual features contribute to predictions. Moreover, paragraph-level analysis is too coarse for detailed attribution, since it can merge distinct features into a single category (e.g., driver and vehicle details under “unit information”). To overcome these limitations, we propose a sentence-level feature contributions calculation method for inputs of LLMs, which proceeds as follows: The prompts are segmented using delimiters (e.g., commas “,” or periods “.”) to produce sentence-level units. GPT-4o is used to group and label these sentences (see Fig. Each group is represented as ck, where \(k\in {N}^{{\prime} }=\{1,2,3,\ldots \,n\}\). Given index set \({S}^{{\prime} }\subseteq {N}^{{\prime} }\setminus \{\,i\}\), we can construct the prompt \({T}_{j}({S}^{{\prime} })\) similar to the process Equation (8). A higher \({\varphi }_{i,j}^{\,{\mbox{inf}}\,}\) indicates a greater contribution of the ith sentence group to the model's confidence for predicting yj. To reduce computational overhead, we adopt a stratified sampling–based Shapley estimation method using complementary contributions36. The examples of processed prompts generated in this study have been deposited in Zenodo (https://zenodo.org/records/16896765). The completed HSIS raw data29 are available under restricted access, as HSIS is designed to provide data solely for research conducted in the public interest and intended for publication in a scientific journal or other national publication. Access can be obtained by submitting a formal request to HSIS staff via hsis@dot.gov, accompanied by a description of the proposed research and confirmation of compliance with HSIS usage guidelines. Detailed information can be found at https://highways.dot.gov/research/safety/hsis. Source data are provided with this paper. Code is publicly accessible at https://zenodo.org/records/1689676556. Road Safety Annual Report 2023 (OECD Publishing, 2023). Islam, M. R., Wang, D. & Abdel-Aty, M. Calibrated confidence learning for large-scale real-time crash and severity prediction. Bougna, T., Hundal, G. & Taniform, P. Quantitative analysis of the social costs of road traffic crashes literature. & Ge, T. Applications of machine learning methods in traffic crash severity modelling: current status and future directions. Yan, X. et al. Learning naturalistic driving environment with statistical realism. Carrodano, C. Data-driven risk analysis of nonlinear factor interactions in road safety using Bayesian networks. Mannering, F. L. & Bhat, C. R. Analytic methods in accident research: methodological frontier and future directions. & Hassan, H. M. A deep learning based traffic crash severity prediction framework. Sattar, K. et al. Transparent deep machine learning framework for predicting traffic crash severity. Singhal, K. et al. Large language models encode clinical knowledge. Gao, C. et al. S3: social-network simulation system with large language model-empowered agents. Schulze Buschoff, L. M., Akata, E., Bethge, M. & Schulz, E. Visual cognition in multimodal large language models. Advancing real-time infectious disease forecasting using large language models. Larger and more instructable language models become less reliable. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Explainable and interpretable multimodal large language models: a comprehensive survey. Fan, Z. et al. Learning traffic crashes as language: Datasets, benchmarks, and what-if causal analyses. & Liu, N. Leveraging large language models with chain-of-thought and prompt engineering for traffic crash severity analysis and inference. Leveraging Artificial Intelligence and Big Data to Enhance Safety Analysis: A Guide (eds Wang, Y. et al) (The National Academies Press, Washington, DC, 2025). Pei, X., Wong, S. & Sze, N.-N. A joint-probability approach to crash prediction models. Abdel-Aty, M., Hasan, T. & Anik, B. T. H. An advanced real-time crash prediction framework for combined hard shoulder running and variable speed limits system using transformer. Analysis of types of crashes at signalized intersections by using complete crash data and tree-based regression. The statistical analysis of highway crash-injury severities: a review and assessment of methodological alternatives. U.S. Department of Transportation, Federal Highway Administration. From shapley values to generalized additive models and back. International Conference on Artificial Intelligence and Statistics 709–745 (PMLR, 2023). Shapley, L. S. A value for n-person games. in Contributions to the Theory of Games II (eds Kuhn, H. W. & Tucker, A. W.) 307–317 (Princeton University Press, 1953). Revised code of washington: Driving under the influence. Zhang, J., Huang, J., Jin, S. & Lu, S. Vision-language models for vision tasks: a survey. Efficient sampling approaches to shapley value approximation. Hu, E. J. et al. LoRA: low-rank adaptation of large language models. & Sabuj, S. R. A study on road accident prediction and contributing factors using explainable machine learning models: ANAlysis and performance. Chen, T. & Guestrin, C. XGBoost: a scalable tree boosting system. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16 785–794. & Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. Cox, D. R. The regression analysis of binary sequences. Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V. & Gulin, A. CatBoost: unbiased boosting with categorical features. Advances in Neural Information Processing Systems Vol. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) 4171–4186 (Association for Computational Linguistics, 2019). & Pfister, T. Tabnet: Attentive interpretable tabular learning. AAAI Conference on Artificial Intelligence Vol. Transportation Research Board and National Academies of Sciences, Engineering, and Medicine, Lord, D., Geedipally, S., Pratt, M. P., Park, E. S., Khazraee, S. H. & Fitzpatrick, K. Safety Prediction Models for Six-Lane and One-Way Urban and Suburban Arterials (The National Academies Press, 2022). Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems Vol. Explaining a series of models by propagating shapley values. Algorithms to estimate shapley value feature attributions. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. Goldshmidt, R. & Horovicz, M. TokenSHAP: interpreting large language models with monte carlo shapley value estimation. Kokalj, E., Škrlj, B., Lavrač, N., Pollak, S. & Robnik-Šikonja, M. BERT meets shapley: extending SHAP explanations to transformer-based classifiers. EACL Hackashop on News Media Content Analysis and Automated Report Generation (eds Toivonen, H. & Boggia, M.) 16–21 (Association for Computational Linguistics, 2021). Zhao, Y., Wang, P., Zhao, Y., Du, H. & Yang, H. F. SafeTraffic Decision Copilot: adapting large language models for trustworthy safety assessments and policy interventions (this paper). Puw242/SafeTraffic: Release for paper version, Zenodo, https://doi.org/10.5281/zenodo.16896764 (2025). The authors would like to thank Dr. Wei Zhang of the Federal Highway Administration (FHWA) and Jeffrey P. Michael, EdD, of the Bloomberg School of Public Health at Johns Hopkins University, for their valuable input on improving SafeTraffic Copilot. The authors are also grateful to the Office of Safety and Operations R&D, Dr. Carol Tan, and Jessica G. Rich of the FHWA for their generous support in providing the Highway Safety Information System (HSIS) data. These authors contributed equally: Yang Zhao, Pu Wang. Department of Civil and Systems Engineering, Johns Hopkins University, Baltimore, MD, USA Center for Systems Science and Engineering, Johns Hopkins University, Baltimore, MD, USA Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar provided guidance and feedback for the study. The authors declare no competing interests. Nature Communications thanks Dungar Singh and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Zhao, Y., Wang, P., Zhao, Y. et al. SafeTraffic Copilot: adapting large language models for trustworthy traffic safety assessments and decision interventions. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/10/251006051127.htm'>Scientists finally reveal what's behind long COVID's mysterious brain fog</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 06:48:15
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Among the most common and debilitating of these is cognitive impairment, often referred to as "brain fog," which affects over 80% of people with Long COVID. Given the hundreds of millions of global cases, Long COVID represents a massive public health and socioeconomic challenge, as it severely impacts people's ability to work and perform daily activities. Unfortunately, despite its prevalence, the underlying causes of Long COVID and brain fog remain poorly understood. Since it's difficult to observe the molecules that govern communication between brain cells directly, researchers are left without objective biomarkers to confirm a Long COVID diagnosis or develop therapies. To address this challenge, a research team led by Professor Takuya Takahashi from the Graduate School of Medicine at Yokohama City University, Japan, has made a significant breakthrough in understanding the cause of Long COVID brain fog. As explained in their paper, published in Brain Communications on October 1, 2025, the team hypothesized that patients with brain fog might exhibit disrupted expression of AMPA receptors (AMPARs) -- key molecules for memory and learning -- based on prior research into psychiatric and neurological disorders such as depression, bipolar disorder, schizophrenia, and dementia. This elevated receptor density was directly correlated with the severity of their cognitive impairment, suggesting a clear link between these molecular changes and the symptoms. Additionally, the concentrations of various inflammatory markers were also correlated with AMPAR levels, indicating a possible interaction between inflammation and receptor expression. Taken together, the study's findings represent a crucial step forward in addressing many unresolved issues regarding Long COVID. For example, drugs that suppress AMPAR activity could be a viable approach to mitigate brain fog. Interestingly, the team's analysis also demonstrated that imaging data can be used to distinguish patients from healthy controls with 100% sensitivity and 91% specificity. "By applying our newly developed AMPA receptor PET imaging technology, we aim to provide a novel perspective and innovative solutions to the pressing medical challenge that is Long COVID," remarks Prof. Takahashi. "Our findings clearly demonstrate that Long COVID brain fog should be recognized as a legitimate clinical condition. This clinical trial project was supported by donations from the READYFOR crowdfunding platform. ), the Japan Agency for Medical Research and Development (AMED) under grant numbers JP24wm0625304 (T.T. Scientists Discover a Protein That Literally Punches Holes in the Heart Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/10/251006051124.htm'>Scientists find brain circuit that traps alcohol users in the vicious cycle of addiction</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 06:01:44
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>What compels someone to keep engaging in alcohol use, even if it damages their health, relationships and wellbeing? They found that this region becomes more active, driving strong relapse behavior, when rats learn to associate environmental stimuli with the easing of withdrawal symptoms by alcohol. By illuminating this brain pathway, the research sheds light on one of the most stubborn features of addiction -- drinking not for pleasure, but to escape pain -- and could eventually lead to new treatments for substance use disorders (SUDs) as well as other maladaptive behaviors including anxiety. "What makes addiction so hard to break is that people aren't simply chasing a high," says Friedbert Weiss, professor of neuroscience at Scripps Research and senior author of the study. This work shows us which brain systems are responsible for locking in that kind of learning, and why it can make relapse so persistent." "This brain region just lit up in every rat that had gone through withdrawal-related learning," says co-senior author Hermina Nedelescu of Scripps Research. "It shows us which circuits are recruited when the brain links alcohol with relief from stress -- and that could be a game-changer in how we think about relapse." An estimated 14.5 million people in the United States have alcohol use disorder, which encompasses a range of unhealthy drinking behaviors. Like other drug addictions, alcohol addiction is characterized by cycles of withdrawal, abstinence and relapse. When rats initially begin drinking, they learn to associate pleasure with alcohol and seek more. However, that conditioning becomes far stronger during multiple cycles of withdrawal and relapse. After learning that alcohol eased the unpleasant feelings of withdrawal -- what scientists call negative reinforcement or a relief of 'negative hedonic state' -- the animals sought out more alcohol and would remain persistent even when uncomfortable. "When rats learn to associate environmental stimuli or contexts with the experience of relief, they end up with an incredibly powerful urge to seek alcohol in the presence of that stimuli -even if conditions are introduced that require great effort to engage in alcohol seeking," says Weiss. The researchers used advanced imaging tools to scan entire rat brains, cell by cell, and pinpoint areas that became more active in response to alcohol-related cues. While several brain areas showed increased activity in the withdrawal-learned rats, one stood out: the PVT, which is known for its role in stress and anxiety. "In retrospect, this makes a lot of sense," says Nedelescu. "The unpleasant effects of alcohol withdrawal are strongly associated with stress, and alcohol is providing relief from the agony of that stressful state." Environmental stimuli conditioned to negative reinforcement -- the drive to act in order to escape pain or stress -- is a universal feature of the brain, and can drive human behavior beyond substance use disorders such as anxiety disorders, fear-conditioning and traumatic avoidance learning. "This work has potential applications not only for alcohol addiction, but also other disorders where people get trapped in harmful cycles," says Nedelescu. For now, the new study underscores a key shift in how basic scientists think about addiction. "This study shows us where in the brain that learning takes root, which is a step forward." Scientists Halt Toxic Brain Protein Behind Parkinson's in Landmark Study This Vitamin B3 Supplement Could Cut Your Skin Cancer Risk by up to 54% Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/10/251006051117.htm'>You don't have to lose weight to lower your diabetes risk, scientists say</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-07 03:03:13
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Until now, weight reduction has been the primary therapeutic goal for people with prediabetes. An analysis of a large Tübingen study shows that patients who bring their blood sugar levels back within the normal range through a healthy lifestyle but do not lose weight, or even gain weight, still reduce their risk of type 2 diabetes by 71 percent. Prediabetes is a condition in which blood sugar values are elevated but do not yet meet the criteria for diabetes. It often remains undetected for a long time, as affected individuals initially have no symptoms. The body's cells become more resistant to endogenous insulin hormone. The risks are considerable: If left untreated, there is a high risk of developing type 2 diabetes later on -- a disease that affects more than 460 million people worldwide. It can lead to serious complications, such as cardiovascular disease or cancer. Strategies recommended to date -- including in current guidelines -- for the prevention of type 2 diabetes in people with prediabetes primarily focus on reducing weight through a healthy diet and increased physical activity. Nevertheless, a good 22 percent of them normalized their blood sugar levels. This figure is almost identical to that of individuals who were able to reduce their risk of type 2 diabetes by losing weight (73 percent). Visceral fat releases signaling molecules that promote inflammation and disrupt hormone balance, which leads to insulin resistance and is thus directly linked to type 2 diabetes. Study participants whose blood sugar levels returned to normal without losing weight had a lower percentage of abdominal fat as a result of lifestyle changes compared to those whose blood sugar levels remained in the prediabetes range. Losing weight remains helpful, but our data suggests that it is not essential for protection against diabetes," he continues. "In future, guidelines for the prevention and treatment of type 2 diabetes should not only take weight into account, but above all blood glucose control and fat distribution patterns," adds Prof. Dr. Reiner Jumpertz-von Schwartzenberg, who, as last author, was involved in the study alongside Prof. Dr. Birkenfeld. However, the study results highlight the importance of including target glycemic values, i.e., guideline blood sugar values, in practice guidelines in addition to weight reduction targets. Prediabetes remission is the most effective way to prevent future type 2 diabetes, and the analysis suggests that this is partly independent of weight loss. Scientists Halt Toxic Brain Protein Behind Parkinson's in Landmark Study This Vitamin B3 Supplement Could Cut Your Skin Cancer Risk by up to 54% Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            