
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - SCIENCE Article Summaries - 2025-11-20</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            SCIENCE
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/a69451485/your-brain-literally-falls-asleep/'>Scientists Pinpointed the Exact Moment Your Brain Literally ‘Falls' Asleep</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-11-20 14:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>After you close your eyes, there is actually a certain point that brain activity falls off sharply, plunging until it reaches a sleep state. We may earn commission if you buy from a link. Until now, it used to be thought that falling asleep was a gradual process, like being buoyed by clouds and drifting into dreamland. This is what a team of researchers from Imperial College London and the UK Dementia Research Institute, led by neuroscientist Nir Grossman, found out when they hooked subjects up to an EEG and saw that brain activity remains stable until there is a drastic drop in the final minutes before sleep sets in. Previous attempts to define the moment the brain transitions from a waking state to a sleep state had relied on short and mostly inconsistent EEG data fragments. Sleep does not lie at the end of a winding road, but rather at the bottom of an abrupt drop. There is a reason so many magazine articles keep urging you to get more sleep. It boosts regenerative processes such as homeostatic plasticity, meaning the adaptation of neurons to overall neural network activity so they can keep up normal communication with each other. Memories are also stabilized as information is reorganized during sleep, making it less likely to forget. The transition between waking and sleeping states is overseen by clusters of neurons known as nuclei. Some of these nuclei promote wakefulness while others bring on sleep. For the brain to go into sleep mode, certain functions in both nuclei are switched off, which rapidly changes how neurons communicate. What Grossman and his team found is that the transition between states has a defined boundary as opposed to the brain gradually fading into sleep. Sleep distance is how long it takes to fall asleep from the moment you close your eyes. Though it seems counterintuitive, EEG monitoring revealed that if it takes longer fall asleep, less of that time will be spent in the falling phase than for someone who falls asleep sooner. Why the fall takes longer for shorter sleep distances can be explained by the occipital cortex, which processes visual information, reaching that point of falling earlier than the frontal cortex, which is involved with thinking, emotion, personality and memory along with muscle movement and control. Predictions were within about 49 seconds of the actual fall. Recording brain activity for just one night showed that there would be 95% similarity on later nights. Being able to identify the point of no return for falling asleep isn't only demystifying how the brain transitions from awake to asleep. It could also help create new treatments for disordered sleep that not only increase quality of life, but promote safety in situations where someone needs to be fully awake and alert. Her work has appeared in Popular Mechanics, Ars Technica, SYFY WIRE, Space.com, Live Science, Den of Geek, Forbidden Futures and Collective Tales. She lurks right outside New York City with her parrot, Lestat. When not writing, she can be found drawing, playing the piano or shapeshifting. Bees Can Also Learn the Basics of Morse Code The Raw Power of DNA Could Save Our Data</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/animals/a69446658/morse-code/'>Bees Understand Morse Code. It Could Change How We See Human Intelligence.</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-11-20 13:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Bees obviously can't use this skill on human level, but this progress could help scientists better grasp complex cognitive-like traits. We may earn commission if you buy from a link. Even von Frisch, who won the Nobel Prize for his bee-decoding efforts in 1974, once wrote that “the brain of a bee is the size of a grass seed and is not made for thinking. The actions of bees are mainly governed by instinct.” For example, these tiny critters use tools, understand abstract mathematical concepts, and recognize human faces—all with just 0.01 percent of the neurons found in a human brain. Now, scientists are adding to this list of impressive accolades with a new study, published in the journal Biology Letters. “Since bees don't encounter flashing stimuli in their natural environment, it's remarkable that they could succeed at this task,” Alex Davidson, the lead author of the study and a Ph.D. student from Queen Mary University, said in a press statement. “The fact that they could track the duration of visual stimuli might suggest an extension of a time processing capacity that has evolved for different purposes, such as keeping track of movement in space or communication.” While bees aren't going to suddenly start typing out complex messages to their buzzing buddies—one dot and one dash at a time—this experiment does confirm the simple-yet-groundbreaking idea that certain insect minds can keep track of time. The researchers note that, while they don't know of a neural mechanism that explains this mental behavior, future studies could test different ideas in these miniature brains and see what they discover. What they find could even have an impact on human-made technologies. “Processing durations in insects is evidence of a complex task solution using minimal neural substrate,” Elisabetta Versace, a senior author of the study from Queen Mary University, said in a press statement. “This has implications for complex cognitive-like traits in artificial neural networks, which should seek to be as efficient as possible to be scalable, taking inspiration from biological intelligence.” It's true that the brain of a bee is the size of a seed, but that seed certainly has sprouted into a complex mental tree—one that scientists are only beginning to understand. Experts Found a Mass Grave of Roman Soldiers This Strange Blue Goo Contains Evidence of Life</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41467-025-64990-y'>Systematic benchmarking of imaging spatial transcriptomics platforms in FFPE tissues</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-11-20 12:35:01
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Emerging imaging spatial transcriptomics (iST) platforms and coupled analytical methods can recover cell-to-cell interactions, groups of spatially covarying genes, and gene signatures associated with pathological features, and are thus particularly well-suited for applications in formalin fixed paraffin embedded (FFPE) tissues. Here, we benchmark the performance of three commercial iST platforms—10X Xenium, Vizgen MERSCOPE, and Nanostring CosMx—on serial sections from tissue microarrays (TMAs) containing 17 tumor and 16 normal tissue types for both relative technical and biological performance. On matched genes, we find that Xenium consistently generates higher transcript counts per gene without sacrificing specificity. Xenium and CosMx measure RNA transcripts in concordance with orthogonal single-cell transcriptomics. All three platforms can perform spatially resolved cell typing with varying degrees of sub-clustering capabilities, with Xenium and CosMx finding slightly more clusters than MERSCOPE, albeit with different false discovery rates and cell segmentation error frequencies. Taken together, our analyses provide a comprehensive benchmark to guide the choice of iST method as researchers design studies with precious samples in this rapidly evolving field. Spatial transcriptomics (ST) tools measure the gene expression profiles of tissues or cells in situ. These approaches overcome the limitations of single-cell RNA-sequencing (scRNA-seq) methods by negating the need for cell dissociation, thus maintaining both local and global spatial relationships between cells within a tissue. These advantages, coupled with rapidly emerging computational and analytical methods, have led to substantial excitement about deploying these platforms in fundamental biology studies and in the clinic for research and diagnostic purposes3,4,5. ST tools can be split into two broad categories: sequencing (sST) and imaging (iST) based modalities. sST methods tag transcripts with an oligonucleotide address indicating spatial location, most commonly by placing tissue slices on a barcoded substrate; isolating tagged mRNA for next-generation sequencing; and computationally mapping transcript identities to locations6. In contrast, iST methods most commonly use variations of fluorescence in situ hybridization (FISH) where mRNA molecules are tagged with hybridization probes, which are detected in a combinatorial manner over multiple rounds of staining with fluorescent reporters, imaging, and de-staining (Fig. Computational reconstruction then yields maps of transcript identity with single-molecule resolution. Compared to sST methods, iST methods are targeted to subsets of the transcriptome due to their reliance on pre-defined gene panels and they adopt the higher spatial resolution and sensitivity of FISH, yielding single-cell resolution data8. a Overall approach for generating iST data. b Different amplification approaches for Xenium, MERSCOPE, and CosMx. While the iST methods share some similarities, significant differences arise in primary signal detection and amplification, sample processing, and the subsequent fluorescent cycling chemistry (Fig. The need for amplification of signal is coupled to the sample processing, namely whether the sample is cleared, gel-embedded, or photobleached to quench autofluorescence. There are tradeoffs due to differences in sample processing for each iST method. For example, clearing of the sample increases signal quality but can prevent follow-up H&E staining and complicate immunostaining, which, in turn, can make cell segmentation more challenging. Finally, there are tradeoffs between imaging time, molecular plex, and imaging area covered, which result from the particular combination of the molecular protocol and the imaging hardware implementation12. A key historic limitation in the widespread use of iST methods with human clinical samples was the incompatibility of most methods with formalin-fixed, paraffin-embedded (FFPE) tissue samples13,14. FFPE is the standard format for clinical sample preservation for pathology due to its ability to maintain tissue morphology and sample stability at room temperature for decades, accounting for over 90% of clinical pathology specimens15. The ability to process FFPE samples with iST will enable the use of archival tissue banks for studies and obviate the need for specialized sample harvesting workflows. However, FFPE samples tend to suffer from decreased RNA integrity, particularly after having been stored in archives for extended periods of time16. Three companies recently released the first FFPE compatible commercial iST platforms: 10x's Xenium, NanoString's CosMx, and Vizgen's MERSCOPE9,10,11,17. These three platforms each use different protocols, probe designs, signal amplification strategies, and computational processing methods, and therefore may potentially yield different sensitivities and downstream results. The main chemistry difference lies in transcript amplification: 10x Xenium uses a small number of padlock probes with rolling circle amplification; CosMx uses a low number of probes amplified with branch chain hybridization; and MERSCOPE uses direct probe hybridization but amplifies by tiling the transcript with many probes (Fig. However, no head-to-head performance comparisons on matched samples have been published. Understanding the key differences across platforms will allow users to make better-informed decisions regarding panel design, method choice, and sample selection as they design costly experiments, often on precious samples that have been bio-banked for years18. In this study, we compared currently available FFPE-compatible iST platforms on matched tissue samples. We prepared a set of samples representative of typical archival FFPE tissues, comprised of 33 different tumor and normal tissue types, and acquired matched data from sequential sections according to the manufacturer's best practices at the time of writing, generating a dataset of >5.0 M cells. We analyzed the relative sensitivity and specificity of each method on shared transcripts and further quantified the concordance of the iST data across each platform with paired scRNA-seq data collected by 10x Chromium Single Cell Gene Expression FLEX. Then we focused on cell-level comparisons, evaluating the out-of-the-box segmentation for each platform based on detected genes and transcripts and coexpression patterns of known disjoint markers. Finally, we cross-compared the ability of each platform to identify cell type clusters with breast and breast cancer tissues as an example use case. Taken together, our work provides the first head-to-head comparison of these platforms across multiple archival healthy and cancerous FFPE tissue types. To test the performance of the latest generation of FFPE-compatible iST tools, we sought to match gene expression and sample as much as possible given available panel configurations and manufacturer guidelines. To accomplish this, we used three previously generated multi-tissue tissue microarrays (TMAs) from multiple types of clinical discarded tissue (see Methods). We focused on FFPE tissues as the standard method for sample processing and archival in pathology. Tumor TMA 1 (tTMA1) consisted of one hundred and seventy 0.6 mm diameter cores (i.e., sampled regions) from seven different cancer types, with 3-6 patients per cancer type, and 3–6 cores per patient (Fig. Tumor TMA 2 (tTMA2) consisted of forty-eight 1.2 mm diameter cores from nineteen different cancer types, with each tissue type coming from one or two patients and represented in 2–3 cores (Fig. A normal tissue TMA (nTMA) contained forty-five 1.2 mm diameter cores spanning sixteen normal tissue types isolated with each tissue type coming from one patient and represented in 2–3 cores (Fig. CosMx and Xenium suggest pre-screening samples based on H&E, while MERSCOPE recommends a DV200 > 60%. Since our goal was to determine the compatibility of iST platforms under typical workflows for standard biobanked FFPE tissues from clinical pathology labs, and since TMAs are challenging to assay by DV200, samples were not prescreened based on RNA integrity. Samples were screened by H&E in the process of TMA assembly. TMAs were sliced into serial sections for processing by 10x Xenium, Vizgen MERSCOPE, and NanoString CosMx, following manufacturer instructions (see Methods). In terms of panel design, MERSCOPE and Xenium offer either fully customizable panels or standard panels with optional add-on genes, while CosMx offers a standard 1 K (substantially larger plex) panel with optional add-on genes (while this paper was in review Xenium and CosMx began providing options for 5000 and 6000 gene, respectively). We then designed two MERSCOPE panels to match the pre-made Xenium breast and lung panels, by filtering out any genes which could potentially lead to high expression flags in any tissue in the Vizgen online portal. This resulted in a total of six panels, with each panel overlapping the others on > 65 genes (Supplementary Table 4). We carried out multiple runs with each panel following manufacturer instructions as provided in 2023 and 2024 (Supplementary Table 5), with efforts made to ensure that head-to-head comparisons were available at similar time points for each pair of platforms. 2024 data was acquired on tTMA1 and tTMA2 with only the breast and 1k panels, along with reference single-cell transcriptomic data from sequential slices. In an intentional deviation from manufacturer instructions, the 2024 round of tTMA1 was intentionally carried out with matched baking times after slicing for a head-to-head comparison on equally prepared tissue slices. We note that between 2023 and 2024, CosMx updated its detection algorithms and Xenium improved its segmentation capabilities by adding additional membrane staining. and tables as useful comparisons for previous datasets acquired on these platforms. Each data set was processed according to the standard base-calling and segmentation pipeline provided by each manufacturer. The resulting count matrices and detected transcripts were then subsampled and aggregated to individual cores of the TMA (Methods). Across all datasets we generated 394,635,679 transcripts and 5,017,397 cells. Overall, we found that the cores from each TMA were generally well adhered to the tissue and detected transcripts. The total number of transcripts recovered for each run in 2024 was highest for CosMx, followed by Xenium, and then MERSCOPE (Supplementary Table 6). Based on the initially reported number of transcripts, tTMA1 appeared to provide more counts than tTMA2 and nTMA, which we ascribed to differences in tissue quality (Supplementary Table 6). We note that the total number of transcripts from the MERSCOPE nTMA run was below what would be typically thought of as a successful run. Such a sample would normally be excluded from analysis, but we continued with the data through all analyses to illustrate how low transcript capture affects downstream results (Supplementary Table 6, Supplementary Figs. We next sought to directly compare the performance of each iST platform on matched genes. We began with a pseudo-bulk-based approach at the core level since this would not depend on differences in cell segmentation performance (see Methods)19. When the same genes were evaluated by Xenium and MERSCOPE in sequential sections across different panels, we found that the total transcript count of all shared genes was highly correlated across data sets acquired with different panels, regardless of the tissue of origin (Supplementary Fig. 1a) Deviation in total transcript count was primarily driven by changes in morphology of a core from section to section (Supplementary Fig. However, explorations on a gene-by-gene basis, even in morphologically well-matched cores showed that some genes consistently had higher or lower expression in the breast or lung panel, up to 3-fold (Supplementary Fig. We ascribe these differences primarily to slight competition between probes but note that the magnitude of this effect is comparable to findings in bulk and single cell RNA-seq methods in FFPE tissue20. To explore technical replicability further we examined pseudo-bulk gene expression correlation for cores from the same patient and same tissue type in the same dataset and found that correlation was high (Spearman's r > 0.74) in almost all cases (Supplementary Fig. 2a,b), indicating good sample-to-sample reproducibility within a given platform. These correlation was similar to that observed when comparing the same core profiled across slides with different panels, suggesting that each platform was highly self-consistent and matching previous reports for MERSCOPE finding high section to section correlation21,22. We also examined the correlation of the data from each platform between 2023 and 2024 using tTMA1 (23) and tTMA1 (24). Despite several differences in acquisition (age of sample, changes to the Xenium and CosMx analysis algorithms, and using matched baking conditions in 2024 vs manufacturer recommended conditions in 2023) we nonetheless found that the total transcript count of every shared gene from all matched cores was highly correlated (Spearman r = 0.99, 0.94, 0.95 for Xenium, MERSCOPE, and CosMx, respectively, Supplementary Fig. The correlation plots showed an average of only 18% median fold decrease in expression across all cores for Xenium in 2024 vs 2023; a four-fold increase for CosMx, and an eight-fold decrease for MERSCOPE—differences we ascribe primarily to the improved CosMx algorithm and changes in the baking time affecting MERSCOPE. Breaking down the correlations by core showed similar results regardless of the tissue of origin, though with more variance between the datasets for MERSCOPE and CosMx than for Xenium (Supplementary Fig. Thus, the correlation of gene expression across matched samples is high in all three technologies across a variety of conditions, but sensitivities could show significant differences. To evaluate the relative sensitivity of each platform, we plotted the total transcript counts of every shared gene between all combinations of platform and panel, summed across all matched cores. We found generally linear relationships between all pairs of platforms (Fig. When we initially examined CosMx compared to Xenium data using tTMA1 (23) we noted an upward curve in the lower expression regime indicative of higher-than-expected calls associated with the low expression regime by CosMx, though this is no longer present in tTMA1(24) likely because of the updated base calling algorithm (Supplementary Fig. Since the three methods use variable number of probes per transcript, we broke these analyses down by gene length, but found no noticeable differences between the performance on short, medium, or long transcripts (Supplementary Fig. Xenium consistently showed higher expression levels on the same genes than CosMx in tTMA1 (24) when all slides were treated the same way, with the Xenium breast dataset having 2-fold more counts than the CosMx multi-tissue data sets (Fig. CosMx showed higher expression levels than MERSCOPE (median of 11-fold). Finally, Xenium showed 20-fold higher median expression with the breast panels than MERSCOPE. In tTMA2 (24), where slide histology was carried out according to manufacturer protocols, we nonetheless found generally consistent results, though the MERSCOPE data set showed lower expression levels (Supplementary Fig. Intriguingly, in the earlier round of experimentation, tTMA1 (23), while Xenium still showed higher counts than the other two platforms (Supplementary Fig. 3d–j), the MERSCOPE breast panel showed higher expression levels than CosMx levels, which were only 2.6-fold lower than Xenium's (Supplementary Fig. Considering the overall higher transcripts per cell across platforms for tTMA1 over tTMA2 and nTMA, and the differences in sample handling across rounds of experimentation (Supplementary Table 5), this suggests that MERSCOPE's ability to detect transcripts is highly sensitive to both sample quality and processing. a Scatter plots of summed gene expression levels (on a logarithmic scale) of every shared gene between Xenium (breast) and CosMx (1k) data, captured from matched cores from tTMA1(24). Each data point corresponds to a gene. b Same as (a) but between MERSCOPE (breast) and CosMx(1k). d Violin plot of percentage of all transcripts corresponding to genes relative to the total number of calls (including negative control probes and unused barcodes) averaged across cores of the same tissue type. Each data point represents a TMA-tissue type combination, such as tTMA1(24)-BrC or tTMA2(24)-BlC. Violins show kernel density; interior lines denote quartiles (median = 2nd quartile). The full data is shown in Supplementary Fig. e Violin plot of false discovery rate (FDR) where FDR(%) = (blank barcode calls / total transcript calls) x (Number of panel genes /Number of blank barcode) x 100. f Same as (e) but using negative control probes to replace blank barcodes. MERSCOPE is missing in this bar plot as it does not have negative control probes by design. h Same as (g) but normalized to the number of genes in a panel or in percentage. Pairwise differences between platforms were assessed with two-sided Mann–Whitney U tests for (d–h); brackets show unadjusted p-values for each comparison. We next wanted to assess the specificity of each platform. Each of the three platforms includes negative controls which are used to evaluate sample quality23,24. Xenium and CosMx include both negative probes (e.g., real probes targeting nucleic acids that are not present in human tissue) and negative barcodes (e.g., algorithmically allowable barcodes that are not associated with any probe in the experimental panel). MERSCOPE includes only negative barcodes by default. We found that Xenium consistently showed the highest on-target fraction, while CosMx was lowest across most tissue types except for bladder cancer and tonsil, where CosMx showed a higher on-target fraction than MERSCOPE (Supplementary Fig. However, this measurement is biased because of the relative numbers of controls and target barcodes. We therefore also adopted a false discovery rate (FDR) calculation which normalizes for these differences and is calculated against both the negative probes and negative barcodes (see Methods, Fig. We found that Xenium consistently showed the lowest FDR while CosMx showed the highest FDR in most cancer types (15 out of 22 TMA-cancer type combinations, see Supplementary Fig. 5b,c) regardless of whether we standardized to negative control barcodes or probes. Finally, we used the negative control barcodes to evaluate the number of genes reliably detected by each platform in each tissue type. For each core, we calculated the number of genes that were detected two standard deviations above the average expression of the negative control probes. These numbers were then averaged for cores of the same tissue type. Because the CosMx panel was almost three times larger, it yielded a larger absolute number of detected genes in all 22 TMA-cancer type combinations (Fig. CosMx also detected the highest fraction of genes in 15 out of 22 TMA-cancer type combinations, followed by Xenium (5 cancer types across two TMAs: breast cancer, melanoma, pancreatic cancer, kidney cancer, and SCC, see Supplementary Fig. 6–7), except that the 2024 version of CosMx chemistry shows an almost 10-fold lower FDR in some tissues, with a corresponding increase in number of genes above noise, regardless of whether sample histology was carried out the same way or following manufacturer instructions—again in line with increased sensitivity from CosMx's updated data processing. We next sought to determine whether a higher number of expressed genes is representative of increased sensitivity to real biology or increased false positive rates. We aggregated pseudo-bulk both tTMA1 (23 and 24) data from all panels of the three platforms and compared them to data from the TCGA25 program (see Methods)11. The performances across all other cancer types followed a similar trend, with CosMx showing the highest correlation coefficient for a particular tissue type followed by Xenium and then MERSCOPE (Supplementary Table 8). These results stand in contrast to results on tTMA1 (23) where each method was generally comparable in correlation coefficient (Supplementary Fig. We also observed similar correlation coefficients across lung and multi-tissue gene panels relative to bulk RNA-seq expression data (Supplementary Fig 8b). a Scatter plots of common genes, showing the averaged expression of a gene across breast cancer cores profiled by the indicated panel, normalized to 100,000 vs the average FPKM from TCGA for all samples of a matched tissue type from tTMA1 (24). 1st-order polynomial fitting was performed and is shown as a black line. b Same as (a) but showing breast cores from nTMA (23) dataset versus averaged nTPM values from GTEx breast samples. c Scatter plots of overlapping genes, showing the aggregated expression of a gene in tTMA1 (23) across smooth muscle cells profiled by the indicated panel vs the gene expression from scRNA-seq. f Comparisons of residuals between single-cell and tTMA1 (23)/tTMA1 (24) for all platforms. We also compared the pseudo-bulk results from nTMA(23) with bulk RNA-seq data obtained from GTEx12,26 The Xenium breast and CosMx data sets showed similar correlations to breast data obtained from GTEx, while the MERSCOPE had significantly lower correlation, consistent with a run which doesn't pass QC (Spearman's correlation coefficients of 0.33 vs 0.46, 0.57, respectively, Fig. These relative trends remained true across most normal tissue types, though we found that thyroid, pancreas, and lymph nodes showed the lowest correlations across all panels while prostate, tonsil, and liver showed the highest correlations (Supplementary Table 9). We also evaluated the expression of tissue-specific transcript markers across each platform by selecting marker genes from the GTEx database (see Methods). In nTMA (23), we found tissue-specific expression patterns of several of these markers across all selected panels when visualized across each healthy tissue type with Xenium showing the most distinct pattern followed by CosMx and finally MERSCOPE (Supplementary Fig. Overall, our comparison to TCGA and GTEx 2023 data suggests that while platforms may be more highly correlated to reference datasets in some cases, all are within a similar correlation regime regardless of tissue type. We next sought to validate the consistently higher correlation of CosMx vs Xenium vs MERSCOPE. Since TCGA and GTEx data are not generated from matched samples, we also performed a more stringent comparison by generating single-cell transcriptomic data for both tTMA1(24) and tTMA2(24), acquired on sequential sections (Fig. This resulted in a matched single-cell reference dataset comprised of 14,945 and 17,749 high-quality single cells respectively for tTMA1 and tTMA2. Since TMAs include many cell types which are difficult to separate in single cell workflows, we decided to subset out vascular smooth muscle cells based on canonical marker genes identified (Methods). This cell type is readily recognizable in each platform and thus serves as a good basis of comparison of relative gene expression across modalities. The observed population's showed high expression of genes known to be associated with smooth muscle cells (but not used for selection) such as MYH11, DST, and LUM; while showing low expression of genes not associated with smooth muscle cells such as CSTG, CD86, and PDCD1LG2. We then aggregated pseudo-bulk expression from all three spatial platforms and single-cell data and performed correlation analyses across all spatially measured genes in this cell type27. When we compared Spearman correlation coefficients, we recapitulated the trends observed in 2024 data compared to reference bulk RNA-seq: namely, CosMx had the highest correlation coefficients, followed by Xenium, and finally MERSCOPE (Fig. Importantly, this is true for both tTMA1 (23) and tTMA1 (24). We found that on tTMA1 (24), CosMx has significantly higher correlation coefficients relative to Xenium (0.77 vs 0.53, p = 1.2 × 10−10, Fisher's z transformation throughout) and MERSCOPE (0.6, p = 1.7 × 10−6) on tTMA1. However, when we restricted the same correlation analysis to only the expression of the common genes measured by each spatial platform relative to single-cell, the differences in correlation to single-cell data among the platforms were reduced and were no longer statistically significant on tTMA1 (24) (CosMx: 0.60, Xenium: 0.53, p = 0.23, MERSCOPE: 0.54, p = 0.20) while on tTMA2 (24) only the MERSCOPE difference remained significant (CosMx: 0.61, Xenium: 0.69, p = 0.84, MERSCOPE: 0.3, p = 1.9 × 10−3) (Supplementary Fig. We therefore conclude that the primary factor contributing to the higher correlation of CosMx was the larger panel size, which encompassed genes with a broader range of expression values. The availability of a matched reference data set across several replicates allowed us to probe whether there were platform specific systematic biases in measuring certain genes. By comparing the residuals of the fits to single cell data we found that all three platforms consistently over- or undercounted- certain genes—largely non-overlapping sets—relative to single-cell measurements, presumably due to differences in sensitivities among probe designs and decoding chemistries (Fig. While the random noise between replicates could be of equal magnitude to this effect as evidence by genes with large residuals in one data set or another, this systematic effect could explain the spread in gene expression observed between platforms in Fig. Next, we compared the performance of each iST method on a single-cell level. As of 2024, all three platforms generate cell boundaries based on a DAPI image combined with a membrane marker (Fig. Raw images are provided for these stains to facilitate custom cell segmentation approaches, but we opted to use out-of-the-box cell segmentation method for each individual platform to replicate a real-world use-case of these platforms. We evaluated the cell segmentation accuracy by comparing the segmentation outputs against manual annotations of a subset of tTMA1 (24). This subset included three TMA cores selected for their morphological similarity across platforms and their representation of different cellular structures (see Methods) and yielded > 31,400 annotated cells (see Methods) in total. In dense cell distributions, CosMx and Xenium achieved significantly higher precision (0.90) than MERSCOPE (0.83, p < 0.01). Recall and F1 score results are consistent with precision across each platform. No significant performance difference was detected between CosMx and Xenium across the three scenarios. a Top row: Subset of data showing DAPI (blue fill) and membrane staining (green fill) overlaid with cell segmentation boundaries (white outline) and manually annotated cell centroid (red point). Bottom row: segmented cell boundaries (white outline) before and after filtration (Cyan outline: Cells kept after quality control; Orange outline: Cells excluded after quality control). Segmentation was performed on three representative TMA cores for each IST platform, yielding a total of 9 cores and 31,384 annotated cells. Pairwise platform differences were tested using two-sided Tukey's HSD following one-way grouping by platform (per core and scenario). Reported p-values are Tukey-adjusted for multiple comparisons. Exact adjusted p-values are shown above brackets. c Heatmap of transcripts per cell after filtration. All available genes are considered here for each panel. We filtered out cells with fewer than 10 transcripts for Xenium and MERSCOPE, and fewer than 20 transcripts for CosMx, in accordance with each platform's recommended threshold. d Same as (c) but showing unique genes per cell. e Same as (c) but reanalyzed using only shared genes. f Same as (d) but reanalyzed using only shared genes. g Co-expression density map for three pairs of disjoint genes (rows) from all three platforms (columns) from tTMA1 (24). MERSCOPE breast dataset does not have enough cells to generate the 2D histogram for PPARG vs. CD68. All cells across all tissues which include at least one detected transcript of either of the indicated genes are plotted together, with color indicating the number of cells at the indicated expression levels of each gene. We next filtered out empty regions of space and cells without any transcripts for downstream examination and quantified the fraction of cells containing differing numbers of transcripts per cell (Supplementary Fig. We chose a permissive threshold of removing cells with fewer than 10 transcripts for Xenium and MERSCOPE, and 20 transcripts for CosMx from downstream analysis as recommended by each technology11,28,29. tTMA1 (24) consistently had a greater fraction of cells passing filtration, with CosMx retaining the most cells (95.97%) followed by Xenium (94.28%) and MERSCOPE (27.97%) (Supplementary Table 6) while tTMA2 (24) had lower cell retention in Xenium (92.66%) and MERSCOPE (2.98%) but slightly higher cell retention in CosMx (96.30%). Unsurprisingly, filtration decreased the number of retained cells per unit area for all platforms, with the smallest decrease coming for CosMx and Xenium (Supplementary Fig. The cells retained from CosMx and Xenium had similar areas, while filtration of the MERSCOPE data sets resulted in a higher average cell area (Supplementary Fig. This effect is largely driven by the removal of low-quality cells, specifically those with fewer than 10 detected transcripts. After filtration, we compared the number of transcripts and the number of unique genes per retained cell across all tissues and all panels, focusing on tumor cores that were sampled by all three platforms (Fig. As expected, given its larger panel size, CosMx detected the highest number of transcripts per cell and the highest number of unique genes per cell in all tissue types, followed by Xenium breast panel and then MERSCOPE breast panel (Fig. If these analyses were restricted to only the shared genes across all panels, numbers were much lower (Fig. 4e, f), with the Xenium breast panel giving the highest numbers of transcripts per cell in 21 out of 22 TMA-cancer type combinations. The CosMx data showed the highest number of transcripts per cell in liver cancer and comparable transcript counts in testicular cancer to the Xenium breast panel. The MERSCOPE data generally had the lowest number of transcripts per cell, though pancreatic cancer approached the results from Xenium, and had higher transcripts per cell than CosMx (Fig. Xenium had the highest number of unique genes per cell across all tissue types and followed by CosMx and MERSCOPE (Fig. When we performed similar analyses for the 2023 datasets, even though these did not include membrane segmentation for Xenium (Supplementary Fig. 10d–g) and note that MERSCOPE showed significantly higher numbers of transcripts. This is consistent with our previous results (Supplementary Fig. 2i) showing that in certain conditions MERSCOPE could approach Xenium transcript counts. We then determined how different iST platforms' segmentation algorithms perform by assessing the expression of canonical markers. We examined the co-expression of CD3E, a canonical T-cell marker, and EPCAM, a marker for epithelial cells30,31 across all filtered cells; the co-expression of CD4 and CD8A, markers of T-cell subsets32,33,34; and the co-expression of PPARG, a marker for adipocytes and CD68, a marker for macrophages35,36,37. We reasoned that all these marker gene pairs are disjointly expressed, and a well-performing segmentation algorithm should yield few cells expressing both markers. We pooled all the filtered cells from matched tTMA1 (24) cores of each platform and plotted the expression of one gene against the other and converted the scatter plot to a 2D histogram showing cell numbers in each co-expression bin. We found that Xenium showed clear patterns of disjoint expression, separating cells from different lineages, while MERSCOPE showed such a pattern for CD3E vs. EPCAM but not for the other two pairs (Fig. Higher quality MERSCOPE data from tTMA1 (23) showed such a pattern for CD3E vs. CD19, CD4 vs. CD8A, and CD3E vs. EPCAM (Supplementary Fig. Similarly, CosMx only showed such a pattern for PPARG vs. CD68 but not for the other two pairs. Nevertheless, since the CosMx panel is much higher plex, and retained similar numbers of transcripts and genes to Xenium, we next wondered how these two methods performed in terms of cell type recovery. In a typical iST workflow, a key step is reducing the dimensionality of the data by identifying cell types, their unique states, and their expression patterns for further analysis leveraging spatial information38. To compare across platforms, we clustered the data from the filtered cells from all the cores for each TMA with a focus on breast tissues. The initial clustering of whole TMA datasets (except MERSCOPE normal tissue) showed expected batch effects caused by patients and tissue types with broadly similar cluster arrangements around morphological tissue features (Supplementary Fig. We removed batch effects (see Methods) and then performed targeted clustering and cell type annotation for breast samples from tTMA1 (23), tTMA1 (24) and tTMA2 (24) for Xenium breast panel, MERSCOPE breast panel and CosMx multi-tissue panel (Supplementary Fig. When looking at tTMA1 (23), in breast cancer, after batch effect removal (Supplementary Fig. The cell type annotation of Xenium and CosMx was comparable in terms of both transcriptomic profile and subtype depth, with CosMx only unable to annotate immune cell subtypes (B cell and T cell). Gene expression of the same cell type from both platforms correlated well (Fig. MERSCOPE and Xenium showed a high correlation for almost all matching clusters. The correlation map shows a clearer one-to-one mapping between MERSCOPE and Xenium clusters than Xenium and CosMx clusters (Fig. Correlation plot showing the correlation between cell types identified in CosMx and Xenium as well as MERSCOPE and Xenium. Correlation plot showing the correlation between cell types identified in CosMx and Xenium as well as MERSCOPE and Xenium. c Heatmaps show high cell type annotation correlation between experiments conducted for Tumor TMA 1 of Xenium (top), MERSCOPE (middle), CosMx (bottom). d Clustering results of breast cancer samples in tTMA2 (24) from Xenium breast panel and CosMx multi-tissue panel. Results remained similar when the same sample was rerun with all spatial methods receiving the same slicing protocol a year later (Fig. Xenium identified all nine cell types as well as one additional cell type, epithelial cells, while CosMx and MERFISH identified the exact same set of cell types. 5c), where all cell types from tTMA1 (23) show high correlation with the same identifications in tTMA1 (24). However, CosMx, Xenium, and MERSCOPE exhibit differences in cell type annotations, particularly in the identification of cell subtypes. For example, in Core 99, Xenium appears blue in the center because many cells are identified as alveolar, while CosMx appears yellow due to the identification of epithelial cells. Alveolar cells are a subtype of epithelial cells, and closer examination of the heatmaps reveals that the transcriptomic profiles of alveolar cells identified in Xenium often exhibit high correlation with epithelial clusters in CosMx and MERSCOPE (Fig. While performing the cell type annotations independently for each platform, the markers provided by CosMx and MERFISH cannot support a confident classification of the cells as alveolar, unlike Xenium, which shows clear alveolar markers (Supplementary Fig. This lack of specificity is also observed with immune cells: Xenium identifies more subtypes with distinct markers, whereas CosMx and MERSCOPE often only produce clusters broad enough to indicate immune cells without further subtyping. These limitations result in correlation heatmaps where cell type clusters do not always align one-to-one between platforms, such as seen in tTMA1 (24), where the Xenium alveolar cluster correlated with both alveolar and epithelial clusters in CosMx and MERSCOPE. MERSCOPE is not included due to its low number of transcripts. High correlation is again found between the cell types of CosMx and Xenium. In conclusion, for all TMAs in 23 and 24, all platforms are capable of generating results that support reasonable, consistent cell type annotations that would allow further biological analyses. We find that CosMx or Xenium can resolve higher numbers of cell types, with the relative performance likely dictated by the presence of key marker genes in the panel (more likely in the larger CosMx panel) vs the higher sensitivity and lower false positives (more likely in Xenium measurements). In this study, we compared data obtained with three commercially available iST platforms with archival FFPE tissues to assess overall technical performance and help guide experimental design with human patient-derived samples that represent an important use case of these platforms. We focused our analyses on technical performance as a function of tissue source, including 17 different tumor types and 16 normal tissue types. Overall, we found that each iST platform presented various tradeoffs in terms of implementation, panel design and panel options, and resulting total transcript quantification and downstream analyses, including cell segmentation, cell quality, and biological interpretation. All these factors must be considered when designing iST experiments. There are significant workflow differences between the different platforms which factor into the choice of method. Cutting samples onto MERSCOPE coverslips is more technically challenging than on standard microscope slides. We found that MERSCOPE and CosMx are well set up for batch processing in the wet lab, either due to built-in pause points or the instrument's ability to run multiple samples. After staining, selecting regions of interest (ROIs) presented a surprising challenge for some systems: the Xenium platform could readily image the entire slide as a single ROI which easily covered entire TMAs, but the MERSCOPE ran into a 1 cm2 imaging area limit which meant cores in the addressable region were left unimaged, while the CosMx workflow required a demanding manual selection of ROIs for each core. These factors are likely to change as each company updates its protocol, but currently, Xenium offers the shortest, least hands-on workflow. We analyzed each resulting dataset with a combination of manufacturer recommended processes for each platform and computational tools that can be implemented by the user downstream. These pipelines each result in count matrices and detected transcripts that can be analyzed using a suite of emerging tools. To facilitate the comparisons of iST platforms at a high level and enhance readability, we synthesized our findings into a qualitative summary table (Table 1). When analyzed at a core level to abrogate the effects of individual cell-segmentation performance, we found that the total number of transcripts varied substantially across iST platform. With 2023 chemistry, Xenium yielded the highest number of transcripts, followed by CosMx and MERSCOPE, but with updated chemistry in 2024, CosMx showed higher total numbers of transcripts, consistent with a larger overall panel size. However, when this analysis was restricted to shared genes, Xenium generally had higher sensitivity for the same genes over CosMx and MERSCOPE across each tissue type, panel, sample, histological preparation, and acquisition round. The Xenium platform also showed the highest specificity by several false discovery rate metrics relative to CosMx and MERSCOPE. The most prominent observation across these comparisons for CosMx was the improvement from 2024 to 2023 chemistry. With 2024 protocols, CosMx showed the smallest difference relative to Xenium, a roughly two-fold decrease in transcript counts in samples prepared the same way (tTMA1) and in samples prepared according to manufacturer instructions (tTMA2). This is a dramatic improvement over the ~12-15 fold difference in sensitivity observed with 2023 protocols. The improvement in sensitivity also comes with an improvement in specificity, which, for some tissues, reached almost a 10-fold difference. This suggests that the new version of the CosMx algorithm presents a substantial improvement and presents a choice in slightly lower specificity and sensitivity relative to Xenium vs higher molecular plex. The most prominent observation for MERSCOPE was the large degree of variability between samples, TMAs, and protocols. Relative to Xenium, MERSCOPE sensitivities for shared genes varied between 2.6- and 100-fold lower. While the test case here was for typical archival tissues, these results underscore the importance of pre-screening samples for RNA-integrity for MERSCOPE to ensure high quality input tissue. This would require more tissue and limit the number of compatible samples, which can be weighed against the benefit of highly customizable panel design. Across all methods, we note that because of the small number of replicates from each tissue we stop short of making blanket statements about relative performance across a particular tissue type. Instead, we conclude that Xenium shows the highest per-gene sensitivity on matched samples, though with updated protocols CosMx's sensitivity approaches Xenium and provides more overall transcripts when considering the larger panel size. MERSCOPE can achieve similar sensitivities relative to Xenium, but is highly sensitive to sample input quality and protocol deviations. While this paper was in review, two other comparison studies came to similar conclusions. A similar head-to-head comparison in FFPE tissue between CosMx and Xenium by Cook et al. found a similar difference in sensitivity to our results39. Interestingly, a comparison of public (unmatched) data from high quality mouse brain by Hartman and Satija found that MERSCOPE provided the highest sensitivities40, in line with the idea that this technique is highly sensitive to input tissue quality. Regardless of these differences in specificity and sensitivity we found that on matched samples, all platforms were highly correlated to each other. Similarly cores from the same patient were also highly correlated. This suggests that all three platforms can be trusted for normalized measurements of gene expression within a sample. Furthermore, all three platforms also show similar performance relative to orthogonal (non-spatial) technologies, finding comparable correlation of pseudo-bulk data to RNA-seq data from GTEx or the TCGA, as well as scRNA-seq data generated for tTMA1 and tTMA2, across each panel and platform. The correlation to orthogonal data was high but not perfect, and we found that each platform appeared to repeatedly over- or under-estimate the expression of a small number of genes (Fig. 3F)—suggesting, in some cases, middlingly-expressed genes could be seen as highly or lowly expressing. A significant advantage of spatial transcriptomics data is the ability to map expression in single cells. We compared each platform on a cell-level basis by assessing cell identification and cell clustering. Overall, it appears that the out-of-the-box segmentation from CosMx and Xenium achieved better accuracy than MERSCOPE in terms of precision, recall, and F1 score across dense, sparse, and elongated cells. All platforms now use a membrane marker for segmentation, but the specific markers and segmentation algorithms are different, which could affect cell typing41,42,43,44, by misattributing marker genes to adjacent cells or by missing cells whose nuclei are out of the imaging plane. It is likely that segmentation performance could be improved on a sample-by-sample or tissue-type-by tissue-type basis, and future work should seek to assess cell segmentation tools and their performance across data from each platform to help inform the choice of analytical method where needed. After applying an expression level filter, CosMx overall retained the highest number of cells across various filtering stringencies. Xenium was able to effectively separate cells from different lineage markers, as judged by finding minimal coexpression of disjoint markers, while CosMx and MERSCOPE either showed more double positive cells or had fewer cells expressing the target genes overall). To determine whether clearer identification of lineage markers resulted in improved ability to identify cell types, we performed clustering analyses specifically in breast cancer samples. We found that all three platforms had similar cell type annotation results and similar transcriptomic profile for tTMA1 (23) and tTMA1 (24). This consistency across platforms and over 2 years of sequencing highlights the reproducibility and robustness of our cell type annotations and transcriptomic profiling for TMAs. On the highest quality cores in tTMA1, MERSCOPE still successfully identified cell groups, capturing the patterns seen in other platforms, consistent with the trends in the overall transcript counts which show that high quality samples could achieve similar performance to the other platforms. We note that while similar numbers of cell types are recovered, the spatial component of the data shows that occasionally low expression of key marker genes can result in misannotation of a cell subtype which is identified in another technique. Examining the latest protocols (2024 data), in some cases, Xenium showed larger number of annotated cell types while in others CosMx did. Since we used the full panel, not only the shared genes, when performing these clustering analyses, these differences are likely due to a balance between the benefit of more sensitive detection of tissue relevant genes and the possibility of missing informative genes in the larger panel. The number of genes included in a panel or plex is an important factor in ST experiments, and we recommend subsampling existing atlas data to determine whether the gene set which can be studied will be sufficient to cluster the cell types of interest and identify the necessary biological programs. We note that each of the manufacturers has released new products and now offer increased panel sizes since the completion of these experiments with panels now reaching 5000 genes for Xenium, 6000 genes for CosMx, and 1000 genes for MERSCOPE. There are several limitations of our study. Due to insufficient tissue, we were unable to run the same TMA across multiple years and multiple slicing and fixation protocols, requiring us to split our efforts between tTMA1 and tTMA2. Nonetheless, we were able to show both the effects on matched sample preparation and sample preparation following manufacturer recommendations. Because of insufficient tissue and the TMA design we could not achieve an equal level of sampling with single cell approaches as with spatial methods. Thus, while we were able to identify similar cell subpopulations for the purposes of exploring relative gene expression, we leave the question of the ability of spatial methods to perform generalized cell type clustering relative to single cell to other investigations. This limit did not apply to comparing clustering between spatial platforms, where we had similar numbers of cells and could identify tissues of origin based on TMA location. Most importantly, we only attempted to compare the performance of iST platforms under typical use cases for clinical samples obtained from archival biobanks. Indeed, there have been reports that MERSCOPE, in previous studies of the mouse brain, shows comparable or even superior results to those reported by 10x Xenium45. Given the large change in data quality between TMAs, and even the same sample run multiple times, we cannot exclude the possibility that in the highest quality samples MERSCOPE would provide higher transcript numbers, with the associated downstream benefits relative to Xenium and CosMx. However, the current guidance of DV200 > 0.6 restricts studies to the upper regime sample quality and limits archival investigations. Finally, we note that this study only compares the ability of spatial transcriptomic platforms to reveal transcriptional information. Specifically, we do not answer whether ST can predict protein expression of clinical biomarkers. Our preliminary exploration based on PD-L1 status annotation in tTMA1 did not show significant expression differences for any platform, but rigorous testing of this would require larger powered cohorts in each individual tissue type and clinical annotation. Despite these limitations, our overall interpretation of these results is that amplification of RNA signal is especially important for recovery of transcript counts by iST in low-quality samples where RNA may be highly degraded and fewer landing sites are available for probes. Platforms (such as Xenium and CosMx) which rely on small numbers of landing sites and are subsequently heavily amplified are robust to RNA degradation and are thus more broadly compatible with a broad range of samples. On the other hand, when sample quality is high (as in some of our tumor samples) the gap between amplified and unamplified platforms' performance closes and most platforms can yield useful data for subsequent downstream spatial analysis. Three TMAs were constructed using FFPE clinical discards at Brigham and Women's Hospital Pathology Core and were acquired with a waiver of consent for non-sequencing based readouts under IRB 2014P001026. Tumor TMA 1(tTMA1): A tumor TMA of 170 cores, 0.6 mm in diameter, including a variety of cancer samples and healthy lymphoid tissue as a positive staining control. The TMA samples were selected from samples previously characterized by ImmunoProfile and were selected to encompass both high and low levels of the biomarkers in the ImmunoProfile panel [CD8, PD-1, PD-L1, Foxp3, tumor marker (Cytokeratin, Sox10, or PAX8)]. Cores included both tumor and healthy control annotation, though for the purpose of this study, all were combined under their tumor label. Tumors were also chosen to be a mixture of PD-L1 high and PD-L1 low. This TMA had previously been studied by both H&E, and several highly multiplexed immunostaining approaches, and was known to be of high morphological integrity. Tumor TMA 2 (tTMA2): A tumor TMA of 48 cores, 1.2 mm in diameter, including a variety of cancer. This TMA was chosen for the breadth of tissue lineages included and the relatively large core size. Samples were sourced from the same patient in either duplicate or triplicate. This TMA was chosen for the breadth of tissue lineages included and the relatively large core size. All samples were fully de-identified before assembly into TMAs. Sequential sections were prepared according to manufacturer instructions (“Tissue Preparation Guide Demonstrated Protocol CG000578” for Xenium, “91600112 MERSCOPE User Guide Formalin-Fixed Paraffin-Embedded Tissue Sample Preparation RevB” for Vizgen, and “MAN-10159-01CosMx SMI Manual Slide Preparation Manual” for CosMx) at the Brigham and Women's Hospital Pathology Core. Prior to collecting samples, ~50 µm of each TMA were faced off to reach deeper into the sample where RNA integrity was likely higher. 5 µm sequential sections were then collected, floated in a 37 °C water bath, and adhered to Xenium slides (10x, PN 1000460), Vizgen FFPE coverslips (Vizgen, PN 10500102), or standard Superfrost+ slides for CosMx (Leica BOND PLUS slides, Leica Biosystems S21.2113.A). TMAs were sliced as close to the center of the active area as possible for each platform. To benchmark technologies under same preparation protocol, samples of tTMA1 (24) data were baked overnight and stored at room temperature. Samples for 10x Xenium and Vizgen MERSCOPE were brought to the Spatial Technology Platform at the Broad Institute for processing, while samples for NanoString CosMx were processed at the Wei lab at Brigham and Women's Hospital. Gene lists were uploaded to the Vizgen panel design portal and were checked against all profiled tissues, removing genes that were overexpressed in any individual tissue based on Vizgen's design guidelines (FPKM > 900), and ensuring that the total panel FPKM did not exceed the allowed limit in any individual sample type. The final gene lists, for all three iST modalities are available in Supplementary Table 4. Samples were processed in two batches, the first of four samples, two of each TMA and with each library prepped in parallel; and a follow up sample of each TMA re-run with the breast panel. Samples were first hybridized with anchoring probes overnight before being embedded in a polyacrylamide gel. Samples were incubated for two hours with a digestion solution at 37 °C and then overnight at 47 °C overnight in a detergent clearing solution and proteinase K to remove native proteins while the anchoring probes kept nucleic acids bound to the gel. After clearing, samples were additionally photobleached using Vizgen's MERSCOPE Photobleacher for three hours at room temperature in the clearing solution. Samples were hybridized with encoding probes and a cell boundary stain (PN 10400118) and then imaged with imaging kits (PN 10400005). Samples were stored at 37 °C in clearing solution after hybridization and before final imaging. After an initial examination of the data, a second batch of both TMAs was run a second time with the human breast panel, increasing the set imaging capture thickness from 5 µm to 10 µm to capture more tissue from cores that had lifted during the gel embedding process. MERSCOPE data acquired with a 10 µm imaging depth (Supplementary Table 5), resulted in a median 3.0-fold increase in expression across all transcripts. We excluded the 5 µm MERSCOPE breast panel data from most comparisons have left references to the lung panel data in as an illustrative example of an unsuccessful run (indicated as such throughout the figures). Data was processed on premises through the standard Vizgen workflow to generate cell by gene and transcript by location matrices. We segmented the data with a built-in Cellpose method on the most accurate looking cell boundary stain. 10x Xenium samples were processed in three batches according to manufacturer protocols “Probe Hybridization, Ligation & Amplification, User Guide CG0000582” and “Decoding & Imaging, User Guide CG000584”. Samples were stained utilizing 10x's predesigned Human Breast (10x, PN 1000463), Human Multi-Tissue and Cancer (10x, PN 1000626), and Human Lung panels (10x, PN 1000601), as they became available from the manufacturer. Slides for both TMAs were processed in pairs according to which probe library they were receiving. Slides were stained with a Xenium imaging kit according to manufacturer instructions (10x, PN 1000460). Briefly, padlock probes were incubated overnight before rolling circle amplification and native protein autofluorescence was reduced with a chemical autofluorescence quencher. Slides were processed on a 10 Xenium Analyzer, with ROIs selected to cover the entire TMA region. Data was processed on premises through the standard 10x workflow to generate cell by gene and transcript by location matrices. NanoString CosMx samples were prepared with one 1000 plex panel. NanoString CosMx samples were prepared with Human Universal Cell Characterization 1000 Plex Panel (part number 122000157) according to manufacturer protocol “MAN-10159-01 CosMx SMI Manual Slide Preparation Manual”. Firstly, slides were baked at 60°C overnight for better tissue adherence. After baking, slides were treated sequentially with deparaffinization, target retrieval (15 min at 100°C), permeabilization (3 µg/mL proteinase K, 15 min at 40 °C), fiducials application, post-fixation, NHS-acetate application and then hybridized with denatured probes from universal panel and default add-on panel. After in situ hybridization (18 h at 37 °C), slides were washed and incubated with DAPI (15 min at RT) and marker stain mix (with PanCK, CD45, CD68 and cell segmentation marker CD298/B2M). Slides were washed and loaded onto the CosMx SMI for UV bleaching, imaging, cycling and scanning. Raw images were decoded by default pipeline on Atomx SIP (cloud-based service). After data acquisition, the resulting outputs were uploaded to a Google bucket associated with a terra.bio Workspace for distribution and follow on analysis. To facilitate standardized data formatting and subsequent analytical processes, we built a data ingestion pipeline with the following objectives: (a) to grab cell-level and transcript-level data from diverse platforms and normalize the data structure; (b) to tag each cell and transcript with essential metadata including tissue type, tumor status, PD-L1 status, among others (Supplementary Fig. 14); and (c) to transform the data into various formats tailored to the requirements of particularized analyses. Specifically, to tag the data, core centers in the TMA were manually identified using DAPI images (Xenium) or cell metadata that contains global coordinates (MERSCOPE and CosMx) using QGIS(version:3.16.10-Hannover). Cells or transcripts within a specified radius were then labeled with core metadata via spatial joining (implemented by GeoPandas, version:0.13.0). In instances where cores are in close proximity or when a uniform radius cannot be applied effectively, we manually generated the core boundary masks. For each sample, two formalin-fixed paraffin-embedded (FFPE) curls (25 μm each) were dissociated using the Miltenyi Biotech FFPE Tissue Dissociation Kit (CG000632 RevA, 10X Genomics). The resulting cell suspension was divided equally into four centrifuge tubes, each containing approximately 300,000 cells. Cells in each tube were hybridized with a unique Probe Barcode, as per the instructions in the “Chromium Fixed RNA Profiling Reagent Kits for Multiplexed Samples” user guide (CG000527, 10X Genomics). Post-hybridization, cells from the four tubes were washed, counted, and pooled in equal proportions. Approximately 40,000 cells from the pooled suspension were loaded onto a Chromium Q chip (PN-1000422, 10X Genomics). The sequencing data was demultiplexed using bcl2fastq (Illumina). To evaluate panel to panel reproducibility using tTMA1 (23) and nTMA (23) we summed the expression level of shared genes between indicated panels (breast vs. multi-tissue and breast vs. lung panels from Xenium and breast vs. lung panels from MERCOPE) over an individual core and plotted all cores present in each panel, before calculating a Spearman's correlation. To evaluate core to core reproducibility, the individual gene counts of replica 1 were plotted against those of replica 2 and a Spearman's correlation was calculated. To further leveraged data from tTMA1 (24) to evaluate run-to-run reproducibility or check how consistent each platform's results were across different experiments, we compared the total gene expression levels of the same genes measured in 2023 and 2024 from the same panel using scatter plots. We did this for all matching tissue samples and calculated Spearman's correlation to quantify the reproducibility. To assess consistency between different but similar samples or sample-to-sample reproducibility, we performed the same comparison on a core basis for all the cores that have matching shapes and cellular structures when imaged in 2023 and 2024. We then summarize the Spearman's correlation values in boxplot to show the relative performance between different iST platforms. While a high correlation value shows that the gene expression patterns are similar, it doesn't account for the overall levels of expression. To get a more complete picture, we also calculated the average log2 fold change values of gene expression between 2024 and 2023 of a core and presented the results in a boxplot (Supplementary Fig. To compare across panels and platforms, we subset all datasets to include only cores assayed in all runs. The fraction of on-target barcodes was calculated as a percentage of all transcripts corresponding to genes relative to the total number of calls (including negative control probes and unused barcodes or blank barcodes). These measurements were performed on individual cores and averaged across all cores of the same tissue type. Because the difference in relative numbers of controls and target barcodes across different platforms, we adopted the false discovery rate (FDR) calculation to evaluate the specificity in a more normalized way (Fig. We calculated the FDR of platform p panel m data in tissue t using the following Eq. (1) and cell level data (see example in Supplementary Table 11): Where N is the total number of cores that belong to tissue type t, I is the total number of unique genes, J is the total number of negative control probes, \({g}_{{in}}\) is the gene expression of gene i in core n, \({{neg}}_{{jn}}\) is the total calls negative control probe j in core. Since MERSCOPE does not include negative control probes, FDR was recalculated by substituting negative control with blank barcodes (Fig. Where N is the total number of cores that belong to tissue type t, I is the total number of unique genes, L is the total number of unused barcodes or blank barcodes, \({g}_{{in}}\) is the gene expression of gene i in core n, \({{blank}}_{{{\mathrm{ln}}}}\) is the total calls of unused barcode or blank barcode l in core n, specifically, we used “BLANK” for Xenium, “Blank” for MERSCOPE, and “SystemControl” for CosMx. We only used the data from matched cores, so N is same for different platform p. Sensitivity was measured by the percentage of the total number of unique genes detected above noise level, where the noise was estimated as two standard deviations above average expression of the negative control probes. Cell Ranger from 10x Genomics was used to demultiplex the raw sequencing data into FASTQ and to align the FASTQ files. RNA sequencing results for tTMA1 and tTMA2 went through quality filtering where cells with greater than 5% mitochondrial gene expression or less than 200 expression counts were removed, resulting in 14,945 and 17,749 cells respectively. The number of features and percent mitochondrial can be seen in Supplementary Fig. The data was scaled and normalized, and PCA with dim 1:20 as well as FindCluster with resolution 0.5 were used to find the cell clusters. RNA TCGA cancer sample gene data summarizes 7932 samples from 17 different cancer types, and it provides FPKM for each gene documented. For GTEx, we selected the tissue types matching the annotation in our normal tissue TMA. To get pseudo-bulked iST values, the expression level of each gene in each core was normalized to the sum of all genes in that core and scaled by 100,000. We then averaged these scaled pseudo-bulk expression values across cores and plotted them against the averaged FPKMs from reference RNA-seq data sets. ACTA2-active cells (ACTA > 0, 526 and 1117 cells for tTMA1 and tTMA2 respectively) from single-cell RNA sequencing experiment are also used to compare to the iST data. to identify the top gene identifying smooth muscle cells which was contained in each panel: ACTA2-active cells (ACTA > 0) for Xenium and CosMx or MYLK-active cells (MYLK > 0) for MERSCOPE are extracted from the iST datasets. (ACTA2 was too highly expressed to include in the MERSCOPE panel.) Then, gene expression is also aggregated by summing the transcript counts. We use Spearman's correlation to compute the correlation coefficient, and lm (linear model) is used to fit the line. To determine the assay's ability to specifically identify known lineage markers, we focused on the normal tissue TMA profiled with multi-tissue panel of Xenium, breast panel of MERSCOPE, and 1 K panel of CosMx. We selected genes with known canonical expression patterns using based on transcriptomics data from GTEx. If a gene had 20-fold higher expression in a specific tissue than every other tissue combined, this gene was considered to be a tissue marker and was used for assessing specificity for each platform. Counts for each gene were normalized to the total counts within the core, and then the Z-score of this gene across tissue types was plotted in a heatmap (Supplementary Fig. We calculated average expression of a gene across cores of the same tissue type and normalized to the total averaged expression of all genes. Z-scores were calculated with the mean and standard deviation across all averaged genes. In this study, the cell segmentation accuracy of three iST platforms was systematically evaluated using manually annotated ground truth data (31384 cells in total) from 3 TMA cores. These cores were selected to represent 3 distinct cellular scenarios—dense, sparse, and elongated cell distributions—and were morphologically matched across the platforms. We employed both DAPI and membrane staining to identify cells, then marked their centroids using the open-source software QGIS (version 3.16.10-Hannover). The resulting shapefiles were subsequently processed in Python (version 3.10) with GeoPandas (version 0.13.1) for data analysis. Multiple segmentation instances were generated to account for variability and ensure statistical robustness. We used precision, recall, and F1 score as performance metrics. Precision was calculated as the ratio of correctly identified positive segments to the total predicted positive segments, reflecting the accuracy of positive predictions. Recall measured the ratio of correctly identified positive segments to the actual positive segments, indicating the platform's ability to capture all relevant segments. To compare the performance across different platforms and core types, statistical analyses were performed using one-way Analysis of Variance (ANOVA), followed by Tukey's Honestly Significant Difference (HSD) post-hoc tests for pairwise comparisons. To evaluate the biological performance of the segmentation, we plotted coexpression plots of gene pairs that are mutually exclusive including CD3E vs. EPCAM, CD4 vs. CD8, and PPARG vs. CD68. We pooled all the filtered cells from matched cores of each platform from tTMA1, dropped cells which do not express either gene, plotted the expression of one gene against the other, and converted the scatter plot to a 2D histogram showing cell numbers in each co-expression bin (Fig. Segmented cells were aggregated by TMA cores. For Xenium and MERSCOPE data, the estimation of tissue area was performed by calculating the area of a discernible circle, utilizing respective radius of 0.3 µm, 0.6 µm, and 0.6 µm for tTMA1, tTMA2, and nTMA, respectively. Conversely, for the CosMx dataset, the tissue area estimation was approached differently due to its square-like data presentation, a result of the FOV selection process. Here, the tissue area was deduced by multiplying the number of FOVs covered by each core with the area of a single FOV. For cell filtering, cells with less than 10 transcript counts in MERFISH and Xenium datasets were removed, and cells with less than 20 transcript counts in CosMx datasets were removed. We followed standard processes to then cluster and annotate cell types across each dataset using Scanpy47. To identify the cell type for each cluster, we used a t-test to find the markers for each Leiden cluster and annotated them according to previous literature50,51,52,53,54,55,56,57. Heatmaps of the top 3 markers for each cluster are drawn for each dataset from all three panels (refer to Supplementary Fig. For datasets that showed batch effect with patients, Harmony was used to remove this variance58. Correlation heatmaps were generated over overlapping genes that exist in both datasets, and the Spearman correlation coefficient was calculated. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. The raw data used in this study have been deposited in NCBI's Gene Expression Omnibus59, and are accessible through GEO Series accession numbers GSE308145, GSE308146, GSE308147, GSE308148 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSExxx). In addition, we developed a web portal (https://broadinstitute.github.io/ist_benchmarking_showcase_portal/) to visualize the IST data used in this project with Celldega (https://broadinstitute.github.io/celldega/). Source data are provided with this paper. All code used in this manuscript for data processing and analysis are available on GitHub (https://github.com/broadinstitute/ist_benchmarking/). Chen, W. T. et al. Spatial transcriptomics and in situ sequencing to study Alzheimer's disease. Lein, E., Borm, L. E. & Linnarsson, S. The promise of spatial transcriptomics for neuroscience in the era of molecular cell typing. Lewis, S. M. et al. Spatial omics and multiplexed imaging to explore cancer biology. Haque, A. et al. A practical guide to single-cell RNA-sequencing for biomedical research and clinical applications. Williams, C. G. et al. An introduction to spatial transcriptomics for biomedical research. Janesick, A. et al. High resolution mapping of the breast cancer tumor microenvironment using integrated single cell, spatial and insitu analysis of FFPE tissue. He, S. et al. High-plex multiomic analysis in FFPE tissue at single-cellular and subcellular resolution by spatial molecular imaging. Moffitt, J. R. et al. Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region. Du, J. et al. Advances in spatial trasnciptomics and related data analysis strategies. Phan, H. V. et al. High-throughput RNA sequencing of paraformaldehyde-fixed single cells. Xu, Z. et al. High-throughput single nucleus total RNA sequencing of formalin-fixed paraffin-embedded tissues by snRandom-seq. Gnanapragasam, V. J. Unlocking the molecular archive: the emerging use of formalin-fixed paraffin-embedded tissue for biomarker research in urological cancer. Matsunaga, H. et al. Reproducible and sensitive micro-tissue RNA sequencing from formalin-fixed paraffin-embedded tissues for spatial gene expression analysis. Chen, K. H. et al. Spatially resolved, highly multiplexed RNA profiling in single cells. & Quail, D. F. Decoding the tumor microenvironment with spatial technologies. Junttila, S. et al. Benchmarking methods for detecting differential states between conditions from multi-subject single-cell RNA-seq data. Newton, Y. et al. Large scale, robust, and accurate whole transcriptome profiling from clinical formalin-fixed paraffin-embedded samples. Liu, J. et al. Concordance of MERFISH spatial transcriptomics with bulk and single-cell RNA sequencing. Zhang, M. et al. Molecularly defined and spatially resolved cell atlas of the whole mouse brain. Zimmerman, S. M. et al. Spatially resolved whole transcriptome profiling in human and mouse tissue using digital spatial profiling. Tomczak, K., Czerwinska, P. & Wiznerowicz, M. Review The Cancer Genome Atlas (TCGA): an immeasurable source of knowledge. Genetic effects on gene expression across human tissues. Confronting false discoveries in single-cell differential expression. He, S. et al. High-plex imaging of RNA and proteins at subcellular resolution in fixed tissue by spatial molecular imaging. Garrido-Trigo, A. et al. Macrophage and neutrophil heterogeneity at single-cell spatial resolution in human inflammatory bowel disease. Nieto, P. et al. A single-cell tumor immune atlas for precision oncology. Elmentaite, R. et al. Single-cell atlases: shared and tissue-specific cell types across human organs. Double positive T cells: more than just a developmental stage? & Munier, M. L. CD4+ C. T. L. immune warriors in the battle against viruses and cancer. Zaunders, J. J. et al. CD4+CD8+ T cells: biological confusion and serendipity. Part I: Synthesis of the data on dual T-cell phenotype. Miao, J. et al. PPARγ in adipocyte biology and systemic metabolism. Tremblay, A. M. & Huot, J. R. Adipose tissue infiltration by macrophages in obesity: crosstalk with adipocytes. Zhang, H. et al. Tissue-resident macrophages in adipose tissue. Heumos, L. et al. Best practices for single-cell analysis across modalities. Cook, D. P. et al. A comparative analysis of imaging-based spatial transcriptomics platforms. & Satija, R. Comparative analysis of multiplexed in situ gene expression profiling technologies. Greenwald, N. F. et al. Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. Stringer, C. et al. Cellpose: a generalist algorithm for cellular segmentation. Chen, H., Li, D. & Bar-Joseph, Z. SCS: cell segmentation for high-resolution spatial transcriptomics. Salas, S. M. et al. Optimizing Xenium In Situ data utility by quality assessment and best-practice analysis workflows. CZI Cell Science Program et al. CZ CELLxGENE Discover: a single-cell data platform for scalable exploration, analysis and modeling of aggregated data. Wolf, F., Angerer, P. & Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Dries, R. et al. Advances in spatial transcriptomic data analysis. From Louvain to Leiden: guaranteeing well-connected communities. Kumar, T. et al. A spatially resolved single-cell genomic atlas of the adult human breast. Gray G. K. et al. A human breast atlas integrating single-cell proteomics and transcriptomics. Reed, A. D. et al. A single-cell atlas enables mapping of homeostatic cellular shifts in the adult human breast. Travaglini, K. J. et al. A molecular cell atlas of the human lung from single-cell RNA sequencing. Sikkema, L. et al. An integrated cell atlas of the lung in health and disease. Wu, S. Z. et al. A single-cell and spatially resolved atlas of human breast cancers. The breast cancer single-cell atlas: defining cellular heterogeneity within model cell lines and primary tumors to inform disease subtype, stemness, and treatment options. Yeo, S. K. et al. Single-cell RNA sequencing reveals distinct patterns of cell state heterogeneity in mouse models of breast cancer. Korsunsky, I. et al. Fast, sensitive and accurate integration of single-cell data with Harmony. Barrett, T. et al. NCBI GEO: archive for functional genomics data sets—update. We thank Nir Hacohen, Ilya Korsunsky, Roopa Madhu, and Kseniia Anufrieva for helpful discussions, as well as Patricia Rogers and Natan Pierete for assistance with the 10x Xenium data acquisition. This work is supported by a Broad Institute SPARC grant, by a HTAN grant (3U2CCA233195-05S1), and by a Brigham and Women's Hospital Department of Medicine - Broad Institution collaborative research Award. is supported in part through the Geisel School of Medicine at Dartmouth's Center for Quantitative Biology through a grant from the National Institute of General Medical Sciences (NIGMS, P20GM130454) of the NIH. Present address: Immunai, New York, NY, USA These authors contributed equally: Huan Wang, Ruixu Huang, Jack Nelson, Ce Gao. These authors jointly supervised this work: Kevin Wei, Brittany A. Spatial Technology Platform, Broad Institute of MIT and Harvard, Cambridge, MA, USA Huan Wang, Jack Nelson, Anna Yeaton, Sachi Krishna & Samouil L. Farhi Thayer School of Engineering, Molecular and Systems Biology and Program in Quantitative Biomedical Sciences at Dartmouth College, Hanover, NH, USA Division of Rheumatology, Inflammation, and Immunity, Brigham and Women's Hospital at Harvard Medical School, Boston, MA, USA ImmunoProfile, Brigham & Women's Hospital and Dana-Farber Cancer Institute, Boston, MA, USA Center for Immuno-Oncology, Tissue Biomarker Laboratory, Dana-Farber Cancer Institute, Boston, MA, USA Department of Pathology, Brigham and Women's Hospital, Boston, MA, USA Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Conceptualization: A.Y., S.F., tissue-microarray construction: K.F., K.P., T.B., S.R., pathological annotation: K.P., S.R., gene selection: A.Y., S.F., Xenium and MERSCOPE data acquisition: J.N., CosMx data acquisition: C.G., M.T., K.W., analysis: H.W., R.H., B.G., S.F., S.K., figure generation: H.W., R.H., B.G., S.F., writing original draft: H.W., R.H., J.N., B.G., S.F., draft reviewing and editing: H.W., R.H., K.W., B.G., S.F., supervision: K.W., B.G., S.F., funding acquisition: S.F., K.W. Consumables used in this study from both companies were purchased at full price. The remaining authors declare no competing interests. Nature Communications thanks Muhammad Dawood and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Systematic benchmarking of imaging spatial transcriptomics platforms in FFPE tissues. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: Cancer newsletter — what matters in cancer research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41467-025-65168-2'>Sea level rise and flooding of hazardous sites in marginalized communities across the United States</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-11-20 10:33:43
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Sea level rise (SLR) increases the risk of flooding at coastal sites that use and produce hazardous substances. We assess whether socially marginalized populations in the United States are more likely to be impacted by projected SLR-related flooding of hazardous sites that could result in contaminant releases. We identify 5500 facilities at risk of a 1-in-100-year flood event by 2100 under a scenario of continued high greenhouse gas emissions, including coastal power plants, sewage treatment facilities, fossil fuel infrastructure, industrial facilities, and formerly used defense sites. Seven states (Louisiana, Florida, New Jersey, Texas, California, New York, and Massachusetts) account for nearly 80% of projected at-risk facilities. Controlling for population density and county, a one standard deviation increase in the proportion of linguistically isolated households, neighborhood residents identifying as Hispanic, households with incomes below twice the federal poverty line, households without a vehicle, non-voters, and renters is associated with 19-41% higher likelihood of having a site at risk of SLR-related flooding within 1 kilometer (odds ratios [95% confidence intervals]: 1.19 [1.09, 1.31], 1.22 [1.08, 1.37], 1.27 [1.16, 1.39], 1.35 [1.21-1.51], 1.36 [1.21, 1.53], and 1.41 [1.32, 1.52], respectively). Results elucidate the need for disaster planning, land-use decision-making, as well as mitigation strategies that address the inequitable hazards and potential health threats posed by SLR. Global sea level has risen more than 11 cm over the last three decades and that rate is accelerating1, leading to an increase in coastal flooding due to high tides, waves, storm surge, El Niño events and other factors. Extreme coastal flooding is projected to more than double by 2050 across much of the world2. By 2100, nearly all of the coastal United States (U.S.) is expected to experience elevated water levels on a daily basis that today occur only twice per century3, with a rapid increase in the frequency of high tide flooding projected to begin in multiple cities during the next decade4. Extreme flood events result in the release of toxic substances into the environment. For example, over 200 contaminant releases were reported in the Texas Gulf Coast after flooding resulting from Hurricane Harvey in 2017. Over 10 million pounds of regulated air pollutants were released from refineries, petrochemical, and other industrial facilities5, and the catastrophic explosion of a chemical plant due to the loss of power for refrigeration necessitated the evacuation of 40,000 people6. Around the world, industrial facilities are disproportionately located along coastlines due to the historical importance of maritime trade to the establishment of industrial port cities, strategic access to global trade routes for raw materials and finished products via ports, and need for sea water for cooling and wastewater disposal. Marginalized racial and ethnic groups are more likely to live near hazardous waste sites and industrial facilities, and fenceline communities are typically subject to multiple forms of discrimination resulting in limited financial, political, and social capital to mitigate contaminant exposures7. Moreover, longitudinal analyses show that disproportionate hazard burdens faced by racially and economically marginalized groups are largely due to discriminatory land-use, permitting, and facility siting decisions8,9,10,11. Racial residential segregation and the inequitable distribution of stormwater infrastructure further contribute to racialized patterns of flood risk across U.S. cities12. Building upon a prior California analysis13, we conducted a nationwide equity assessment of flood risk at hazardous sites in the U.S. due to sea level rise (SLR). We derived probabilistic estimates of flood risk in 2050 and 2100 across an expanded range of legacy contamination sites and facilities that contain, handle, produce or emit hazardous substances. We then assessed the geographic distribution of at-risk sites with respect to multiple present-day measures of social marginalization, including race/ethnicity, poverty (household income below twice the federal poverty line), voter turnout, housing tenure, and linguistic isolation. Our objectives were to characterize inequities in residential proximity to hazardous sites at risk of future flooding due to sea-level rise and identify communities where additional resources are needed to prevent exposure to toxic substances and enhance climate resilience. We first assessed the annual probability of at least one flood exceeding the land elevation of over 47,646 coastal hazardous site locations compiled from one proprietary and four publicly available administrative data sources (Supplemental Table S1). We considered all sites within counties with land area below the 18 m elevation above current mean higher high water line across all coastal U.S. states and Puerto Rico. We defined sites as at risk if their projected annual probabilities exceeded 0.01 (i.e., they were threatened by a 1-in-100-year flood event) integrated across the full distribution of SLR projections using the law of total probability for one low (Reference Concentration Pathway [RCP] 4.5) and one high (RCP 8.5) greenhouse gas emissions scenario (see “Methods”). We found that over 11% of coastal sites in our analysis are at risk of SLR-related flooding by 2100 under the high emissions scenario (RCP 8.5) (Table 1). Figure 1 shows the distribution of at-risk sites by state or territory under RCP 8.5 in 2050 and 2100. Seven states (Louisiana, Florida, New Jersey, Texas, California, New York, and Massachusetts) account for nearly 80% of projected at-risk sites in 2100 (Fig. Restricting greenhouse gas emissions to the low emissions scenario makes little difference in terms of the number of projected sites at risk in the near term (2050) but would reduce the number of at-risk sites from 5500 to 5138 (a reduction of 362 or 7% of sites) in the long term (2100) (Table 1). Oil and gas wells and industrial facilities that emit quantities of hazardous substances that require reporting to the U.S. Environmental Protection Agency's Toxic Release Inventory (hereafter “TRI sites”) make up the largest proportion of sites we considered and sites at risk (Table 1). Under the high emissions scenario (RCP 8.5), over a fifth of coastal sewage treatment facilities, refineries and formerly used defense sites, roughly a third of power plants, and over 40% fossil fuel ports and terminals are projected to be at risk by 2100 (Table 1). States are shaded by the total number of at-risk sites, with darker colors representing a higher number of sites at risk (maps). The number of sites at risk in each state is broken down by type, with each facility type represented by a unique color (bar chart). We next considered the distribution of at-risk sites with respect to community demographics and indicators of social marginalization derived from three secondary datasets: the American Community Survey, a proprietary data source on recent voter turnout, and the federal Climate and Economic Justice Screening Tool14. We utilized census block groups as the geographic unit of analysis (hereafter “neighborhoods”) and considered block groups with at least one at-risk site located within 1 km of a populated area as being potentially affected (see “Methods”). Given the prominence of racial discrimination as a means of establishing and maintaining social inequality in the U.S.15, we considered measures of racial and ethnic makeup, as well as indicators of socioeconomic status, civic engagement (voter turnout), and vulnerability that relate to communities' ability to anticipate, mitigate, and cope with flooding, such as age, linguistic isolation (% of households where no one 14 years or older speaks English “very well”), and vehicle ownership. Table 2 summarizes the population characteristics of neighborhoods near versus far from hazardous sites at risk of flooding due to SLR in 2100 under a high emissions scenario. Figure 2 shows the increase in the likelihood of an at-risk site within 1 km per one standard deviation increase in each demographic and social vulnerability measure, which we estimated using logistic regression models controlling for population density and county to minimize bias related to the higher concentration of people of color and renters in urban areas and demographic variation across U.S. regions. Black circles are adjusted odds ratios from models that considered one population characteristic at a time and controlled for population density and county fixed effects. Error bars indicate 95% confidence intervals and were calculated using robust standard errors. The dashed line indicates no association. Disadvantaged status (as defined by the federal Climate and Economic Justice Screening Tool [CEJST]) is a binary variable; all other variables are continuous and were scaled by unit standard deviation to facilitate comparisons between effect estimates. Compared to other coastal neighborhoods, neighborhoods with one or more at-risk site nearby have lower voter turnout, proportions of residents identifying as Asian/Pacific Islander, and individuals under the age of 18, and higher present-day proportions of renters, households living in poverty, residents identifying as Hispanic and Black, linguistically isolated households, households without a vehicle, single-parent households, and individuals over the age of 65 (Table 2). In the multivariable regression models, all these bivariate associations remained statistically significant with the exception of the proportion of Black and Asian/Pacific Islander residents (Fig. Neighborhoods designated as disadvantaged by the federal Climate and Economic Justice Screening Tool, a nationwide composite assessment of cumulative impact associated with multiple measures of social vulnerability (e.g., poverty) and the presence of climatic and environmental hazardous, had a 50% higher odds of having an at-risk site within 1 km, compared to other coastal, non-disadvantaged neighborhoods (odds ratio [OR] and 95% confidence interval [CI] = 1.50 [1.23, 1.83], Fig. A one standard-deviation increase in the proportion of residents over age 65, linguistically isolated households, residents identifying as Hispanic, households in poverty, households without a vehicle, non-voters, and renters was associated with 15–41% higher likelihood of an at-risk site within 1 km (ORs and 95% CIs shown in Fig. Associations were similar when we considered the presence of at-risk sites within 3 km instead of 1 km of neighborhoods (Supplementary Table S2). Among neighborhoods within 1 km of an at-risk site, social marginalization was also associated with an increase in the number of at-risk sites nearby and the severity of flood risk across those sites (Supplementary Fig. Here we quantify flood risk severity by estimating the neighborhood expected annual exposure (EAE), calculated by summing the annual probabilities of at least one flood occurring at all hazardous sites within 1 km of populated portions of census block groups. This expected value reflects the total number of sites likely to be exposed to flooding in a given year—either 2050 or 2100 (see “Methods”). Among neighborhoods with an at-risk site within 1 km, a one standard deviation increase in the proportion of Hispanic residents, households in poverty, households without a vehicle, non-voters, and renters was associated with a 7–13% higher number of at-risk sites in 2100 under RCP 8.5 and a 0.10–0.21 unit increase in EAE (see Incidence Rate Ratios, mean differences and corresponding 95% CIs in Supplemental Fig S1). These estimates again control for population density and county to minimize bias. Figure 3 presents concentration indices and 95% confidence intervals summarizing the degree of inequality in the distribution of at-risk sites with respect to demographic and social marginalization indicators. We utilized concentration indices to identify the categories of facilities that were the most inequitably distributed for particular populations. Similar to the Gini coefficient commonly used to characterize income inequality, a concentration index ranges from −1 to 1, with negative values (in orange) indicating that the burden of at-risk sites is disproportionately higher for more marginalized groups and positive values (in blue) indicating that the burden is disproportionately lower for marginalized groups. Values are shaded white when confidence intervals include the null, indicating no significant evidence of a disparity. Concentration curves corresponding to the indices given in Fig. These display the distribution of at-risk sites with respect to demographic and social marginalization measures, with the area between the curve and diagonal line of equality being equivalent to the concentration index (e.g. between −1 and 1). To increase the legibility of Fig. 4, for each demographic and social marginalization measure we display only the five site categories with the strongest concentration indices, while the full set of concentration indices is shown in Fig. Negative values (in orange) indicate a disproportionately higher burden of at-risk sites for marginalized groups, while positive values (in blue) indicate that the burden is disproportionately lower for these groups. White values indicate a lack of statistical significance at P > 0.05. No adjustments were made for multiple comparisons. The X-axis gives the cumulative share of block groups in descending order of each of the demographic and social marginalization variables. Curves above the equality line indicate a disproportionately higher burden of at-risk sites for marginalized groups, while a curve below the equality line indicate that the burden is disproportionately lower for these groups. For example, the top left panel shows that the 50% of low-lying block groups with the highest proportion of renters (indicated by the x-axis value of 0.5) host roughly 80% of at-risk fossil fuel ports and terminals and industrial facilities (indicated by y-axis values of 0.8), whereas if these sites were equitably distributed, the y-axis value would be close to 0.5 and the curves would fall closer to the bolded diagonal line of equality. For legibility, the top 5 facility categories with the strongest concentration indices for each measure are shown. At-risk power plants, industrial TRI sites, clean-up sites, and fossil fuel ports and terminals disproportionately burdened neighborhoods with higher proportions of renters, non-voters, households without a vehicle, households living in poverty, and linguistically isolated households, as indicated by negative concentration index values in Fig. 3 and curves above the line of equality in Fig. In contrast, at-risk concentrated animal feeding operations and active oil and gas wells more often did not disproportionately burden marginalized groups, as indicated by mostly positive concentration index values in Fig. 3 and curves below the line of equality for many panels in Fig. At-risk refineries disproportionately burden neighborhoods with higher proportions of non-voters, households in poverty, and Black residents, and at-risk TRI facilities disproportionately burden neighborhoods with higher proportions of Black, Hispanic and Asian/Pacific Islander residents. In contrast, neighborhoods with a higher proportion of Native American residents are projected to be disproportionately burdened by at-risk active oil and gas wells, hazardous waste sites, landfills, and formerly used defense sites (Figs. Conclusions about which at-risk site types are inequitably distributed are largely but not entirely consistent across different metrics of flood risk (number of at-risk sites, which is presented in Fig. 3 vs. EAE across sites which is presented in Supplementary Fig S2). For example, neighborhoods with higher proportions of renters, linguistically isolated households, and households without a vehicle were not burdened by a disproportionate share of at-risk refineries (Fig. 3), but when assessing EAE, they were disproportionately burdened (Supplementary Fig S2 and S3). Similarly, Hispanic and Asian/Pacific Islanders were not burdened by a disproportionate share of at-risk refineries (Fig. 3), but are disproportionately burdened when considering EAE (Supplementary Fig S2 and S3). This may be because although the number of at-risk refineries tends to be higher near neighborhoods with smaller proportions of these residents, the severity of projected flooding at those refineries is higher than it is near other neighborhoods. We present a national assessment of projected SLR-related flooding threats to multiple categories of coastal sites and facilities that contain, use or produce hazardous materials. Our results show that of the more than 47,600 coastal facilities in the U.S. included in our analysis, over 11% (5500 facilities) are projected to be at risk of a 1-in-100-year or more frequent flood event by the end of the 21st century (2100) under a high (RCP 8.5) greenhouse gas emission scenario. A handful of states, including Louisiana, Florida, New Jersey, Texas, California, New York, and Massachusetts account for nearly 80% of projected at-risk sites. A prior study estimated the number of wastewater treatment plants and service populations across the U.S. that could be exposed to SLR scenarios from 1 to 6 ft, with projections ranging from 60 impacted treatment plants serving 4 million people to 394 plants serving over 31 million people16. That analysis did not incorporate elevated water levels due to tides, waves, and storm surge, which likely explains why we projected a larger number of sewage treatment facilities to be at risk of SLR-related flooding in the RCP 8.5 scenario. Another prior assessment of how unmitigated greenhouse gas emissions could affect U.S. power-generating capacity in 2100 among power plants in coastal areas estimated a similar number of power plants at risk as we did in our analysis. That study additionally considered the generation capacity of at-risk power plants. The authors found significant variation across states with exposed power capacities relative to current generation capacities being highest in Delaware, New Jersey and Florida (80%, 63% and 43%, respectively)17. Our analysis shows that industrial facilities that are part of the Toxic Release Inventory make up nearly a third (34%) of the total sites at risk of SLR-related flooding (N = 1870), second to fossil fuel infrastructure (41%), including refineries, fossil fuel ports and terminals and active oil and gas wells. Because we did not include pipelines in our analysis, our projections of the extent to which the nation's fossil fuel infrastructure may threaten coastal communities due to SLR-related flooding and associated contaminant releases are likely an underestimate. Indeed, extreme weather events such as Hurricanes Katrina, Rita and Harvey, while very different from the slower moving, incremental flooding related to SLR, have dramatically revealed the vulnerability of industrial facilities and oil and gas infrastructure. Flooding following these hurricanes led to oil and chemical spills, pipeline ruptures, as well as excess air pollutant emissions due to incidental releases as well as intentional shutdowns, flaring, and subsequent restarting of operations at petrochemical facilities18,19,20,21,22,23. Our prior equity analysis of contaminant releases related to Hurricanes Harvey, Rita and Ike found that these natural-technological (natech) disasters disproportionately impacted Hispanic, renter, low-income, and rural populations5. Similarly, results in this study show significant inequities in projected SLR flooding threats to potentially hazardous facilities; communities defined as disadvantaged by the federal Climate and Economic Justice Screening Tool (CEJST) have a 50% higher odds of having an at-risk site within 1 km, and a one standard deviation increase in the proportion of linguistically isolated households, neighborhood residents identifying as Hispanic, households in poverty, households without a vehicle, non-voters, and renters is associated with 19–41% higher likelihood of an at-risk site. Our findings align with prior equity studies of current and projected distributional burdens of flood risk among diverse populations in the U.S. A national study using Federal Emergency Management Agency maps from 2001–2019 in urban areas along with National Land Cover Data and county-level Census data found that 100-year flood zones, particularly in coastal counties, are often occupied by a higher proportion of disadvantaged populations24. Another study of coastal and inland areas estimated an average increase of 26.4% (24.1–29.1%) in climate change related flooding by 2050 under an RCP4.5 scenario, with the future increase in flooding risk concentrated on the Atlantic and Gulf coasts and disproportionately affecting Black communities25; although this study examined flooding and economic losses related to residential and non-residential properties, it did not consider risks to potentially hazardous sites. Other studies have examined flooding threats to active and legacy sites containing hazardous material. For example, a report found low-income communities were disproportionately represented among the populations living in proximity to clean-up sites (listed or candidate sites for the Superfund program) at risk of coastal flooding under low, medium, and high SLR scenarios in the East and Gulf Coasts26. A follow-up study identified coastal land below 10 m of elevation as potentially exposed to rising groundwater and identified 326 Superfund sites in these coastal areas that could experience mobilization of toxic compounds from contaminated soil due to groundwater inundation driven by SLR; results also showed that socially marginalized groups in several states would be disproportionately affected by this groundwater rise scenario27. Another analysis of former hazardous manufacturing facilities in six U.S. cities identified more than 6000 relic industrial sites with elevated flood risk over the next 30 years (2050), with socially vulnerable groups, including people of color and low income, disproportionately likely to live in these areas28. Studies outside of the U.S., for example in coastal regions in India, Copenhagen, Vietnam and Italy have investigated the risks posed by climate change-driven SLR and storm surge on infrastructure and vulnerable sites29,30,31,32, but none to our knowledge have evaluated these risks using an environmental justice framework. Strengths of our study include the use of tax parcel data to characterize the extent of facility boundaries, a probabilistic approach to estimating SLR-related flood risk, and the application of dasymetric mapping techniques to estimate populations and community demographics near at-risk sites. Limitations of our analysis include the fact that our flood models assume that the frequency and magnitude of flood events will remain static over the next century. However, studies indicate that tropical cyclone activity is likely to intensify due to the acceleration of climate change33,34,35, which would result in more damaging impacts to coastal communities36. Additionally, our estimations of annual probabilities of flood level exceedance, based on a modified “bathtub” approach, do not consider scenarios of groundwater intrusion and upwelling or nonlinear interactions between extreme flood events and local topography. These dynamics could cause increased flood levels at inland locations, especially where marshlands shrink, and land becomes more developed37. Our analysis also does not account for floodwater level attenuation particularly in areas where land is wide and flat, which may overestimate exposure during extreme storm events38. Locational errors for hazardous sites may have also led to over- or under-estimates of the number of at-risk sites, and data limitations precluded inclusion of other facility types, including underground storage tanks, brownfields, and non-National Priority List Superfund sites that could experience contaminant releases due to SLR-related flooding. Inaccuracies in the delineation of coastline boundaries may have resulted in the inclusion of offshore drilling sites and the overestimation of flood risk. Finally, we did not account for future flood risk mitigation efforts or population and demographic shifts, given the high degree of uncertainty in predicting these scenarios. Therefore, future actions to mitigate flood risk near hazardous sites, gentrification, displacement, migration, and other factors could change the associations we observed between demographics, measures of social marginalization, and proximity to at-risk sites. Our analysis highlights the disproportionate burden of projected SLR-related flooding threats to hazardous sites on marginalized racial and socioeconomic groups and elevates the importance of centering environmental justice in future climate change adaptation and land-use planning strategies to protect vulnerable coastal communities from natech disasters. Given that nearly 80% of projected at-risk sites are in seven states, future in-depth work can target these areas and more precisely characterize the potential hazards posed by these facilities to nearby communities with the goal of mitigating and preventing future harmful exposures and health risks. With over 30% of nuclear and fossil fuel power plants, 23% of refineries, and 44% of fossil fuel ports and terminals in coastal areas projected to be at risk, federal reporting requirements for these facilities could be expanded to include the forecasting of SLR-related flooding threats and preventive plans for mitigation, including future relocation, to avoid catastrophic contamination. Critical to these efforts will be ensuring that federal and state agencies provide publicly available, accessible, and continually updated data on projections of SLR-related flooding threats to hazardous sites for diverse end-users, in particular at-risk communities, planners, regulatory agencies, scientists, and decision-makers39. Future research focusing on a smaller subset of facilities and more localized regions or municipalities could further elucidate and potentially untangle the extent to which place-based trends in industrial, economic, labor market, and housing development trajectories, demographic churning, changes in land-use decision-making as well as other shifting structural factors account for the origins and persistence of inequities in exposures to at-risk sites that disproportionately affect marginalized populations. Finally, many other climate-related phenomena, such as groundwater rise, wildfires, landslides, major storms, and extreme heat, also threaten clean-up sites and active facilities that use and store hazardous materials27,40,41. To achieve a fuller picture, information on these threats should be integrated with projected SLR flood risk data. Risks may be reduced through enhanced regulatory requirements (1) for at-risk facilities to mitigate and prevent contamination threats and (2) for more robust assessment of clean-up sites to inform abatement activities and decisions about future land reuse. Action-oriented partnerships between communities living near at-risk sites and government agencies at local, state, and federal levels may increase the chances for success of these strategies by marshalling much-needed resources aimed at preventing contamination from acute natech disasters and slower-moving threats, including SLR-related flooding. Our analytic approach entailed four steps: 1) the identification of coastal hazardous site locations and the cleaning of associated descriptive data; 2) the estimation of future flood risk due to sea level rise at each site location; 3) the compilation of measures of demographics and social marginalization; and 4) a neighborhood-level analysis of the relationship between these measures and residential proximity to at-risk sites. We co-developed these methods with an advisory committee comprised of staff members from environmental justice and public health organizations with whom we collectively decided on greenhouse gas emissions scenarios, timeframes (2050 and 2100), flood risk metrics, categorization of sites, and the demographic and social vulnerability metrics to include13. The spatial extent of our analysis was U.S counties and county equivalents with any land area below 18 meters elevation above current mean higher high water line across all coastal U.S. states and Puerto Rico (see Supplementary Fig S4). Areas farther inland are at no conceivable risk of flooding due to sea level rise this century and were therefore excluded. We scaled up an approach for a prior analysis of California13 to compile a national dataset of active industrial facilities and other potentially hazardous sites. To achieve this, we sourced data from the U.S. Environmental Protection Agency's (EPA) Facility Registry Service (FRS)42, the U.S. Energy Information Administration's (EIA) Energy Atlas43 (petroleum refineries and terminals), the U.S. Army Corp of Engineers' (USACE) Waterborne Commerce Statistics Center44 (petroleum ports) and Formerly Used Defense Sites database45, and a proprietary dataset of active oil and gas production and stimulation wells from EnverusTM 46. For the FRS, we chose to exclude remediated and closed facilities and facilities with inaccurate or imprecise locational information (e.g. latitude and longitude values derived from zip codes only or with inaccuracy >50 m). This included sites with environmental interest “end dates” indicating they would no longer be regulated after 2020 or where records indicated contamination had been addressed or the site was permanently closed. We retained inactive facilities and facilities with expired permits because residual hazardous materials may remain at these sites. We organized the remaining sites into one of seven categories using (1) environmental permits or regulatory programs; (2) the North American Industry Classification System (NAICS) code; and/or (3) keyword filters (see Cushing et al. 13 for further detail). We made sure that each category was mutually exclusive and without duplicate entries, as sites can have more than one environmental permit and/or NAICS code and appear in more than one database. We manually removed FRS entries for refineries using refinery names and coordinates from the EIA Energy Atlas dataset. For oil and gas wells, we utilized latitude and longitude point locations to represent sites due to the small size of well pads compared to other site categories. We identified offshore oil and gas wells as those that were beyond the boundaries of 2010 Census block groups, and excluded them from the analysis to focus on wells located on land. Block group boundaries from the National Historical Geographic Information System do not include coastal water areas and terminate at the coastline. All other site types were represented as polygons in our analysis to better approximate a site's extent. For FRS, EIA, and USACE petroleum port sites, we used the Google API47 to (re)geo-code addresses, then joined the resulting coordinates to tax parcels obtained from Loveland Technologies (now Regrid)48 to approximate their spatial boundaries. Wherever a site's geocoded location overlapped with a tax parcel, we used that parcel to approximate that site's spatial extent for the purposes of projecting flood risk. Around 79% of our geocoded site locations fell within tax parcel boundaries. For sites that did not intersect tax parcels, we approximated boundaries by drawing a buffer equal to the median parcel area of intersected parcels for each site category (See Supplementary Material Table S1 for median areas applied for each category). For formerly used defense sites (FUDS), spatial data were available in both point and polygon format. Not all sites had a point or polygon, while some sites had both. To ensure we included all FUDS in our dataset, we (i) first included all facilities with polygon data, then (ii) identified facilities with point locations falling outside of these provided polygon boundaries, and (iii) drew a buffer around these point facilities using a circular radius that would result in the median area observed among facilities in (i). Wherever a polygon boundary overlapped with a buffered point, we clipped the latter based on the physical extent of the former (n = 25). For overlaps between two polygon boundaries, we used an overlap ratio (calculated as the area of the overlap divided by the area of the smaller polygon) to determine whether to split the overlapping area evenly between two facilities with minimal overlap (≤45%, n = 33), or to merge substantially overlapping facilities together into one (>45% overlap, n = 42). We resolved 13 complex cases involving three or more overlapping facilities manually on a case-by-case basis. The result of this process was a dataset of FUDS boundaries derived from original point locations or polygon boundary extents that contained no spatial overlaps between sites. For all sites, we then clipped parcels and circular buffer areas at the coast if they extended past the mean high tide line. As a final cleaning step, we flagged and subsequently consolidated duplicate sites (n = 656) if they met three criteria: they were assigned to the same category, had identical geo-coded coordinates, and were associated with the same or similar addresses (we quantified similarity using a fuzzy text match). We retained facilities with identical coordinates and similar addresses if they had been assigned to different categories (n = 232). We dropped facilities with matching coordinates but dissimilar addresses (n = 30) if their geocoded coordinates appeared inaccurate or implausible via manual visual inspection (e.g., located in the middle of a forest far away from any established roads). We used the same approach to assess flood risk at individual site locations as detailed in Cushing et al. (see Supplementary Fig S5)13,49,50. In brief, we considered probabilistic sea level rise projections51 for two greenhouse gas emissions scenarios (Reference Concentration Pathway [RCP] 4.5 and 8.5) and 2 years (2050, 2100)51. For each site, year, and emissions scenario, we estimated the total annual probability of at least one flood event exceeding, in height, the 25th percentile of land elevation for a given site's parcel or buffer boundary. Projections account for vertical land movement and coastal flood height return level curves using methods from Tebaldi et al. and updated tide station data from across the United States52. We derived elevation profiles from NOAA's Coastal Topographic Lidar digital elevation model53, and estimated the annual flooding probabilities using Equation (1) from Buchanan et al.50 We considered sites to be at risk if their projected annual probabilities exceeded 0.01 (i.e., threatened by a 1-in-100 year flood event). We also summed these probabilities across sites to derive a total expected annual exposure (EAE) across all at-risk sites within a given distance of neighborhood (block group) boundaries. We used 2010 U.S. Census block group boundaries as our definition of geographic neighborhoods. Census block groups are generally contiguous geographic areas that contain between 600 and 3000 people and are the smallest unit for which the U.S. Census Bureau reports a full range of demographic statistics. We used the U.S. Census American Community Survey's (ACS) 2015–2019 five-year estimates54 to approximate demographic characteristics at the block group level: age (% under 18 and % 65 and older), race/ethnicity (% people of color, defined as the inverse of % non-Hispanic White and disaggregated into % Hispanic, and % non-Hispanic [NH] Black, NH Asian or Pacific Islander, NH Native American, and NH other including multiracial), poverty (% below twice the federal poverty line), housing tenure (% renter-occupied units), vehicle ownership (% of households without a vehicle), family structure (% single parent-headed households), linguistic isolation (% of households where no one 14 years or older speaks English “very well”). We used voter turnout data from the 2016 and 2020 general elections from Catalist's National Database to approximate civic engagement (% of registered voters that did not vote averaged across the two elections). Finally, we used the federal Climate and Economic Justice Screening Tool (CEJST) that identifies disadvantaged communities in all 50 states, the District of Columbia, and U.S. Territories55. Developed as part of the Justice40 Initiative, CEJST was used by federal agencies to identify disadvantaged communities facing disproportionate climate and environmental burdens as well as economic marginalization. CEJST identifies disadvantaged communities through eight categories of vulnerability metrics related to climate change, energy, health, housing, legacy pollution, transportation, water and wastewater, and workforce development. Census tracts are identified as disadvantaged if they meet 90th percentile thresholds for indicators within any of the eight categories and are at or above the 65th percentile for low-income. We began by identifying and including only counties in our study area with at least one site at risk under RCP 8.5 by 2100. We then further restricted the geographic extent of our analysis to “coastal” block groups in these counties within 3-km Euclidean distance of the 10-m elevation line above mean higher high water line. Our primary outcome of interest was the presence of at least one at-risk site within 1 km. We considered block groups to have this outcome if they contained populated areas within a kilometer of at least one at-risk site (see Supplementary Fig S4). Because block groups can be quite large in rural areas, we utilized gridded population estimates at a 30 × 30 m resolution56 to define populated portions of block groups with the exception of Alaska, Hawaii and Puerto Rico for which these estimates were not available and where we therefore relied on block group boundaries alone. We secondarily considered (1) the total number of at-risk sites within 1 km, and (2) the sum of annual flood event probabilities (total “expected annual exposure”, EAE) across all at-risk sites within 1 km. We conducted sensitivity analyses considering alternate versions of these outcomes using a 3 km rather than 1 km buffer distance. We examined descriptive statistics and correlation coefficients between our demographic measures and indicators of social marginalization and compared the distribution of neighborhood characteristics between exposed and unexposed block groups using Mann-Whitney U test because variables were not normally distributed. We then ran multivariable regression models for each outcome variable and vulnerability indicator pair, with block-group population density (people per square kilometer), and county fixed effects as additional independent variables. We did not include multiple demographic or social marginalization indicators in the same model due to multicollinearity. We chose to include population density as a potential confounder due to known associations between race/ethnicity, population density, and proximity to industrial facilities57,58. We included county fixed effects to control for regional demographic differences and in effect compare block groups with and without at-risk sites within the same county. We scaled continuous variables by unit standard deviation (SD), using the mean and SD from all block groups in our universe to allow for easier comparisons between effect estimates. In our primary analysis, we used a logistic model to estimate the odds of proximity to an at-risk site (yes/no variable). Restricting to exposed block groups, we used a negative binomial model to estimate associations with the number of sites nearby (count variable) and a linear model to estimate associations with EAE (continuous variable). We used county-clustered robust standard errors to control for the spatial autocorrelation. Finally, we used concentration curves to visualize the cumulative distribution of the number of exposed facilities and EAE with respect to each indicator of social marginalization. We also derived the concentration index (C) equal to the area beneath the curve and line of equality in our concentration plots to quantify the cumulative distributions. C ranges between −1 and 1, with negative values indicating that block groups with higher proportions of residents from socially marginalized groups have a greater number of at-risk facilities and EAE, and positive values indicating they have a smaller burden of at-risk sites and EAE. A C value with a confidence interval that includes the null value of 0 indicates that the number of exposed facilities and EAE are similar between more and less marginalized populations. We calculated C using all coastal block groups, and we calculated separate indices for each facility category focusing on year 2100 under RCP 8.5. Concentration curves and indices were computed using R (version 4.5.0). All other statistical analyses were conducted using Python (version 3.13.7). Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. The datasets generated during the current study are available from the Toxic Tides maps in Climate Central's Coastal Risk Screening Tool (flood risk projections, https://coastal.climatecentral.org/), GitHub (analytic dataset and code, https://github.com/yangju-90/toxic_tides_us), and Zenodo (analytic dataset and code, https://doi.org/10.5281/zenodo.16925499). The rate of global sea level rise doubled during the past three decades. Doubling of coastal flooding frequency within decades due to sea-level rise. Taherkhani, M. et al. Sea-level rise exponentially increases coastal flood frequency. Rapid increases and extreme months in projections of United States high-tide flooding. Berberian, A. G., Morello-Frosch, R., Karasaki, S. & Cushing, L. J. Climate justice implications of Natech disasters: excess contaminant releases during hurricanes on the Texas Gulf Coast. U.S. Chemical Safety and Hazard Investigation Board. Bullard, R. D., Mohai, P., Saha, R. & Wright, B. Toxic Wastes and Race at Twenty: 1987–2007. Toxic facilities, minority move-in, and environmental justice. Discrimination and the political economy of environmental inequality. Saha, R. & Mohai, P. Historical context and hazardous waste facility siting: understanding temporal patterns in Michigan. Mohai, P. & Saha, R. Which came first, people or pollution? Assessing the disparate siting and post-siting demographic change hypotheses of environmental injustice. Hendricks, M. D. & Van Zandt, S. Unequal protection revisited: planning for environmental justice, hazard vulnerability, and critical infrastructure in communities of color. Toxic tides and environmental injustice: social vulnerability to sea level rise and flooding of hazardous sites in coastal California. White House Council on Environmental Quality. Climate and Economic Justice Screening Tool Technical Support Document. Wilkerson, I. Caste: The Origins of Our Discontents (Random House, 2020). Hummel, M. A., Berry, M. S. & Stacey, M. T. Sea level rise impacts on wastewater treatment systems along the U.S. coasts. US power plant sites at risk of future sea-level rise. B., Castor, A., Grineski, S. E., Collins, T. W. & Mullen, C. Petrochemical releases disproportionately affected socially vulnerable populations along the Texas Gulf Coast after Hurricane Harvey. Ruckart, P. Z., Orr, M. F., Lanier, K. & Koehler, A. Hazardous substances releases associated with Hurricanes Katrina and Rita in industrial settings, Louisiana and Texas. & Cruz, A. M. Analysis of hazardous material releases due to natural hazards in the United States. Picou, J. S. Katrina as a Natech disaster: toxic contamination and long-term risks for residents of New Orleans. & Sengul, H. Petroleum and hazardous material releases from industrial facilities associated with Hurricane Katrina. Davis, A., Thrift-Viveros, D. & Baker, C. M. S. NOAA scientific support for a natural gas pipeline release during hurricane Harvey flooding in the Neches River Beaumont, Texas. Environmental justice implications of flood risk in the contiguous United States – a spatiotemporal assessment of flood exposure change from 2001 to 2019. Inequitable patterns of US flood risk in the Anthropocene. & Kalman, C. A Toxic Relationship: Extreme Coastal Flooding and Superfund Sites. Hill, K., Hirschfeld, D., Lindquist, C., Cook, F. & Warner, S. Rising coastal groundwater as a result of sea-level rise will influence contaminated coastal sites and underground infrastructure. Marlow, T., Elliott, J. R. & Frickel, S. Future flooding increases unequal exposure risks to relic industrial pollution. Climate change risk: an adaptation and mitigation agenda for Indian cities. In Adapting Cities to Climate Change (Routledge, 2009). Hallegatte, S. et al. Assessing climate change impacts, sea level rise and storm surge risk in port cities: a case study on Copenhagen. Noi, L. V. T. & Nitivattananon, V. Assessment of vulnerabilities to climate change for urban water and wastewater infrastructure management: case study in Dong Nai river basin, Vietnam. Bondesanf, M. et al. Coastal areas at risk from storm surges and sea-level rise in Northeastern Italy. Knutson, T. R. et al. Dynamical downscaling projections of twenty-first-century Atlantic hurricane activity: CMIP3 and CMIP5 model-based scenarios. Emanuel, K. A. Downscaling CMIP5 climate models shows increased tropical cyclone activity over the 21st century. Emanuel, K. Response of global tropical cyclone activity to increasing CO2: results from downscaling CMIP6 models. Geiger, T., Gütschow, J., Bresch, D. N., Emanuel, K. & Frieler, K. Double benefit of limiting global warming for tropical cyclone exposure. Dynamic simulation and numerical analysis of hurricane storm surge under sea level rise with geomorphologic changes along the northern Gulf of Mexico. Gallien, T. W., Sanders, B. F. & Flick, R. E. Urban coastal flood prediction: integrating wave overtopping, flood defenses and drainage. Linking sea-level research with local planning and adaptation needs. & Buck, K. National hazards vulnerability and the remediation, restoration and revitalization of contaminated sites—1. González, D. J. X. et al. Wildfires increasingly threaten oil and gas wells in the western United States with disproportionate impacts on marginalized populations. Layer Information for Interactive State Maps. WCSC Waterborne Commerce Statistics Center. US Army Corps of Engineers Institute for Water Resources Website https://www.iwr.usace.army.mil/About/Technical-Centers/WCSC-Waterborne-Commerce-Statistics-Center-2/ (2020). Formerly Used Defense Sites (FUDS), all data FY22. Loveland Technologies Nationwide Parcel Database. Kulp, S. & Strauss, B. H. Rapid escalation of coastal flood exposure in US municipalities from sea level rise. Buchanan, M. K. et al. Sea level rise and coastal flooding threaten affordable housing. et al. Probabilistic 21st and 22nd century sea-level projections at a global network of tide-gauge sites. Tebaldi, C., Strauss, B. H. & Zervas, C. E. Modelling sea level rise impacts on storm surges along US coasts. NOAA, O. for C. M. Digital Coast Data. White House Council on Environmental Quality. Climate and Economic Justice Screening Tool Technical Support Document, Version 2.0. & Stepinski, T. F. A high-resolution population grid for the conterminous United States: the 2010 edition. Molitor, J. et al. Identifying vulnerable populations through an examination of the association between multipollutant profiles and poverty. Sadd, J. L., Pastor, M., Morello-Frosch, R., Scoggins, J. Playing it safe: assessing cumulative impact and social vulnerability through an environmental justice screening method in the South Coast Air Basin, California. This project has been funded wholly or in part by the United States Environmental Protection Agency (EPA) under assistance agreement 84003901 to L.J.C. The contents of this document do not necessarily reflect the views and policies of the EPA, nor does the EPA endorse trade names or recommend the use of commercial products mentioned in this document. We thank the Toxic Tides Advisory Council—comprised of community leaders from the Asian Pacific Environmental Network, Central Coast Alliance for a Sustainable Economy, Physicians for Social Responsibility Los Angeles, Public Health Institute, and WE ACT for Environmental Justice—for advising on the methods. Department of Environmental Health Sciences, University of California, Los Angeles, CA, USA Department of Land Resources and Tourism, School of Geography and Ocean Science, Nanjing University, Nanjing, China Energy and Resources Group, University of California, Berkeley, CA, USA Climate Central, Princeton, NJ, USA Department of Environmental Science, Policy and Management, University of California, Berkeley, CA, USA Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar contributed to manuscript writing and jointly conceived of the project, acquired the funding, and supervised the work. conducted the statistical analysis and reviewed and edited the manuscript. S.Kulp conducted the flood risk projections and reviewed and edited the manuscript. N.D. and J.J. contributed to data curation. contributed to data curation, preparation of figures and tables, and edited the manuscript. contributed to manuscript writing and jointly conceived of the project, acquired the funding, and supervised the work. Correspondence to Lara J. Cushing or Rachel Morello-Frosch. The authors declare no competing interests. Nature Communications thanks Daniella Hirschfeld and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Cushing, L.J., Ju, Y., Karasaki, S. et al. Sea level rise and flooding of hazardous sites in marginalized communities across the United States. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: Anthropocene newsletter — what matters in anthropocene research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/11/251120002828.htm'>Scientists reawaken exhausted T cells to supercharge cancer immunity</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-11-20 05:53:48
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>A new study has identified a molecular cue that cancer cells use to exhaust the T cells responsible for destroying them, and the findings show that shutting down this signal may help restore the body's immune defenses. The work, led by researchers at Weill Cornell Medicine and published Nov. 17 in Nature Immunology, reveals that tumors do more than slip past the immune system. They can also alter immune cells in ways that reduce their ability to fight back. This discovery moves us closer to a future where the immune system itself defeats tumors," said the study's co-senior author, Dr. Taha Merghoub, Margaret and Herman Sokol Professor in Oncology Research, and professor of pharmacology at Weill Cornell Medicine. Modern immunotherapies have reshaped cancer treatment by boosting the body's own defense system. "Our findings reveal a completely new way that tumors suppress the immune system," said co-senior author Dr. Jedd Wolchok, the Meyer Director of the Sandra and Edward Meyer Cancer Center, professor of medicine at Weill Cornell and an oncologist at NewYork-Presbyterian/Weill Cornell Medical Center. "By blocking this pathway, we can help exhausted T cells recover their strength and make existing immunotherapies work better for more patients." T cell exhaustion occurs when the immune system faces long-term infections or persistent tumor activity. He added that although this loss of activity seems harmful, it can prevent uncontrolled inflammation and sepsis. Earlier studies showed that a surface protein called PD1 contributes to this exhaustion process. The research team set out to determine whether CD47, a molecule found on cancer cells, also plays a role in pushing T cells toward exhaustion. Previous work revealed that tumors use CD47 as a "don't eat me signal" to prevent certain immune cells from ingesting them. What surprised the scientists was discovering that T cells themselves display CD47. Experiments showed that mice lacking CD47 had slower tumor growth, suggesting the exhaustion effect came from CD47 on immune cells rather than on cancer cells. The team then investigated how cancer cells might manipulate this process. When mice were engineered to lack thrombospondin-1, their T cells showed fewer signs of exhaustion. "It showed us that CD47 and thrombospondin are clearly key players because eliminating either one gives you the same effect." The results were clear: TAX2 helped maintain T cell activity and slowed tumor progression in mice with melanoma or colorectal cancer. TAX2 also enhanced the effectiveness of PD1 immunotherapy in colorectal tumor models. Blocking this interaction could serve as an effective therapy by itself and may also help sustain tumor-targeting T cells in patients who are at risk of becoming resistant to current immune checkpoint treatments. According to Dr. Merghoub, early experiments in animal models suggest that inhibiting both PD1 and CD47 creates T cells that are significantly better at destroying cancer cells. Profiles for Dr. Taha Merghoub and Dr. Jedd Wolchok contain details about these affiliations. The First Hooved Reptile: Dinosaur “Mummies” Reveal a Shocking Evolutionary Twist New Microwave Technique Could Turn CO2 Into Fuel Far More Efficiently Doctors “Astounded”: Long-Held Belief About Coffee and Heart Rhythm Was Wrong Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            