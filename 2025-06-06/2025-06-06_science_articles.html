
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - SCIENCE Article Summaries - 2025-06-06</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            SCIENCE
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/inside-the-secret-meeting-where-mathematicians-struggled-to-outsmart-ai/'>Inside the Secret Meeting Where Mathematicians Struggled to Outsmart AI</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 16:37:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>At Secret Math Meeting, Researchers Struggle to Outsmart AI The world's leading mathematicians were stunned by how adept artificial intelligence is at doing their jobs The group's members faced off in a showdown with a “reasoning” chatbot that was tasked with solving problems they had devised to test its mathematical mettle. “I have colleagues who literally said these models are approaching mathematical genius,” says Ken Ono, a mathematician at the University of Virginia and a leader and judge at the meeting. The chatbot in question is powered by o4-mini, a so-called reasoning large language model (LLM). It was trained by OpenAI to be capable of making highly intricate deductions. Compared with those earlier LLMs, however, o4-mini and its equivalents are lighter-weight, more nimble models that train on specialized datasets with stronger reinforcement from humans. The approach leads to a chatbot capable of diving much deeper into complex problems in math than traditional LLMs. To track the progress of o4-mini, OpenAI previously tasked Epoch AI, a nonprofit that benchmarks LLMs, to come up with 300 math questions whose solutions had not yet been published. Even traditional LLMs can correctly answer many complicated math questions. Yet when Epoch AI asked several such models these questions, which were dissimilar to those they had been trained on, the most successful were able to solve less than 2 percent, showing these LLMs lacked the ability to reason. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. Epoch AI hired Elliot Glazer, who had recently finished his math Ph.D., to join the new collaboration for the benchmark, dubbed FrontierMath, in September 2024. The project collected novel questions over varying tiers of difficulty, with the first three tiers covering undergraduate-, graduate- and research-level challenges. By February 2025, Glazer found that o4-mini could solve around 20 percent of the questions. He then moved on to a fourth tier: 100 questions that would be challenging even for an academic mathematician. Only a small group of people in the world would be capable of developing such questions, let alone answering them. The mathematicians who participated had to sign a nondisclosure agreement requiring them to communicate solely via the messaging app Signal. Other forms of contact, such as traditional e-mail, could potentially be scanned by an LLM and inadvertently train it, thereby contaminating the dataset. The group made slow, steady progress in finding questions. But Glazer wanted to speed things up, so Epoch AI hosted the in-person meeting on Saturday, May 17, and Sunday, May 18. For two days, the academics competed against themselves to devise problems that they could solve but would trip up the AI reasoning bot. “I came up with a problem which experts in my field would recognize as an open question in number theory—a good Ph.D.-level problem,” he says. Then it wrote on the screen that it wanted to try solving a simpler “toy” version of the question first in order to learn. Five minutes after that, o4-mini presented a correct but sassy solution. “It was starting to get really cheeky,” says Ono, who is also a freelance mathematical consultant for Epoch AI. “I was not prepared to be contending with an LLM like this,” he says, “I've never seen that kind of reasoning before in models. Ono likened it to working with a “strong collaborator.” Yang Hui He, a mathematician at the London Institute for Mathematical Sciences and an early pioneer of using AI in math, says, “This is what a very, very good graduate student would be doing—in fact, more.” The bot was also much faster than a professional mathematician, taking mere minutes to do what it would take such a human expert weeks or months to complete. Ono and He express concern that the o4-mini's results might be trusted too much. “If you say something with enough authority, people just get scared. I think o4-mini has mastered proof by intimidation; it says everything with so much confidence.” If AI reaches that level, the role of mathematicians would undergo a sharp change. For instance, mathematicians may shift to simply posing questions and interacting with reasoning-bots to help them discover new mathematical truths, much the same as a professor does with graduate students. As such, Ono predicts that nurturing creativity in higher education will be a key in keeping mathematics going for future generations. “I don't want to add to the hysteria, but in many ways these large language models are already outperforming most of our best graduate students in the world.”</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-01805-6'>Skyrocketing mpox outbreak in Sierra Leone raises fears of wider spread</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 15:46:31
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. You can also search for this author in PubMed Google Scholar The work also suggests that there have been at least four times more infections than officials have reported, says Edyth Parker, a genomic epidemiologist at the Institute of Genomics and Global Health in Ede, Nigeria, who is a co-author. “The situation is so dire — the number of cases keeps increasing rapidly,” says Jia Kangbai, an infectious-diseases epidemiologist at Njala University in Freetown, Sierra Leone. The virus, which is thought to persist in some wild rodent species, is endemic in several African countries, including Sierra Leone, and occasionally spills into people. Health officials have been on high alert for surges of the virus that causes mpox ever since it spread to regions that it had never reached before, causing a global outbreak of the disease in 2022. The virus that causes mpox (pink) is shown infecting a cell (green) in this colourized electron microscope image. But the boom in infections in Sierra Leone has virologists worried once again: the country has reported a near-exponential growth in mpox cases that seem to be spreading from person to person, mainly among young people, both men and women. Epidemiological data suggest that sexual contact is a driver of the outbreak, because several sex workers have been infected and many people have reported genital lesions, according to a 27 May study posted on the preprint server medRxiv ahead of peer review1. This shows that when mpox is “put into a densely populated area where you have these dense sexual networks, it can take off very easily”, says Jason Kindrachuk, a virologist at the University of Manitoba in Winnipeg, Canada, who co-authored the medRxiv preprint. He adds that the country is entering its rainy season, meaning that more people will congregate indoors, creating more opportunities for the virus to spread. This outbreak comes as several African countries continue to battle their own mpox outbreaks, putting pressure on resources across the continent. Monkeypox virus: dangerous strain gains ability to spread through sex, new data suggest Monkeypox virus: dangerous strain gains ability to spread through sex, new data suggest An animal source of mpox emerges — and it's a squirrel Monkeypox virus: dangerous strain gains ability to spread through sex, new data suggest NIH grant cuts will axe clinical trials abroad — and could leave thousands without care EMBL is seeking a Group Leader to develop an independent research program on molecular mechanisms driving phenotype and environmental response. The Graduate School of Pharmaceutical Studies at Tohoku Universities invites applications for one open tenure-track assistant professor position. Monkeypox virus: dangerous strain gains ability to spread through sex, new data suggest An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/forest-preservation-tree-planting-could-actually-worsen-climate-change/'>Tree Planting Was Supposed to Be a Climate Change Solution. It Could Be Making Things Worse</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 15:45:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>With wildfires turning forests into “massive carbon emitters,” planting trees in some places could inadvertently increase carbon emissions, a new report says Ross Moore Lake wildfire in British Columbia, Canada on July 28, 2023. CLIMATEWIRE | Carbon markets that fund forest preservation and tree-planting might actually be worsening climate change by increasing risks for wildfires that emit massive levels of greenhouse gases, a new United Nations-affiliated report says. Forests have been seen as one of the most effective places to counter climate change by absorbing carbon emissions. But that's changed, says a May paper from the United Nations University Institute for Water, Environment and Health (UNU-INWEH), an academic arm of the international U.N. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. “Forests and peatlands have increasingly transitioned into massive carbon emitters in many parts of the world due to increasing wildfires,” the report says. Climate policies and carbon-mitigation activities ”fail to account for these substantial emissions.” The report highlights weaknesses in a central global strategy for addressing climate change — planting and protecting trees — which has attracted billions of dollars from polluters that fund the projects to offset their own carbon emissions. A large share of the money is paid through the voluntary carbon market, a largely unregulated system that has come under increasing scrutiny over its integrity. In California, wildfires in 2024 destroyed parts of forests that were supposed to be storing carbon through an offset program under the state's cap-and-trade carbon market. Without systematic monitoring of forest conditions, the paper says, the voluntary carbon market and other policies that promote forests “may unintentionally exacerbate wildfire hazards.” Planting new trees to absorb carbon could have the opposite effect, the report says, as more heat and increased carbon dioxide emissions from climate change accelerate forest growth while also depleting soil moisture. “Planting more and more trees in such an environment with the purpose of carbon mitigation will likely increase carbon emissions due to future fires,” the report warns. When the businesses certifying forest projects in the voluntary carbon market consider wildfire risk, they normally look at historical incidents of fires, Lee said. As a result, fire risk is typically underestimated by nonprofits such as Verra, which sets standards for and certifies climate projects to be listed on the voluntary market, Lee said. Concerns about forests and their changing dynamics have existed for more than a decade, Kaveh Madani, director of the UNU-INWEH, said in an interview from Toronto. Madani emphasized that not all forest programs in the voluntary carbon market and elsewhere create a wildfire threat. The paper advocates reforming the voluntary carbon market and similar systems to better account for forest conditions and to prevent unintended consequences, including more wildfires. Rainfall, soil health, and expected future droughts and heat waves should be considered before approving forest projects “as a carbon emissions reduction solution,” the paper says. Satellite observations could identify areas where forests are growing and fuels are accumulating, leading them to be excluded from carbon markets “due to the potential high emissions in case of future fires,” the paper says.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/space/a64967180/3-dimensions-of-time/'>Time Might Exist in 3 Dimensions—And That Changes Our Ideas About the Universe</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 11:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>But it also makes the concept of time-traveling into alternate futures a reality. What would the universe look like if you could travel faster than light? Our universe plays by certain rules, which originate from different branches of physics. But sometimes the rules seem inconsistent with each other or don't quite line up. And in other cases the rules seem to come out of nowhere, without clear explanation. This theory tells us that time and space are linked together in a four-dimensional fabric, called space-time, with three dimensions of space and one of time. Plus, special relativity tells us that movement in space-time is limited to no faster than the speed of light, 380,000 feet per second. Quantum mechanics, on the other hand, gives us rules about how subatomic systems work, those particles smaller than atoms. For example, in quantum mechanics particles can seem to appear in multiple locations at once, and we are never exactly sure where a particle will end up. Instead they are waves of probability that slosh around. These rules of quantum mechanics also allow for seemingly contradictory behavior like quantum entanglement, where one particle can influence another instantaneously—blowing well past the speed of light limit from special relativity. In an effort to tackle these issues, researchers from the Universities of Warsaw and Oxford asked an altogether different kind of question. What would the universe be like if you could travel faster than light? Now in special relativity it's impossible to move from below the speed of light and accelerate to reach it, let alone go faster than light. But the rules of special relativity also allow the reverse situation, where objects can start off faster than light already—but then they can never slow down below that limit. According to the researchers, for observers who are already traveling faster than light, the connection between time and space is flipped. This means that particles follow more than one trajectory at once. In effect, they travel into more than one future simultaneously. If you throw a baseball it only follows one path. Things like quantum entanglement, random probabilities, wave-particle duality, and all the other usual consequences of quantum mechanics seem to be natural outgrowths of this faster-than-light perspective. Instead of springing into existence without explanation—which is the default mode in quantum mechanics—these rules now have a potential reason. Instead you can only talk about fields, which are waves that exist throughout all of space and time. However, whether or not the individual particles known as tachyons can travel faster than light remains an open question. Paul M. Sutter is a science educator and a cosmologist at Johns Hopkins University and the author of How to Die in Space: A Journey Through Dangerous Astrophysical Phenomena and Your Place in the Universe: Understanding Our Big, Messy Existence. Sutter is also the host of various science programs, and he's on social media. ‘Dark Comets' May Have Ferried Water to Earth Could We Be Searching for Alien Signals All Wrong? This Strange Stuff May Be Older Than the Cosmos Humanity May Find Clues of Aliens Within 20 Years Is This the Warp Drive We've Been Waiting For?</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s43588-025-00798-6'>Advancing real-time infectious disease forecasting using large language models</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 10:14:52
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Nature Computational Science (2025)Cite this article A preprint version of the article is available at arXiv. Forecasting the short-term spread of an ongoing disease outbreak poses a challenge owing to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables, and the intersection of public policy and human behavior. Here we introduce PandemicLLM, a framework with multi-modal large language models (LLMs) that reformulates real-time forecasting of disease spread as a text-reasoning problem, with the ability to incorporate real-time, complex, non-numerical information. This approach, through an artificial intelligence–human cooperative prompt design and time-series representation learning, encodes multi-modal data for LLMs. The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial and epidemiological time-series data, and is tested across all 50 states of the United States for a duration of 19 months. PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and shows performance benefits over existing models. This is a preview of subscription content, access via your institution Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription cancel any time Subscribe to this journal Receive 12 digital issues and online access to articles only $8.25 per issue Buy this article Prices may be subject to local taxes which are calculated during checkout All data utilized in this study derive from publicly accessible sources. Hospitalization data were collected from the US Department of Health and Human Services33 and reported case data were sourced from the Johns Hopkins University CSSE COVID-19 Dashboard34. Vaccination data were obtained from the US CDC Vaccine Tracker36. For spatial data, demographic information was sourced from the US Census Bureau37, healthcare system data from the Commonwealth Fund39, and presidential election results from the Federal Elections Commission41. Public health policy data were acquired from the Oxford COVID-19 Government Response Tracker43. Official reports on variants were collected from the World Health Organization (WHO) and the European Centre for Disease Prevention and Control (ECDC), with estimated variant proportions sourced from the CDC30. The data are available at an archived repository52 and at https://github.com/miemieyanga/PandemicLLM. Source data for Figs. 3–5 and Extended Data Fig. 1 are provided with this paper. All codes are written using Python 3.11.5. Codes are publicly accessible at an archived repository52 and at https://github.com/miemieyanga/PandemicLLM. Giordano, G. et al. Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy. Li, X. et al. Wastewater-based epidemiology predicts COVID-19-induced weekly new hospital admissions in over 150 USA counties. Du, H. et al. Incorporating variant frequencies data into short-term forecasting for COVID-19 cases and deaths in the USA: a deep learning approach. Reich, N. G. et al. Collaborative hubs: making the most of predictive epidemic modeling. Rosenfeld, R. & Tibshirani, R. J. Epidemic tracking and forecasting: lessons learned from a tumultuous year. Hsiang, S. et al. The effect of large-scale anti-contagion policies on the COVID-19 pandemic. Li, J., Lai, S., Gao, G. F. & Shi, W. The emergence, genomic diversity and global spread of SARS-CoV-2. Nixon, K. et al. An evaluation of prospective COVID-19 modelling studies in the USA: from data to science translation. Castro, M., Ares, S., Cuesta, J. & Manrubia, S. The turning point and end of an expanding epidemic cannot be precisely forecast. Ioannidis, J. P., Cripps, S. & Tanner, M. A. Forecasting for COVID-19 has failed. Telenti, A. et al. After the pandemic: perspectives on the future trajectory of COVID-19. Nepomuceno, M. R. et al. Besides population age structure, health and other demographic factors can contribute to understanding the COVID-19 burden. Ruggeri, K. et al. A synthesis of evidence for policy from behavioural science during COVID-19. The language of genes. Singhal, K. et al. Large language models encode clinical knowledge. Jiang, L. Y. et al. Health system-scale language models are all-purpose prediction engines. Thirunavukarasu, A. J. et al. Large language models in medicine. Bzdok, D. et al. Data science opportunities of large language models for neuroscience and biomedicine. Williams, R., Hosseinichimeh, N., Majumdar, A. & Ghaffarzadegan, N. Epidemic modeling with generative agents. Preprint at https://arxiv.org/abs/2307.04986 (2023). Gruver, N., Finzi, M., Qiu, S. & Wilson, A. G. Large language models are zero-shot time series forecasters. Covid Data Tracker. Nixon, K. et al. Real-time COVID-19 forecasting: challenges and opportunities of model performance and translation. Touvron, H. et al. LLaMA 2: open foundation and fine-tuned chat models. Preprint at https://arxiv.org/abs/2307.09288 (2023). Rufibach, K. Use of Brier score to assess binary predictions. Leung, K., Wu, J. T. & Leung, G. M. Real-time tracking and prediction of COVID-19 infection using digital proxies of population mobility and mixing. Cramer, E. Y. et al. Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States. Lopez, V. K. et al. Challenges of COVID-19 case forecasting in the US, 2020–2021. Kalia, K., Saberwal, G. & Sharma, G. The lag in SARS-CoV-2 genome submissions to GISAID. TAG-VE statement on Omicron sublineages BQ.1 and XBB. Ma, K. C. Genomic surveillance for SARS-CoV-2 variants: circulation of Omicron lineages—United States, January 2022–May 2023. Bertozzi, A. L., Franco, E., Mohler, G., Short, M. B. & Sledge, D. The challenges of modeling and forecasting the spread of COVID-19. Dong, E. et al. The Johns Hopkins University Center for Systems Science and Engineering COVID-19 dashboard: data collection process, challenges faced, and lessons learned. COVID-19 reported patient impact and hospital capacity by facility. Department of Health and Human Services https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/anag-cw7u/about_data (2024). Dong, E., Du, H. & Gardner, L. An interactive web-based dashboard to track COVID-19 in real time. Du, H., Saiyed, S. & Gardner, L. M. Association between vaccination rates and COVID-19 health outcomes in the United States: a population-level statistical analysis. BMC Public Health 24, 220 (2024). COVID Data Tracker. CDC https://covid.cdc.gov/covid-data-tracker/#vaccine-delivery-coverage (2024). State population totals and components of change: 2020–2023. US Census Bureau https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html#v2022 (2024). B. et al. Demographic science aids in understanding the spread and fatality rates of COVID-19. Radley, D., Collins, S. & Hayes, S. The Commonwealth Fund 2019 Scorecard on State Health System Performance (The Commonwealth Fund, 2022). Bollyky, T. J. et al. Assessing COVID-19 pandemic policies and behaviours and their economic and educational trade-offs across US states from Jan 1, 2020, to July 31, 2022: an observational analysis. Haug, N. et al. Ranking the effectiveness of worldwide COVID-19 government interventions. Hale, T. et al. A global panel database of pandemic policies (Oxford COVID-19 Government Response Tracker). Stockdale, J. E., Liu, P. & Colijn, C. The potential of genomics for infectious disease forecasting. Vaswani, A. et al. Attention is all you need. OpenAI et al. GPT-4 Technical Report. Preprint at https://arxiv.org/abs/2303.08774 (2024). Fu, Y., Peng, H., Sabharwal, A., Clark, P. & Khot, T. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations (2023). Liang, J. et al. Code as policies: language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA) 9493–9500 (IEEE, 2023). Touvron, H. et al. LLaMA: open and efficient foundation language models. Preprint at https://arxiv.org/abs/2302.13971 (2023). Gage, P. A new algorithm for data compression. C Users J. Liu, H., Li, C., Wu, Q. Visual instruction tuning. Du, H. & Zhao, Y. Advancing real-time infectious disease forecasting using large language models. Hadfield, J. et al. Nextstrain: real-time tracking of pathogen evolution. This work was supported by NSF Award ID 2229996 (L.M.G. ), NOA: 6 NU38FT000012-01, from the CDC Center for Forecasting and Outbreak Analytics (L.M.G. ), Merck KGaA Future Insight Prize (L.M.G. ), NSF Award ID 2112562 (Y.C.) Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the funding agencies. These authors contributed equally: Hongru Du, Yang Zhao, Jianan Zhao. Center for Systems Science and Engineering, Johns Hopkins University, Baltimore, MD, USA Hongru Du, Yang Zhao, Shaochong Xu, Lauren M. Gardner & Hao ‘Frank' Yang Department of Civil and Systems Engineering, Johns Hopkins University, Baltimore, MD, USA Hongru Du, Yang Zhao, Shaochong Xu, Lauren M. Gardner & Hao ‘Frank' Yang Mila - Quebec AI Institute, Montreal, Quebec, Canada Department of Computer Science, Universite de Montréal, Montreal, Quebec, Canada Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA Department of Statistics, Harvard University, Cambridge, MA, USA Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA Yiran Chen & Hao ‘Frank' Yang Department of Epidemiology, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar conceptualized and designed the study. collected the data. processed the data and designed prompts. performed the experiments. ran the baseline models. prepared the figures. and H.F.Y analyzed the results. wrote the initial draft. provided guidance and feedback for the study. revised the paper. acquired the funding. provided computational resources. All authors prepared the final version of the paper. Correspondence to Yiran Chen, Lauren M. Gardner or Hao ‘Frank' Yang. The authors declare no competing interests. Nature Computational Science thanks Rafael de Andrade Moral and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available. Primary Handling Editor: Ananya Rastogi, in collaboration with the Nature Computational Science team. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. The black line shows the average COVID-19 hospitalization rate per 100,000 people in the U.S. plotted over time. Each data point (dot) represents the hospitalization rate for a specific state at a particular week. The color of each dot corresponds to the state's resulting HTC category for that week. Three PandemicLLMs were developed, each trained and validated on data ending in June 2021 (PandemicLLM-1), December 2021 (PandemicLLM-2), and September 2022 (PandemicLLM-3), respectively. All models were subsequently evaluated on a dataset spanning from their respective end dates to February 2023, without retraining. Panel (a) visualizes 1-week targets, while panel (b) shows 3-week targets. Dates are in the format year-month-day. 1–43, Results and Tables 1–9. Model performance across all 50 states during the evaluated periods. Confidence threshold and model accuracy, along with confusion matrices. Variants proportion across time, model performances and confidences across time. The hospitalization rates for each state, recorded on a weekly basis. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions Advancing real-time infectious disease forecasting using large language models. Nat Comput Sci  (2025). Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Nature Computational Science (Nat Comput Sci) © 2025 Springer Nature Limited Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s43018-025-00991-6'>Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 10:08:02
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Clinical decision-making in oncology is complex, requiring the integration of multimodal data and multidomain expertise. We developed and evaluated an autonomous clinical artificial intelligence (AI) agent leveraging GPT-4 with multimodal precision oncology tools to support personalized clinical decision-making. Evaluated on 20 realistic multimodal patient cases, the AI agent autonomously used appropriate tools with 87.5% accuracy, reached correct clinical conclusions in 91.0% of cases and accurately cited relevant oncology guidelines 75.5% of the time. These findings demonstrate that integrating language models with precision oncology and search tools substantially enhances clinical accuracy, establishing a robust foundation for deploying AI-driven personalized oncology support systems. The field of large language models (LLMs)1 has witnessed remarkable advancements in recent years. 2) have demonstrated capabilities that closely mimic human reasoning and problem-solving abilities and have shown knowledge across various professional disciplines. In the medical field, for instance, GPT-4 has achieved a passing score on the United States Medical Licensing Examination and is able to provide detailed explanations for its responses3. In oncology, where clinical decision-making is increasingly complex, LLMs can serve as a rapid and reliable reference tool, for instance, by providing medical recommendation suggestions from official medical guidelines4. This capability could not only aid in daily decision-making processes but also educate oncologists to stay updated with the latest treatment recommendations. However, similar to the medical domain itself, where doctors rely on information through speech, written text and imaging, the future of medical artificial intelligence (AI) is multimodal5. Recently, several such AI systems have been introduced6. Examples include models that analyze radiology images together with clinical data7 or integrate information from histopathology with genomic8 or text-based information9. These advancements have fueled anticipations for the advent of generalist multimodal AI systems10,11, characterized by their ability to concurrently analyze and reason across any dimension of medical information. However, it remains to be investigated whether such generalist multipurpose AI models alone are suitable for medical applications. The distribution of human diseases is wide and complex, which is not captured in current performance benchmarks, where these models are predominantly evaluated on a single specific task at a time. In contrast, real-world clinical decision-making often requires multistep reasoning, planning and repeated interactions with data to uncover new insights to make informed and personalized decisions. Despite advances with models such as Med-PaLM M11 or Med-Gemini12, the complexities to develop a generalist foundation LLM that truly performs on par with precision medicine tools remain a substantial challenge. Previous work has shown that some of these limitations can partially be overcome by enriching LLMs with domain-specific information. This can be achieved through fine-tuning14 or retrieval-augmented generation (RAG)15, a process that temporarily enhances an LLM's knowledge by incorporating relevant text excerpts from authoritative sources into the model, such as medical guidelines16 or textbooks. Yet, this strategy, concentrating on augmenting the knowledge base of the models, positions LLMs as mere information extraction tools only, rather than serving as true clinical assistants. Ideally, such a system would engage in reasoning, strategizing and performing actions on patient records and retrieve or synthesize new information to enable customized decision-making. Outside of the medical field, several such autonomous AI systems, also termed agents, have been proposed. Equipping an LLM with a suite of tools, such as calculators or web search, has proven superiority in tasks that require multistep reasoning and planning17,18. Similarly, in biomedical research, Arasteh et al. used the integrated data analysis tools of an LLM to analyze scientific data, achieving results on par with human researchers19. Such an approach would facilitate the opportunity of accessing the information repositories that currently exist in hospital systems, allowing for a true model for integrated patient care20. In this study, we build and evaluate an AI agent tailored to interact with and draw conclusions from multimodal patient data through tools in oncology. Contrarily to the philosophy of an all-encompassing multimodal generalist foundation model, we see the achievements that specialist unimodal deep learning models have brought to precision medicine21 as a viable template even in the future by equipping an LLM, specifically GPT-4, with additional functions and resources. These could be precision oncology deep learning models or the ability to perform web search, all referred to herein as tools. Specifically, this study includes the vision model application programming interface (API) dedicated to generating radiology reports from magnetic resonance imaging (MRI) and computed tomography (CT) scans, MedSAM22 for medical image segmentation and in-house developed vision transformer models trained to detect the presence of genetic alterations directly from routine histopathology slides23, in particular, to distinguish between tumors with microsatellite instability (MSI) and microsatellite stability (MSS)24 and to detect the presence or absence of mutations in KRAS and BRAF. Additionally, the system encompasses a basic calculator, capabilities for conducting web searches through Google and PubMed, as well as access to the precision oncology database OncoKB25. To ground the model's reasoning on medical evidence, we compile a repository of roughly 6,800 medical documents and clinical scores from a collection of six different official sources, specifically tailored to oncology. To quantitatively test the performance of our proposed system, we devise a benchmark strategy on realistic, simulated patient case journeys. Existing biomedical benchmarks and evaluation datasets are designed for one or two data modalities26 and are restricted to closed question-and-answer formats. Recent advancements have been made with the introduction of new datasets by Zakka et al.16, targeting the enhancement of open-ended responses, and LongHealth27, focusing on patient-related content. Yet, these datasets are limited to text and do not capture multimodal data, such as the combination of CT or MRI images with microscopic and genetic data, alongside textual reports. Therefore, in the present study, we develop and assess our agent using a dataset comprising 20 realistic and multidimensional patient cases, which we generate with a focus on gastrointestinal oncology. For each patient case, the agent follows a two-stage process. Upon receiving the clinical vignette and corresponding questions, it autonomously selects and applies relevant tools to derive supplementary insights about the patient's condition, which is followed by the document retrieval step to base its responses on substantiated medical evidence, duly citing the relevant sources. We provide an overview of our entire pipeline in Fig. A detailed description of our methodology is provided in the Methods. A schematic overview of our LLM agent pipeline. At its core, our system accesses a curated knowledge database comprising medical documents, clinical guidelines and scoring tools. This database is refined from a broader collection through keyword-based search, with the selected documents undergoing text embeddings for efficient storage and retrieval (1). The framework is further augmented with a suite of medical tools, including specialized web search capabilities through platforms such as Google, PubMed and access to the OncoKB API. The agent's capabilities are further expanded through the integration of a vision model tailored for generating detailed reports from CT and MRI scans, alongside MedSAM, a state-of-the-art medical image segmentation model and access to a simple calculator. Additionally, the system uses vision transformers specifically developed for the prediction of MSI versus MSS and the detection of KRAS and BRAF mutations in microscopic tumor samples (2). Given a simulated patient case, all tools are selected autonomously by the agent (3) with a maximum of ten per invocation and can be used either in parallel or in a sequential chain (4). This way, the agent can generate relevant patient information on demand and use this knowledge to query relevant documents within its database (4). This enables it to generate a highly specific and patient-focused response that integrates the initial clinical data with newly acquired insights, all while being substantiated by authoritative medical documentation (5). To first demonstrate the superiority of combining medical tools and retrieval with an LLM, we compared our agent to GPT-4 alone. 2a, we highlight three examples where tools and retrieval enabled the LLM to accurately solve cases, whereas, without these enhancements, GPT-4 either stated that it could not solve the patient case and provided hypothetical answers instead or drew incorrect conclusions, such as falsely assuming ‘disease progression' or ‘no evidence of disease' (in red). In contrast, the agent, by using tools, correctly identified treatment response and the presence of disease progression (in green). To quantify this, we assessed the model's ability to develop a comprehensive treatment plan for each patient, specifying appropriate therapies on the basis of recognizing disease progression, response or stability, mutational profile and all other relevant information, much like an oncologist would. Therefore, we compiled a set of 109 statements (completeness) for 20 different patient cases. We show that GPT-4 alone only provided 30.3% of the expected answers. However, our agent's responses achieved a 87.2% success rate, with only 14 instances not being covered (Fig. Overall, our results demonstrate that enhancing LLMs with tools drastically improves their ability to generate precise solutions for complex, realistic medical cases instead of providing only generic or even wrong responses when using LLMs in an out-of-the-box manner. a, Top, to demonstrate the superiority of our approach compared to a standard LLM, we highlight three cases where GPT-4 without tool use either fails to detect the current state of the disease for a given patient or provides very generic responses. Bottom, in contrast, tool access and retrieval enable the model to provide detailed information, such as measuring tumor surface and making appropriate decisions. b, The performance comparison shows a higher fraction of responses being evaluated as complete on our completeness benchmark for the agent with tool use and RAG versus GPT-4 alone. We next investigated the overall capabilities and limitations of our agent across a panel of benchmarks in greater depth. To provide better guidance, we present one complete, simulated patient case and detail every step, from input over tool usage to the model's final output in Fig. We abridge the patient description for readability (* …). The original STAMP pipeline consists of two steps, where the first is the timely and computationally intensive calculation of feature vectors, which we performed beforehand for convenience. First, we evaluated the agent's ability to recognize and successfully use tools (Fig. Of 64 required tool invocations to fully solve all given patient cases, the agent correctly used 56, achieving an overall success rate of 87.5%, with no failures among the required tools. There were two instances where the model tried to call superfluous tools without the necessary data available, which resulted in failures (Fig. We provide an overview of each tool's invocation status in Supplementary Table 1. To provide an illustrative example, in the patient case from Fig. 3, GPT-4 used its tools to evaluate the patient's conditions by first identifying tumor localization from patient data and generating segmentation masks with MedSAM. It calculated tumor progression by measuring the segmented area's growth and referenced the OncoKB database for mutation information. The model then performed literature searches and histological modeling to select targets of interest, ultimately producing the final response through data retrieval with RAG. Results from benchmarking the LLM agent through manual evaluation conducted by a panel of four medical experts. For the metric ‘tool use', we report four ratios: represents the proportion of tools that were expected to be used to solve a patient case and that ran successfully (56/64), with no failures among the required tools. Required/unused (8/64) are tools that the LLM agent did not use despite being considered necessary. Additionally, there are two instances where a tool that was not required was used, resulting in failures. ‘Correctness' (223/245), ‘wrongness' (16/245) and ‘harmfulness' (6/245) represent the respective ratios of accurate, incorrect (yet not detrimental) and damaging (harmful) responses relative to the total number of responses. Here, a response is constituted by individual paragraphs per answer. ‘Completeness' (95/109) measures the proportion of experts' expected answers, as predetermined by keywords, that the model accurately identifies or proposes. Lastly, we measure whether a provided reference is correct (194/257), irrelevant (59/257, where the reference's content does not mirror the model's statement) or wrong (4/257). Results shown here are obtained from the majority vote across all observers, selecting the least favorable response in cases of a tie. We additionally benchmarked GPT-4 against two state-of-the-art open-weights models, Llama-3 70B (Meta)28 and Mixtral 8x7B (Mistral)29, for which we show exemplary results in Fig. We provide results for each patient in Supplementary Tables 2 and 3. Given substantial shortcomings in the latter two, we decided to only focus on GPT-4 as it demonstrated reliable and highly effective performance in identifying relevant tools and applying them correctly to patient cases. a, Example tool results from three state-of-the-art LLMs (Llama-3, Mixtral and GPT-4). While the former two demonstrate failures in calling tools (or performing meaningless superfluous calculations in the case of Llama), GPT-4 successfully uses image segmentation on the MRI images and uses the calculator to calculate tumor changes in size. b, Tool benchmarking calling performance for these three models in a similar fashion to Fig. Overall, our findings reveal that both open-weight models demonstrate only extremely poor function-calling performance. First, both models struggle to identify necessary tools for a given patient context (18.8% of required tools remain unused by Llama and even 42.2% for Mistral). This deficiency results in invalid requests that disrupt program functionality (Llama, 42.2%; Mixtral, 50.0%), ultimately leading to crashes. We saw none of these cases for GPT-4. Consequently, for Llama and Mixtral, the overall success rates were low, registering only 39.1% (Llama) and 7.8% (Mixtral) (‘required, successful'). Moreover, we saw that the Llama model frequently used superfluous tools, for example, performing random calculations on nonsense values or hallucinating (inventing) tumor locations during imaging analysis that did not exist. This led to 62 unnecessary tool calls and failures (‘not required, failed') across all 20 patient cases evaluated. The major shortcoming of the Mixtral model was its frequent disregard for tool use, resulting in fewer than one in ten tools running successfully. Additionally, we observed that GPT-4 is capable of chaining sequential tool calls, using the output from one tool as input for the next. For example, in the tool results for patient G in Supplementary Note 2, GPT-4 invoked the MedSAM tool twice to obtain segmentation masks from two images taken at different time points. As another example, for patient W (Supplementary Note 2), the model used vision transformer models to assess the patient's mutational status, confirming the presence of a suspected BRAF mutation. Additionally, we assessed the robustness of the agent across multiple repetitions and diverse patient populations, specifically considering different combinations in sex (Extended Data Fig. Our evaluation showed that the primary source of performance variation was the number of tools required for each patient case, rather than intrinsic patient factors (Extended Data Fig. In summary, we can demonstrate that GPT-4 can effectively manage complex scenarios by sequentially using multiple tools, integrating their results and drawing informed conclusions on subsequent tool usage on the basis of prior information. In our scenario, GPT-4 had two options: using the GPT-4 Vision (GPT-4V) API to generate comprehensive radiology reports (useful when no radiology report was provided for a patient) or using MedSAM for image segmentation to generate surface segmentation masks of the described tumor reference lesions when localizations of reference lesions were included in the patient case vignette. In our evaluations, we found that, despite occasional omissions, extraneous details, lack of information or making mistakes (highlighted in red in Supplementary Note 2), GPT-4V could nonetheless effectively guide the agent's decision. This process is schematically illustrated in Fig. 3, with a detailed example provided in Supplementary Note 2 for patient G. We show that, in all instances, MedSAM received information on all relevant tumor locations and returned results that were helpful to GPT-4 to determine the appropriate next steps on the basis of calculating whether the patient showed progressive disease or remained in a stable condition. In summary, we show that our pipeline is able to autonomously handle multiple steps, such as determining the need for specific tools, locating the relevant data, understanding their chronological order, sending requests to the appropriate tool, receiving results and integrating these into the next steps of its decision-making, for instance, using a calculator to compare tumor sizes over time. All of these steps are managed fully autonomously, without human supervision, by the LLM agent. Thirdly, a central point in our analysis was the assessment of response accuracy (correctness). To evaluate this, we segmented the responses into smaller, evaluable items on the basis of the appearance of citations or transitions in topic within subsequent sentences, resulting in a total of 245 assessable statements. Our evaluations found 223 (91.0%) of these to be factually correct and 16 (6.5%) to be incorrect, while 6 (2.4%) were flagged as potentially harmful. Detailed instances of erroneous and harmful responses are provided in Supplementary Table 4 for review. Remarkably, the agent was capable of resolving issues, even in instances where contradictory information was provided in a patient's description, such as discrepancies in reported mutations versus results of testing such mutations using tools. In such cases, the agent pointed out these inconsistencies, recommended further genetic confirmation and outlined potential treatment options based on the results (patients D and X). An illustrative example of the model's response is presented in Fig. Among the aggregate of 67 queries, 63 (94.0%) were categorized as having been effectively addressed. Fourth, aiming to ensure transparency in the decision-making process, we instigated its adherence to citing relevant sources. Through manual review, we determined that, of the 257 citations provided in the models' responses, 194 (75.5%) were accurately aligned with the model's assertions, while 59 (23.0%) were found to be unrelated and merely 4 (1.6%) were found to be in conflict with the model's statement. These findings are promising, highlighting that instances of erroneous extrapolation (so-termed hallucinations) by the model are limited. Detailed evaluation results from each human observer are elaborated on in Supplementary Tables 4–6. Evaluation results for GPT-4 without tools and retrieval that we benchmarked for completeness can be found in Supplementary Table 7 and Supplementary Note 3. Overall, our results show that GPT-4 with tools and RAG can deliver highly accurate, helpful and well-cited responses, which enhances its ability to handle complex medical scenarios and provide support for clinical decision-making. Our results demonstrate that combining precision medicine tools with an LLM enhances its capabilities in problem solving, aligning with the concept of using LLMs as ‘reasoning engines' (ref. As we have shown, GPT-4 alone only generates very generic or wrong responses. However, integrating three core elements (a reasoning engine, a knowledge database and tools) enables us to effectively address this limitation, greatly improving accuracy and reliability. Additionally, using tools has several further advantages. Despite the potential future development of a generalist medical multimodal foundation model10, its efficacy in addressing very specialized medical queries, such as predicting rare mutations or measuring disease development on the millimeter scale compared to narrower, domain-specific models, remains uncertain. Moreover, maintaining the alignment of such a generalist model with the evolving medical knowledge and updates in treatment guidelines is challenging, as it requires retraining model components on new data. Our tool-based approach, however, addresses all of these issues. Similarly, state-of-the-art medical devices that are approved by regulatory authorities can be included in our setup. Additionally, by using RAG, our system effectively addresses a major limitation in current LLMs, which often tend to hallucinate by providing wrong but seemingly correct or plausible answers to questions beyond their knowledge4. Mitigating this issue is particularly critical in sensitive domains such as healthcare. We show that our approach demonstrates a high capability in delivering accurate answers, supported by relevant citations from authoritative sources, which also facilitates quick fact-checking as the agent returns the exact original text from the guideline documents. Constructing these cases from real-world data requires careful manual crafting and evaluation while adhering to data protection standards, especially for cloud-based processing. We, therefore, consider our work as a proof-of-concept study for agentic AI workflows in oncology and encourage future efforts to develop larger-scale benchmark cases. An important aspect beyond the scope of our study, which should be addressed in future research, is the selection of medical tools. While the core focus of our work lies in the tool-using abilities of language model agents, it is essential to recognize that individual tools require better independent optimization and validation. For example, we do not possess annotated ground-truth segmentation masks for comparison to MedSAM but use clinical endpoints such as progressive disease or tumor shrinkage as our primary metric to measure outcomes, which even more closely reflects the clinical workflow. As another example, vision transformers in our study were trained and tested on data from The Cancer Genome Atlas (TCGA) and some radiology images available online may have been included in the pretraining of GPT-4V. In a production environment, however, such tools would simply be replaced with better-validated and extensively tested alternatives, such as the clinical-grade MSIntuit model31. Overall, we believe that, by prioritizing clinical endpoints to validate our agent pipeline, we can offer new, directly relevant evidence and metrics to address the current scarcity of objective measures for AI agents in healthcare. Regarding the agent, although equipped with a broad array of tools compared to other frameworks17, it remains in an experimental stage, thus limiting clinical applicability. We anticipate great progress in generalist foundation models with greatly enhanced capabilities in interpreting three-dimensional (3D) images, including medical images. In this regard, we expect the development of more advanced, specialized medical image–text models, similar to the recently introduced Merlin model for 3D CT imaging33, which would allow adding these models as additional tools instead of simple ones such as MedSAM into our pipeline. Furthermore, an image–text model for radiology could also incorporate information on patient history and previous therapies to evaluate the current state of the disease like a radiologist would, instead of relying on lesion size changes only. Moreover, our current agent architecture is a static design choice. Medical professionals, however, have access to an extensive array of complex tools and can alternate between tool usage and knowledge retrieval. For AI agents, integrating RAG and tool use concurrently could also be complementary, as RAG could assist in guiding the agent through complicated steps of tool application for specific diagnostic or treatment decisions. This synergy might help improve complex workflows with more and more challenging tools, which remains an area of further exploration. Additionally, despite being implemented as a multiturn chat agent, our evaluation is currently confined to a single interaction. As a next step, we aim to incorporate multiturn conversations, including human feedback for refinement, akin to a ‘human-in-the-loop' concept34. Furthermore, we restricted our patient scenarios to oncological use cases only, yet it is important to note that the underlying framework could be adapted to virtually any medical specialty, given the appropriate tools and data. Next, regarding data protection in real-world settings, current regulatory restrictions make GPT-4 unsuitable because of its cloud-based nature, which necessitates transferring sensitive patient data to proprietary commercial servers. Consequently, we regard GPT-4 as a best-in-class model for proof-of-concept purposes and aim to explore open-weight models that can be deployed on local servers in the near future. 36), also show considerable potential as local solutions. Additionally, while our model receives relevant context from the RAG pipeline to provide accurate citations, there are several modifications and potential improvements to the retrieval process that we suggest exploring in future work. For instance, we used generalist embedding, retrieval and reranking models, whereas Xiong et al.37 demonstrated that domain-specific models can enhance retrieval performance. Moreover, for rare terms (such as rare diseases), exact match keyword searches can outperform similarity searches in embedding space and both can even be combined (also termed hybrid search38). Moreover, modeling long context dependencies could be further improved by using models with larger context windows, such as Gemini 1.5 (ref. This capability is especially crucial when scaling our approach to settings where patient information is not restricted to a single one-page case vignette but distributed across hundreds of documents. Lastly, from a production setting perspective, another critical aspect is to consider how the LLM handles temporal dependencies in treatment recommendations. For instance, in lung cancer, molecular targeted treatments are subject to rapid changes and official guidelines are not always updated at the same pace as the latest trial results. In such a setting, our multitool agent could cross-reference information from official medical guidelines with more up-to-date information received through internet and PubMed searches. Previous work already showed that LLMs can reliably identify temporal differences in medical documents4. Looking ahead, we anticipate more progress in the development of AI agents with even improved capabilities through further scaling. As a parallel step in this direction, Zhou et al.40 developed MedVersa, an AI system for medical image interpretation that leverages an LLM as a central ‘orchestrator'. This model is fully trainable to determine whether tasks can be completed independently or should be delegated to a specialist vision model and has demonstrated superior performance across multiple medical imaging benchmarks, often surpassing state-of-the-art models. The training of such a system holds immense potential, as it might allow models to learn critical concepts such as uncertainty, enabling them to recognize their limitations and delegate tasks to specialist models whenever appropriate. Although our model currently shows a strong ability to use given tools, task-specific fine-tuning and few-shot prompting41 (providing examples to the model) could further improve performance, particularly when increasing complexity with the addition of more and more complex tools. Additionally, we could enhance the system by incorporating human feedback on edge cases where the initial model performed poorly, similar to a human-in-the-loop approach. In the medical context, this method was already shown to improve prediction accuracy, as demonstrated in the context of molecular tumor board recommendations42. To summarize, in the near future, we envision a framework that embodies characteristics akin to the GMAI model with the added ability to access precision medicine tools tailored to answer any kind of specialized clinical questions. It enables medical AI models to assist clinicians in solving real-world patient scenarios using precision medicine tools, each tailored to specific tasks. Such a strategy facilitates circumventing data availability constraints inherent in the medical domain, where data are not uniformly accessible across all disciplines, preventing a singular entity from developing an all-encompassing foundational model. Instead, entities can leverage smaller, specialized models developed by those with direct access to the respective data, which could greatly improve discovering therapeutic options for personalized treatments. Moreover, this modular approach allows for individual validation, updating and regulatory compliance of each tool. In cases where existing tools are unsatisfactory or completely absent, the agent could rely on its internal strong medical domain knowledge and either refine43 or innovate entirely new tools from scratch. Crucially, this modular approach also offers far superior explainability compared to a large, generalist black-box model, as physicians can investigate the output from each individual tool separately. Herein, our study could serve as a blueprint, providing evidence that agent-based generalist medical AI is within reach. To fully leverage the potential of AI agents in the near future, it will be crucial to integrate them as deeply into routine clinical practice as possible, ideally through direct incorporation into existing clinical information systems for live access to patient data with only minimal workflow disruption for clinicians. However, this first necessitates addressing many deployment challenges, including concerns regarding interoperability with existing systems, data privacy laws such as GDPR and HIPAA (in case of cloud-based models), liability and accountability issues and the need for regulatory approvals as medical devices44, which will also require much broader validation studies to show safety and benefits in actual clinical workflows. Lastly, there will be a critical need to educate medical professionals to effectively collaborate with AI agents while maintaining full authority over the final clinical decision-making. This study did not include confidential patient information. All research procedures were conducted exclusively on publicly accessible, anonymized patient data and in accordance with the Declaration of Helsinki, maintaining all relevant ethical standards. The pipeline's primary goal is to compile a comprehensive dataset from high-quality medical sources, ensuring three main components: correctness, up-to-dateness and contextual relevance, with a particular emphasis on including knowledge across all medical domains while additionally encompassing information specifically tailored to oncology. We restricted our data access to six sources: MDCalc (https://www.mdcalc.com/) for clinical scores, UpToDate and MEDITRON14 for general-purpose medical recommendations and the Clinical Practice Guidelines from the American Society of Clinical Oncology45, the European Society of Medical Oncology and the German and English subset of the Onkopedia guidelines from the German Society for Hematology and Medical Oncology. We retrieved and downloaded the relevant documents as either HTML extracted text or raw PDF files. To reduce the number of documents for the embedding step, we applied a keyword-based filtering of the document contents, targeting terms relevant to our specific use case. Medical guidelines that were obtained from the MEDITRON project were directly accessible as preprocessed jsonlines files. The critical challenge in text extraction from PDF documents arises from the inherent nature of PDF files, which are organized primarily for the user's ease of reading and display while not adhering to a consistent hierarchical structure, thus complicating the extraction process. For instance, upon text mining with conventional tools such as PyPDF2 or PyMuPDF, headers, subheaders and key information from the main text may be irregularly placed, with titles occasionally embedded within paragraphs and critical data abruptly interspersed within unrelated text. However, maintaining the integrity of the original document's structure is crucial in the medical field to ensure that extracted information remains contextually coherent, preventing any conflation or misinterpretation. To overcome these limitations, we used GROBID (generation of bibliographic data), a Java application and machine learning library specifically developed for the conversion of unstructured PDF data into a standardized TEI46 format. Through its particular training on scientific and technical articles, GROBID enables the effective parsing of medical documents, preserving text hierarchy and generating essential metadata such as document and journal titles, authorship, pagination, publication dates and download URLs. This process encompasses the removal of extraneous and irrelevant information such as hyperlinks, graphical elements and tabular data that was corrupted during the extraction with GROBID, as well as any malformed characters or data such as inadvertently extracted IP addresses. To address this, we meticulously reformatted and standardized the text from all sources, denoting headers with a preceding hash symbol (#) and inserting blank lines for the separation of paragraphs. The purified text, along with its corresponding metadata, was archived in jsonlines format for subsequent processing. Below, we delineate the detailed architecture of our agent in a two-step process, beginning with the creation of our RAG15 database, followed by an overview of the agent's tool use and concluding with an examination of the final retrieval and response generation modules. The RAG framework has greatly evolved in complexity recently; thus, we break down its architecture into three major components (embeddings, indexing and retrieval) and outline the implementation details of the first two in the next section. In RAG, we begin with the conversion of raw text data into numerical (vector) representations, also termed embeddings, which are consequently stored in a vector database alongside metadata and the corresponding original text (indexing). In more detail, we compute vector embeddings using OpenAI's ‘text-embedding-3-large' model from text segments of varying lengths (512, 256 and 128 tokens), each featuring a 50-token overlap, from the curated guideline cleaned main texts in our dataset, alongside their associated metadata for potential filtering operations. For storage, we use an instance of a local vector database (Chroma) that also facilitates highly efficient lookup operations using vector-based similarity measures such as cosine similarity (dense retrieval). We store documents from different sources in the same collection. To endow the LLM with agentic capabilities, we equipped it with an array of tools, including the ability to conduct web searches through the Google custom search API and formulate custom PubMed queries. Information retrieved through Google search underwent text extraction and purification and was integrated directly as context within the model, while responses from PubMed were processed akin to the above-described RAG procedure in a separate database. For the interpretation of visual data, such as CT or MRI scans, the LLM agent has access to two different tools. It can either call the GPT-4 Vision (GPT-4V) model, which is instructed to generate a comprehensive, detailed and structured report, or use MedSAM, which we explain in detail below. For both tasks, we primarily use representative slices from in-house CT and MRI chest and abdomen series, although a few cases are from public datasets, as highlighted in Supplementary Note 1. Our in-house slices were selected by an experienced radiologist from our team, who was blinded to the rest of the study at the time of image selection. In scenarios involving multiple images to model disease development over time, the model first investigates and reports on each image separately before synthesizing a comparative analysis. Because of the stringent adherence of OpenAI to ethical guidelines, particularly concerning the management of medical image data, we framed our patient cases as hypothetical scenarios when presenting them to the model. However, instances of refusal still arose, prompting us to discard the respective run entirely and initiate a new one from the beginning. Additionally, tasks that require the ability of the model to precisely measure the size of lesions and compare them over time can be solved using MedSAM22. MedSAM produces a segmentation mask for a given region of interest on the basis of an image and pixel-wise bounding box coordinates, enabling the calculation of the overall surface area. Herein, MedSAM processes each image independently; however, GPT-4 can track the results from MedSAM back to the original image date, as it has access to image file names that are based on date. This capability ensures accurate decision-making of lesion development over time. An example of this is demonstrated in Supplementary Note 2 under tool results for patient G. A potential limitation of the vision tool approach is that, for both scenarios, we currently restrict the use to single-slice images that additionally need to be taken at the same magnification. Moreover, we provide access to a simplified calculation tool that allows elementary arithmetic operations such as addition, subtraction, multiplication and division by executing code locally using the Python interpreter. To facilitate addressing queries related to precision oncology, the LLM leverages the OncoKB25 database to access critical information on medical evidence for treating a vast panel of genetic anomalies, including mutations, copy-number alterations and structural rearrangements. More specifically, GPT-4 can send the HUGO symbol, the change of interest (mutation, amplification or variant) and a specific alteration of interest (such as BRAFV600E) if applicable to the OncoKB API that returns a structured JSON object containing potential Food and Drug Administration-approved or investigational drug options including evidence levels. This enables querying any type of genetic arrangement, given it is listed on OncoKB and an appropriate license is acknowledged by the user. Lastly, GPT-4 is also equipped to engage specialized vision transformer models for the histopathological analysis of phenotypic alterations underlying MSI24 or KRAS and BRAF mutations. For the KRAS and BRAF mutation prediction models, we used a setup similar to Wagner et al.23. These models were obtained by training a vision classifier model using histopathology features extracted from images of colorectal cancer tissue from TCGA. For feature extraction, we used CTranspath47, a state-of-the-art transformer model trained through self-supervised learning. To optimize time and computational resources, features were pre-extracted with CTranspath, which is a common procedure in computational pathology pipelines48. Consequently, instead of the original images, GPT-4 transmits these pre-extracted features to the MSI, KRAS, and BRAF vision transformers; however, it is important to note that this implementation detail is hidden from GPT-4 and, thus, has no influence on our overall pipeline. It, however, allows us to reduce the time and hardware requirements for each run in a research setting. For deployment purposes, one could easily, without making changes to the LLM agent itself, hide additional logic to extract features on the fly from the original images. During each invocation, GPT-4 has to determine the availability of histopathology images for a patient, locate them within the system and select the targets (one, two or all three) for testing. It then receives a binary prediction (MSI versus MSS, KRAS mutant versus wild type or BRAF mutant versus wild type) along with the mutation probability in percentage. All necessary information for calling the designated tools is derivable or producible from the given patient context. However, manual intervention to prompt tool usage is possible, as demonstrated in patients D and X. The specifications for all tools are delineated in JSON format, which is provided to the model and encompasses a brief textual description of each tool's function along with the required input parameters. The deployment of these tools can be executed either independently in parallel or sequentially, wherein the output from one tool serves as the input for another in subsequent rounds; for instance, the size of the segmentation areas obtained from two images through MedSAM can be used to compute a ratio and, thus, define disease progression, stability or response, as shown schematically in Fig. The final retrieval and response generation pipeline is implemented using DSPy49, a library that allows for a modular composition of LLM calls. All instructions to the model can be found in our official GitHub repository. In a method similar to that described by Xiong et al.50, we used chain-of-thought reasoning51 to let the model decompose the initial user query into up to 12 more granular subqueries derived from both the initial patient context and the outcomes from tool applications. This facilitates the retrieval of documents from the vector database that more closely align with each aspect of a multifaceted user query. Precisely, for each generated subquery we extract the top k most analogous document passages from the collection. Subsequently, these data are combined, deduplicated, reranked and finally forwarded to the LLM. We next use cosine distance (lower is better) to compare the query with any embedded chunk from the medical guidelines in the vector database, sorting in ascending order to retrieve the top 40 vectors, each mapped to their respective original text passage. Then, we use Cohere's reranking model (Cohere Rerank 3 English) to reorder the retrieved text passages on the basis of their semantic similarity to the LLM's query. This step filters out passages that exhibit falsely high similarity (low distance) in embedding space but are contextually irrelevant. and text from a guideline saying ‘Drug A is not approved for non-small cell lung cancer' may show high cosine similarity (low distance), yet the former is irrelevant to the query. The reranking step helps to rank such passages lower. To reduce token usage, we remove duplicates from the entire collection of guideline text chunks, prefixed each with an enumeration of ‘Source [x]: …' to allow for accurate citations and then sent the data back to the LLM. Before generating the final answer, we instruct the LLM to generate a step-by-step strategy to build a structured response including identifying missing information that could help refine and personalize the recommendations. To enhance the system's reliability and enable thorough fact-checking, both of which are fundamental in real-world medical applications, the model was programmatically configured to incorporate citations for each statement (as defined as a maximum of two consecutive sentences) using DSPy suggestions49. On the implementation level, the LLM performs a self-evaluation step, wherein it compares its own output to the respective context from our database in a window of one to two sentences. We perform a single iteration over this procedure. All prompts are implemented using DSPy's signatures. We additionally performed comparisons of GPT-4 with two state-of-the-art open-weight LLMs, namely Llama-3 70B from Meta AI28 and Mixtral 8x7B from Mistral29. On the basis of initial evidence from testing, we slightly simplified and shortened the original prompt from GPT-4 but left all other parameters with respect to the original tool composition unaltered. Lastly, we measured the ratio of superfluous tool calls that the models invoked and that failed (not required/failed). In our study, we consistently used the following models through the official OpenAI Python API for all experiments. The temperature value for both models was empirically set to 0.2 for the agent phase and 0.1 during RAG upon initial experimentation and no further modifications of model hyperparameters were performed. Additionally, for generating text embeddings, we used the latest version of OpenAI's embedding models, specifically the text-embedding-3-large model, which produces embeddings with a dimensionality of 3,072. To establish a baseline for comparison without tools and retrieval, we also evaluated GPT-4 with identical hyperparameters using a chain-of-thought reasoning module. Additionally, for model benchmarking, we used the Meta Llama-3 70B model (llama3-70b-8192) and Mistral's Mixtral 8x7B model (mixtral-8x7b-32768) through the Groq API, setting temperature values to 0.2 and the maximum number of output tokens to 4096. To address the limitations in current biomedical benchmarks, we compiled a collection of 20 distinct multimodal patient cases, primarily focusing on gastrointestinal oncology, including colorectal, pancreatic, cholangiocellular and hepatocellular cancers. Each case provided a comprehensive but entirely fictional patient profile, which includes a concise medical history overview encompassing diagnoses, notable medical events and previous treatments. We paired each patient with either a single or two slices of CT or MRI imaging that served as either sequential follow-up staging scans of the liver or lungs or simultaneous staging scans of both the liver and the lungs at a single point in time. Images were obtained predominantly from internal sources from the Department of Diagnostic and Interventional Radiology, University Hospital Aachen, but we included a few samples from the web and the Cancer Imaging Archive52,53. We highlight these cases in Supplementary Note 1. We additionally included information into genomic variations (mutations and gene fusions) in several patient descriptions. To evaluate our model's proficiency in handling complex information, we decided to not pose a single straightforward question but instead structured each query with multiple subtasks, subquestions and instructions, necessitating the model to handle an average of three to four subtasks in each round. To evaluate the robustness of the model in using tools across various patient factors (Extended Data Fig. To enhance the assessment of free-text output, we developed a structured evaluation framework, drawing inspiration from the methodology of Singhal et al.54. In reference to the former, we established a manual baseline for the expected use of tools necessary for generating additional patient information that is crucial for resolving the patient's task. We measured this by the ratio of actual versus expected (required) tool uses. The requirement of tool use was defined as either the model is directly instructed to use a certain tool or the output of a tool is essential to proceed in answering the question, which is the default setting in almost all situations. In assessing the textual outputs, our evaluation first encompassed factual correctness, defined by the proportion of correct replies relative to all model outputs. To segment answers into more manageable units, we split each reply according to statement (where a statement is considered a segment that concludes with either a reference to literature or is followed by a shift in topic in the subsequent sentence). Correspondingly, we distinguished between incorrectness and harmfulness in responses. Incorrect responses may include suggestions for superfluous diagnostic procedures or contain requests for irrelevant patient information. Conversely, harmful responses, while also incorrect, were determined by clinical judgment as potentially deleterious, such as advising suboptimal or contraindicated treatments. These keywords represent expected interventions, such as treatments or diagnostic procedures and were carefully selected for their case relevance and crafted to be as specific as possible (for example, precise treatment combinations such as ‘FOLFOX and bevacizumab' instead of ‘chemotherapy and antiangiogenic drugs'). This criterion, which we term ‘completeness', was supposed to measure the extent to which the agent's response aligns with the essential information that oncologists would anticipate in a human-generated answer under similar conditions. For each reference in the model's output, we investigated the corresponding reference by its source identifier. All evaluations described here were performed independently by four certified clinicians with expertise in oncology. For all benchmarks, we reported the majority vote. In cases of a tie, we selected the most adverse outcome, adhering to a hierarchical schema: correct, irrelevant and wrong for citations and correct, wrong and harmful for the correctness evaluation. Data were analyzed using the pandas library and visualized with matplotlib in Python. No statistical methods were used to predetermine sample sizes. Experiments were not randomized, as only one test group (the AI agent) existed in this proof-of-concept study. Experiments were limited to n = 1 per case because of stringent rate and access limitations at the time of experimentation, when access to GPT-4V was available only as a preview. No data were excluded in the analysis. Reproducibility of the study's findings may be affected by silent changes implemented to the model by its developers, which may not always be publicly disclosed, as well as by potential future model deprecations. Despite these factors, the results are expected to remain reproducible when using other current state-of-the-art models. In these instances, we reran each sample in newly instantiated settings until no such refusals occurred. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. All radiology images used in this study are available from the respective URLs as indicated in the Supplementary Information. The histopathology related results in our study are based upon data generated by TCGA Research Network (https://www.cancer.gov/tcga). Source data are provided with this paper. An implementation of our source codes for researchers to extend on our work is available from GitHub (https://github.com/Dyke-F/LLM_RAG_Agent). Zhao, W. X. et al. A survey of large language models. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on medical challenge problems. Ferber, D. et al. GPT-4 for information retrieval and comparison of medical oncology guidelines. Acosta, J. N., Falcone, G. J., Rajpurkar, P. & Topol, E. J. Multimodal biomedical AI. Artificial intelligence for multimodal data integration in oncology. Khader, F. et al. Multimodal deep learning for integrating chest radiographs and clinical parameters: a case for transformers. Chen, R. J. et al. Pan-cancer integrative histology-genomic analysis via multimodal deep learning. Lu, M. Y. et al. A multimodal generative AI copilot for human pathology. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Derraz, B. et al. New regulatory thinking is needed for AI-based personalised drug and cell therapies in precision oncology. Chen, Z. et al. MEDITRON-70B: scaling medical pretraining for large language models. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. 34th International Conference on Neural Information Processing Systems 793, 9459–9474 (2020). Zakka, C. et al. Almanac—retrieval-augmented language models for clinical medicine. Yao, S. et al. ReAct: synergizing reasoning and acting in language models. Schick, T. et al. Toolformer: language models can teach themselves to use tools. Tayebi Arasteh, S. et al. Large language models streamline automated machine learning for clinical studies. Messiou, C., Lee, R. & Salto-Tellez, M. Multimodal analysis and the oncology patient: creating a hospital system for integrated diagnostics and discovery. Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer. Wagner, S. J. et al. Transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study. El Nahhas, O. S. M. et al. Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology. Chakravarty, D. et al. OncoKB: a precision oncology knowledge base. He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. PathVQA: 30000+ questions for medical visual question answering. Adams, L. et al. LongHealth: a question answering benchmark with long clinical documents. Introducing Meta Llama 3: the most capable openly available LLM to date. Truhn, D., Reis-Filho, J. S. & Kather, J. N. Large language models should be used as scientific reasoning engines, not knowledge databases. Saillard, C. et al. Validation of MSIntuit as an AI-based pre-screening tool for MSI detection from colorectal cancer histology slides. Blankemeier, L. et al. Merlin: a vision language foundation model for 3D computed tomography. Improving the applicability of AI for psychiatric applications through human-in-the-loop methodologies. Bruch, S., Gai, S. & Ingber, A.An analysis of fusion functions for hybrid retrieval. Gemini Team Google et al. Gemini 1.5: unlocking multimodal understanding across millions of tokens of context. & Rajpurkar, P. A generalist learner for multifaceted medical image interpretation. Lammert, J. et al. Expert-guided large language models for clinical decision support in precision oncology. The Claude 3 model family: Opus, Sonnet, Haiku. Gilbert, S., Harvey, H., Melvin, T., Vollebregt, E. & Wicks, P. Large language model AI chatbos require approval as medical devices. Lyman, G. H. ASCO clinical practice guidelines and beyond. Wang, X. et al. Transformer-based unsupervised contrastive learning for histopathological image classification. El Nahhas, O. S. M. et al. From whole-slide image to biomarker prediction: a protocol for end-to-end deep learning in computational pathology. Khattab, O. et al. DSPy: compiling declarative language model calls into self-improving pipelines. Xiong, W. et al. Answering complex open-domain questions with multi-hop dense retrieval. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Preoperative CT and survival data for patients undergoing resection of colorectal liver metastases. The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository. Singhal, K. et al. Large language models encode clinical knowledge. is supported by the German Federal Ministry of Health (DEEP LIVER, ZMVI1-2520DAT111; SWAG, 01KD2215B), the Max-Eder Program of the German Cancer Aid (70113864), the German Federal Ministry of Education and Research (PEARL, 01KD2104C; CAMINO, 01EO2101; SWAG, 01KD2215A; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan), the German Academic Exchange Service (57616814), the German Federal Joint Committee (Transplant.KI, 01VSF21048), the European Union's Horizon Europe and Innovation program (ODELIA, 101057091; GENIAL, 101096312) and the National Institute for Health and Care Research (NIHR, NIHR213331), Leeds Biomedical Research Center. is funded by the German Federal Ministry of Education and Research (TRANSFORM LIVER, 031L0312A), the European Union's Horizon Europe and Innovation program (ODELIA, 101057091) and the German Federal Ministry of Health (SWAG, 01KD2215B). is supported by the TUM School of Medicine and Health Clinician Scientist Program (project no. receives intellectual and financial support through the DKTK School of Oncology Fellowship. No other funding is disclosed by any of the authors. Dyke Ferber, Dirk Jäger & Jakob Nikolas Kather Dyke Ferber, Omar S. M. El Nahhas, Isabella C. Wiest, Jan Clusmann, Marie-Elisabeth Leßmann & Jakob Nikolas Kather Department of Gynecology and Center for Hereditary Breast and Ovarian Cancer, University Hospital rechts der Isar, Technical University of Munich (TUM), Munich, Germany Center for Personalized Medicine (ZPM), University Hospital rechts der Isar, Technical University of Munich (TUM), Munich, Germany European Network for Rare Cancers (EURACAN) Initiative, Munich, Germany Integrated Pathology Unit, Institute for Cancer Research and Royal Marsden Hospital, London, UK Department of Epidemiology and Biostatistics, Memorial Sloan Kettering Cancer Center, New York, NY, USA You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar provided feedback and contributed to writing the manuscript. has received honoraria for lectures by Bayer and holds shares in StratifAI and holds shares in Synagen. has received honoraria from MSD and BMS. The remaining authors declare no competing interests. Nature Cancer thanks Anant Madabhushi, Arsela Prelaj, Pranav Rajpurkar and Wayne Zhao for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. We show more details on the simulated patient cases from our benchmarking experiments including the sex (A) and age (B) distribution for all 20 cases. (D) To investigate whether gender, age, and origin influence the models' tool-calling behavior, we conducted an additional experiment with 15 random permutations on all 20 patient cases (300 in total). Notably, we observed that in contrary to patient cases requiring relatively fewer tools (for example, patients Adams, Lopez and Williams), there was higher variability in tool-calling behavior in situations requiring more tools (for example, patient Ms Xing), regardless of the combinations of age, sex, and ethnicity/origin. Heatmaps are annotated on the x-axis as ‘age-sex-ethnicity/country of origin'. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Ferber, D., El Nahhas, O.S.M., Wölflein, G. et al. Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: Cancer newsletter — what matters in cancer research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41593-025-01944-z'>Spatial reasoning via recurrent neural dynamics in mouse retrosplenial cortex</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 09:59:36
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. From visual perception to language, sensory stimuli change their meaning depending on previous experience. Recurrent neural dynamics can interpret stimuli based on externally cued context, but it is unknown whether they can compute and employ internal hypotheses to resolve ambiguities. Here we show that mouse retrosplenial cortex (RSC) can form several hypotheses over time and perform spatial reasoning through recurrent dynamics. In our task, mice navigated using ambiguous landmarks that are identified through their mutual spatial relationship, requiring sequential refinement of hypotheses. RSC encoded hypotheses as locations in activity space with divergent trajectories for identical sensory inputs, enabling their correct interpretation. Our results indicate that interactions between internal hypotheses and external sensory data in recurrent circuits can provide a substrate for complex sequential cognitive reasoning. External context can change the processing of stimuli through recurrent neural dynamics1. In this process, the evolution of neural population activity depends on its own history as well as external inputs2, giving context-specific meaning to otherwise ambiguous stimuli3. To study how hypotheses can be held in memory and serve as internal signals to compute new information, we developed a task that requires sequential integration of spatially separated ambiguous landmarks4. We trained freely moving mice to distinguish between two perceptually identical landmarks, formed by identical dots on a computer-display arena floor, by sequentially visiting them and reasoning about their relative locations. The landmarks were separated by <180 degrees in an otherwise featureless circular arena (50-cm diameter), to create a clockwise (CW) (‘a') and a counterclockwise (CCW) (‘b') landmark. The mouse's task was to find and nose-poke at the CCW ‘b' landmark for water reward (‘b' was near one of 16 identical reward ports spaced uniformly around the arena; other ports caused a time out). At most, one landmark was visible at a time (enforced by tracking mouse position and modulating landmark visibility based on relative distance (Extended Data Fig. In the interval after first encountering a landmark (‘LM1' phase), an ideal agent's location uncertainty is reduced to two possibilities, but there is no way to disambiguate whether it saw ‘a' or ‘b.' After seeing the second landmark, an ideal agent could infer landmark identity (‘a' or ‘b'; this is the ‘LM2' phase; Fig. 1b) by estimating the distance and direction traveled since the first landmark and comparing those with the learned relative layout of the two landmarks; thus, an ideal agent can use sequential spatial reasoning to localize itself unambiguously. For most analyses, we ignored cases where mice might have gained information from not encountering a landmark, for example, as the artificial neural network (ANN) does in Fig. To randomize the absolute angle of the arena at the start of each new trial (and thus avoid use of any olfactory or other allocentric cues), mice had to complete a separate instructed visually guided dot-hunting task, after which the landmarks and rewarded port were rotated randomly together (Extended Data Fig. a, Two perceptually identical landmarks are visible only from close up, and their identity is defined only by their relative location. One of 16 ports, at landmark ‘b,' delivers reward in response to a nose-poke. The animal must infer which of the two landmarks is ‘b' to receive reward; wrong pokes result in timeout. Tetrode array recordings in RSC yield 50–90 simultaneous neurons. b, Top, schematic example trial; bottom, best possible guesses of the mouse position. LM0, LM1 and LM2 denote task phases when the mouse has seen zero, one or two landmarks and could infer their position with decreasing uncertainty. c, Left, example training curve showing Phit/Pfalse-positive; random chance level is 1/16 for 16 ports. Mice learned the task at values >1, showing they could disambiguate between the two sequentially visible landmarks. This requires the formation, maintenance and use of spatial hypotheses. Asterisks denote per-session binomial 95% significance for the correct rate. Right, summary statistics show binomial CIs on last half of sessions for all four mice. d, Mouse location heatmap from one session (red) with corresponding spatial firing rate profiles for five example cells; color maps are normalized per cell. e, Task phase (corresponding to hypothesis states in b can be decoded from RSC firing rates. Horizontal line, mean; gray shaded box, 95% CI. f, Spatial coding changes between LM1 and LM2 phases (Euclidean distances between spatial firing rate maps, control within versus across condition; see Extended Data Fig. 2a for test by decoding, median and CIs (bootstrap)). g, Spatial versus task phase information content of all neurons and position and state encoding for example cells. Gray, sum-normalized histograms (color scale as in d). a, Schematic examples of hypothesis-dependent landmark interpretation. Left, mouse encounters first LM, then identifies the second as ‘a' based on the short relative distance. Right, a different path during LM1 leads the mouse to a different hypothesis state, and to identify the perceptually identical second landmark as ‘b.' b, Structure of an ANN trained on the task. Right, mean absolute localization error averaged across test trials for random trajectories. c, Activity of output neurons ordered by preferred location shows transition between LM0, LM1 and LM2 phases. During LM1 (when the agent has only seen one landmark), two hypotheses are maintained, with convergence to a stable unimodal location estimate in LM2 after encountering the second landmark. d, 3D projection from PCA of ANN hidden neuron activities. e, Example ANN trajectories for two trials show how identical visual input (black arrowheads) leads the activity to travel to different locations on the LM2 attractor because of different preceding LM1a/b states. 1c), showing that they learn to form hypotheses about their position during the LM1 phase, retain and update these hypotheses with self-motion information until they encounter the second (perceptually identical) landmark, and use them to disambiguate location and determine the rewarded port. We hypothesized that RSC, which integrates self-motion5, position6,7,8, reward value9 and sensory10 inputs, could perform this computation. RSC is causally required to process landmark information11, and we verified that RSC is required for integrating spatial hypotheses with visual information but not for direct visual search with no memory component (Extended Data Fig. We recorded 50–90 simultaneous neurons in layer 5 of RSC in four mice during navigational task performance using tetrode array drives12 and behavioral tracking (Fig. RSC neurons encoded information about both the mouse's location (Fig. 1d) and about the task phase, corresponding to possible location hypotheses (Fig. This hypothesis encoding was not restricted to a separate population: most cells encoded both hypothesis state as well as the animal's location (Fig. This encoding was distinct from the encoding of landmark encounters in the interleaved dot-hunting task and was correlated per session with behavioral performance (Extended Data Fig. The encoding of mouse location changed significantly across task phases (Fig. 1d,f), similar to the conjunctive coding for other spatial and task variables in RSC6. This mixed co-encoding of hypothesis, location and other variables suggests that RSC can transform new ambiguous sensory information into unambiguous spatial information through the maintenance and task-specific use of internally generated spatial hypotheses. To test whether recurrent neural networks can solve sequential spatial reasoning tasks that require hypothesis formation, and to provide insight into how this might be achieved in the brain, we trained a recurrent ANN on a simplified one-dimensional (1D) version of the task, since the relevant position variable for the landmarks was their angular position (inputs were random noisy velocity trajectories and landmark positions, but not their identity; Fig. 2b), outperforming path integration with correction (corresponding to continuous path integration13,14 with boundary/landmark resetting15,16) and represented multimodal hypotheses, transitioning from a no-information state (in LM0) to a bimodal two-hypothesis coding state (LM1) and finally to a full information, one-hypothesis coding state (LM2) (Fig. Bimodal hypothesis states did not emerge when the ANN was given the landmark identity (Extended Data Fig. Together, this shows that recurrent neural dynamics are sufficient to internally generate, retain and apply hypotheses to reason across time based on ambiguous sensory and motor information, with no external disambiguating inputs. Both ANN and RSC neurons encoded several navigation variables conjunctively (Extended Data Fig. 2b) and transitioned from encoding egocentric landmark-relative position during LM1 to a more allocentric encoding during LM2 (Extended Data Fig. Instantaneous position uncertainty (variance derived from particle filter) could be decoded from ANN activity (Extended Data Fig. ANN neurons preferentially represented landmark locations (Extended Data Fig. 2c; consistent with overrepresentation of reward sites in hippocampus17,18), but we did not observe this effect in RSC. Average spatial tuning curves of ANN neurons were shallower in the LM1 state relative to LM2, corresponding to trial-by-trial ‘disagreements' between neurons, evident as bimodal rates per location. RSC rates similarly became less variable across trials per location in LM2 (Extended Data Fig. 7), indicating that, in addition to the explicit encoding of hypotheses/uncertainty (Fig. 1e,g), there is a higher degree of trial-to-trial variability in RSC as a function of spatial uncertainty. The ANN computed, retained and used multimodal hypotheses to interpret otherwise ambiguous inputs: after encountering the first landmark, the travel direction and distance to the second is sufficient to identify it as ‘a' or ‘b' (Figs. There are four possible scenarios for the sequence of landmark encounters: ‘a' then ‘b', or ‘b' then ‘a', for CW or CCW travel directions, respectively. To understand the mechanism by which hypothesis encoding enabled disambiguation, we examined the moment when the second landmark becomes visible and can be identified (Fig. We designate LM1 states in which the following second landmark is ‘a' as ‘LM1a' and those that lead to ‘b' as ‘LM1b.' Despite trial-to-trial variance resulting from random exploration trajectories and initial poses, ANN hidden unit activity fell on a low-dimensional manifold (correlation dimension d ≈ 3; Fig. 3d) and could be well captured in a three-dimensional (3D) embedding using principal component analysis (PCA) (Fig. Activity states during the LM0,1,2 phases (green, blue and gray/red, respectively) were distinct, and transitions between phases (mediated by identical landmark encounters; black arrows) clustered into discrete locations. Examining representative trajectories (for the CCW case; Fig. 2e) reveals that LM1a and LM1b states are well-separated in activity space. In both cases, an identical transient landmark input pushes the activity from distinct hypothesis-encoding regions of activity space onto different appropriate locations in the LM2 state, constituting successful localization. a, Correlation structure in ANN activity is maintained across task phases, indicating maintained low-dimensional neural dynamics across different computational regimes. Top, pairwise ANN tuning correlations in LM1 and LM2 (same ordering, by preferred location). b, Same analysis as a, but for RSC in one session (N = 64 neurons, computed on entire spike trains, sorted via clustering in LM1). The reorganization of spatial coding as hypotheses are updated (Fig. 1d,f) is constrained by the stable pairwise structure of RSC activity. c, Summary statistics (session median and quartiles) for maintenance of correlations across task phases. This also extends to a separate visually guided dot-hunting task (Extended Data Fig. d, Activity in both the ANN and RSC is locally low-dimensional, through correlation dimension (the number of points in a ball of some radius grows with radius to the power of N if data is locally N-dimensional) on 20 principal components. As these correlation matrices are the basis for projections into low-dimensional space, this shows that the same low-dimensional dynamics were maintained, despite spanning different computational and hypothesis-encoding regimes (metastable two-state encoding with path integration in LM1 versus stable single-state path integration unchanged by further landmark inputs in LM2; Extended Data Fig. Low-dimensional pairwise structure was also conserved across different landmark configurations and varied ANN architectures, and the low-dimensionality of ANN states was robust to large perturbations (Extended Data Fig. In sum, these computations were determined by one stable set of underlying recurrent network dynamics, which, together with appropriate self-motion and landmark inputs, can maintain and update hypotheses to disambiguate identical landmarks over time, with no need for external inputs. We hypothesized that RSC and its reciprocally connected brain regions may, similarly to the ANN, use internal hypotheses to resolve landmark ambiguities using recurrent dynamics. Using the ANN as a template for a minimal dynamical system that can solve the task (Fig. 2), we asked whether neural activity in RSC is consistent with a system that could solve the task with the same mechanisms. To be described as a dynamical system, neural activity must first be sufficiently constrained by a stable set of dynamics, that is, the activity of neurons must be sufficiently influenced by that of other neurons, and these relationships must be maintained over time1. To test this property, we first computed pairwise rate correlations and found a preserved structure between LM1 and LM2, as in the ANN (median R (across sessions) of Rs (across cells) = 0.74 in RSC, versus 0.73 in ANN; Fig. Firing rates could be predicted from rates of other neurons, using pairwise rate relationships across task phases; this maintained structure also extended to the visual dot-hunting behavior (Extended Data Fig. Because pairwise correlations form the basis of dimensionality reduction, this shows that low-dimensional RSC activity is coordinated by the constraints of stable recurrent neural dynamics and not a feature of a specific behavioral task or behavior. To employ neural firing rates as states of a dynamical system that act as memory and computational substrates in the same manner as in the ANN, they should also be low-dimensional. Consistent with the stable relationships between neurons, most RSC population activity was low-dimensional (around six significant principal components, and correlation dimension of around 5.4; Fig. Together, we find that despite significant changes in neural encoding as different hypotheses are entertained across task phases (Fig. 4a–d), the evolution of firing rates in RSC is constrained by stable dynamics that could implement qualitatively similar states as the ANN. The ANN solves the task using distinct hypothesis states that are updated with visual inputs and locomotion, by placing them in the state space so that visual input arriving at different hypothesis states within LM1 (LM1a versus LM1b) pushes activity onto the correct states in LM2 (Fig. We examined this process in RSC by first looking at the evolution of neural states during the spatial reasoning process. States evolved at speeds correlated with animal locomotion, consistent with the observation that hypotheses are updated by self-motion in between landmark encounters and were driven by landmark encounters consistent with findings in head-fixed tasks11 (Extended Data Fig. Neural states were also driven by failures to encounter landmarks at expected positions, which can also be informative (Fig. 2e, right), albeit with a different neural encoding than we observed for encountering the landmarks (Extended Data Fig. We next tested whether sufficiently separated neural states, LM1a and LM1b, together with stable low-dimensional attractor dynamics could resolve the identity of the second landmark. If so, this would suggest that, as in the ANN, the ensemble activity state in RSC can serve both as memory and affect future computations. We identified subsets of trials in which mouse motion around the LM1 to LM2 transition was matched closely and aligned them in time to the point when the second landmark became visible (Fig. In these trials, locomotion and visual inputs are matched, and only the preceding hypothesis state (LM1a or b) differs. RSC firing rates differed between LM1a and LM1b states, as did subsequent rates in LM2 (comparing within- to across-group distances in neural state space across matched trials, and by decoding state from firing rates: Fig. a, Top, to study hypothesis encoding and its impact without sensory or motor confounds, we used trials with matched egocentric paths just before and after the second landmark (‘a' or ‘b') encounter. Bottom, 3D neural state space trajectories (isomap); RSC latent states do not correspond directly to those of the ANN. b, RSC encodes the difference between LM1a and LM1b, and between subsequent LM2 states, as in the ANN (Fig. Blue, within-group and grey, across-group distances in neural state space. State can also be decoded from raw spike rates (Extended Data Fig. c, Neural dynamics in RSC are smooth across trials: pairwise distances between per trial spike counts in a 750 ms window before LM2 onset remain correlated with later windows; line, median; shading, CIs (bootstrap). d, RSC activity preceding the second landmark encounter predicts correct/incorrect port choice (horizontal line, mean; gray shaded box, 95% CI from bootstrap, cross-validated regression trees). e, Decoding of hypothesis states and position from RSC using ANNs to illustrate the evolution of neural activity in the task-relevant space (see b, c and d and Fig. Left, if RSC encodes only current spatial and sensorimotor states and no hypotheses beyond landmark count (LM1a or LM2b, derived from seeing the first landmark and self-motion integration that lead to identifying the second landmark as ‘a' or ‘b'), an external disambiguating input is needed. Right, because task-specific hypotheses arising from the learned relative position of the landmarks are encoded (this figure), and activity follows stable attractor dynamics (Fig. 3), ambiguous visual inputs can drive the neural activity to different positions, disambiguating landmark identity in RSC analogously to the ANN. To compute with the same mechanism as the ANN, neural states must be governed by stable dynamics consistently enough for current states to reliably influence future states, which requires that nearby states do not diffuse or mix too quickly1. We found that RSC firing rates were predictable across trials such that neighboring trials in activity space remained neighbors (Fig. 4c), which further confirms stable recurrent dynamics, that these states can be used as computational substrate, and indicates a topological organization of abstract task variables19. This indicates that stably maintained hypothesis-encoding differences in firing over LM1 could interact with ambiguous visual landmark inputs to push neural activity from distinct starting points in neural state space to points that correspond to correct landmark interpretations, as in the ANN. The ANN achieved high correct rates, but mice make mistakes. If the dynamical systems interpretation holds, such mistakes would be explainable by LM1a or b states that are not in the right location, and lead to the wrong LM2 interpretation. Indeed, we observed that neural trajectories from LM1a that were close in activity space to LM1b were dragged along LM1b trajectories and vice-versa (they had similar movement directions; Extended Data Fig. 9g,h), suggesting that behavioral landmark identification outcomes might be affected by how hypotheses were encoded in RSC during LM1. We tested this hypothesis and found that RSC activity in LM1 (last 5 s preceding the transition to LM2) was predictive of the animal's behavioral choice of the correct versus incorrect port (Fig. Notably, this behaviorally predictive hypothesis encoding was absent during training in sessions with low task performance (Extended Data Fig. 4), indicating that the dynamical structures and hypothesis states observed in RSC were task-specific and acquired during learning. Our unrestrained nonstereotyped behavior is not amenable to direct comparison of activity trajectories between ANNs and the brain as others have done in highly stereotyped trials of macaque behavior1. Instead, we found that the dynamics of firing rates in mouse RSC are consistent with, and sufficient for, implementing hypothesis-based disambiguation of identical landmarks using a similar computational mechanism as observed in the ANN. We report that RSC represents internal spatial hypotheses, sensory inputs and their interpretation and fulfills the requirements for computing and using hypotheses to disambiguate landmark identity using stable recurrent dynamics. Specifically, we found that low-dimensional recurrent dynamics were sufficient to perform spatial reasoning (that is to form, maintain and use hypotheses to disambiguate landmarks over time) in an ANN (Fig. 10 for non-negative ANNs and when no map input was given). We then found that RSC fulfills the requirements for such dynamics, that is, encoding of the required variables (Figs. 3) and smooth dynamics that predicted behavioral outcomes (Fig. Due to the higher trial-to-trial variability and lower number of recorded cells, we do not draw direct connections between specific latent states of the ANN and neural data, as was done in previous studies in primates2,3,20 or simpler mouse tasks19,21. We observed that local dynamics in RSC can disambiguate sensory inputs based on internally generated and maintained hypotheses without relying on external context inputs at the time of disambiguation (Fig. 4), indicating that RSC can derive hypotheses over time and combine these hypotheses with accumulating evidence from the integration of self-motion (for example, paths after the first landmark encounter) and sensory stimuli to solve a spatiotemporally extended spatial reasoning task. These results do not argue for RSC as an exclusive locus of such computations. There is evidence for parallel computations, likely at different levels of abstraction, across subcortical22 and cortical regions such as PFC3,23,24, PPC25, LIP26 and visual27,28 areas. Further, hippocampal circuits contribute to spatial computations beyond representing space by learning environmental topology29 and constraining spatial coding using attractor dynamics19,30,31 shaped by previous experience32. Finally, the landmark disambiguation that we observed probably interacts with lower sensory areas33, reward value9,34 and action selection computations21,35. The emergence of conjunctive encoding, explicit hypothesis codes and similar roles for dynamics across RSC and the ANN suggests that spatial computations and, by extension, cognitive processing in neocortex may be constrained by simple cost functions36, similar to sensory37 or motor38 computations. The ANN does not employ sampling-based representations, which have been proposed as possible mechanisms for probabilistic computation39,40, showing that explicit representation of hypotheses and uncertainty as separate regions in rate space could serve as alternative or supplementary mechanism to sampling. A key open question is how learning a specific environment, task or behavioral context occurs. We observed that hypothesis coding emerges with task learning (Extended Data Fig. Possible, and not mutually exclusive, mechanisms include: (1) changes of the stable recurrent dynamics in RSC, as is suggested in hippocampal CA1 (ref. 29); (2) modification of dynamics by context-specific tonic inputs3,20; or (3) changes in how hypotheses and sensory information are encoded and read out while maintaining attractor dynamics that generalize across environments or tasks, as indicated by the maintenance of recurrent structure across tasks in our data (Extended Data Fig. Further, how such processes are driven by factors such as reward expectation34 is an active area of research. Our findings show that recurrent dynamics in neocortex can simultaneously represent and compute with task and environment-specific multimodal hypotheses in a way that gives appropriate meaning to ambiguous data, possibly serving as a general mechanism for cognitive processes. Lightweight drive implants with 16 movable tetrodes were built as described previously12. The tetrodes were arranged in an elongated array of approximately 1,250 × 750 µm, with an average distance between electrodes of 250 µm. Tetrodes were constructed from 12.7-µm nichrome wire (Sandvik–Kanthal, QH PAC polyimide coated) with an automated tetrode twisting machine45 and gold-electroplated to an impedance of approximately 300 kΩ. Mice (male, C57BL/6 RRID: IMSR_JAX:000664) were aged 8–15 weeks at the time of surgery. Animals were housed in pairs or triples when possible and maintained on a 12-h cycle, at 65–70 °F with ~60% humidity. Mice were anesthetized with isofluorane (2% induction, 0.75–1.25% maintenance in 1 l min−1 oxygen) and secured in a stereotaxic apparatus. A heating pad was used to maintain body temperature; additional heating was provided until fully recovered. The scalp was shaved, wiped with hair-removal cream and cleaned with iodine solution and alcohol. After intraperitoneal (IP) injection of dexamethasone (4 mg kg−1), carprofen (5 mg kg−1), subcutaneous injection of slow-release buprenorphine (0.5 mg kg−1) and local application of lidocaine, the skull was exposed. The skull was cleaned with ethanol, and a thin base of adhesive cement (C&B Metabond and Ivoclar Vivadent Tetric EvoFlow) was applied. A stainless steel screw was implanted superficially anterior of bregma to serve as electrical ground. A 3-mm craniotomy was drilled over central midline cortex, a durotomy was performed on one side of the central sinus and tetrode drives12 were implanted above RSC, at around anterior–posterior (AP) −1.25 to −2.5 mm and medio–lateral (ML) 0.5 mm, with the long axis of the tetrode array oriented AP and the tetrode array tilted inwards at an angle of ~15–20° and fixed with dental cement. All other tetrodes were lowered into superficial layers of cortex within 3 days postsurgery. Mice were given 1 week to recover before the start of recordings. After implant surgery, individual tetrodes were lowered over the course of several days until a depth corresponding to layer 5 was reached and spiking activity was evident. Data were acquired with an Open Ephys46 ONIX47 prototype system at 30 kHz using the Bonsai software48 (v.2.2; https://bonsai-rx.org/). Tetrodes were occasionally lowered by small increments of ~50 µm to restore good recording conditions or to ensure sampling of new cells across sessions. Voltage data from the 16 tetrodes, sampled at 30 kHz were bandpass filtered at 300–6,000 Hz, and a median of the voltage across all channels that were well connected to tetrode contacts was subtracted from each channel to reduce common-mode noise such as licking artifacts. Spike sorting was then performed per tetrode using the Mountainsort software49 (https://github.com/flatironinstitute/mountainsort_examples), and neurons were included for further analysis if they had a noise overlap score <0.05, an isolation score >0.75 (provided by Mountainsort49), a clear refractory period (to ensure spikes originated from single neurons), a spike waveform with one peak and a clear asymmetry (to exclude recordings from passing axon segments) and a smooth voltage waveform and ISI (inter spike interval) histogram (to exclude occasional spike candidates driven by electrical noise). Units were not excluded based on firing rates, tuning or any higher order firing properties. The number of simultaneously recorded cells per mouse for the main analyses was as follows. 3), electrolytic lesions were created by passing currents of 20 µA through a subset of tetrodes (roughly four tetrodes per animal) for 30 s each under isoflurane anesthesia, and animals were perfused and brain processed 1 h later. Behavior was carried out in a circular arena of 50-cm diameter. The floor of the arena was formed by a clear acrylic sheet, under which a diffusion screen and a flat-screen TV was positioned on which visual stimuli were displayed. The circular arena wall was formed by 32 flat black acrylic segments, every other one of which contained an opening for a recessed reward ports, 16 in total. Each reward port contained an optical beam break (880-nm infrared (IR), invisible to mouse) that detected if a mouse was holding its nose in the port, a computer-controlled syringe pump for water reward delivery and a dedicated beeper as a secondary reward indicator. The behavior arena was housed in a soundproof and light-insulated box with no indicators that could allow the mice to establish their heading. Video was acquired by a central overhead camera at 30 Hz using a low level of infrared light at 850 nm and the mouse position was tracked using the oat software50 (https://github.com/jonnew/Oat). A custom behavioral control state machine written in Python was triggered every time a new camera frame was acquired, and the position of the animal, time passed and port visits were used to transition the logic of the state machine (Extended Data Fig. For analysis purposes, all behavioral data was resampled to 100 Hz and synchronized to the electrophysiological data. For pharmacological inactivation of RSC (Extended Data Fig. 1i–l), four mice were trained on a simplified parametric task that permitted us to causally test the role of RSC in individual recording and inactivation sessions. The task required integration of an allocentric position hypothesis with visual landmarks (Extended Data Fig. After mice learned the task—quantified as reaching a hit rate of above 30% in the simple conditions (high eccentricity; Extended Data Fig. 1j)—they were given access to unrestricted water and implanted following the procedure described for the main experiment but, instead of a chronic drive implant, a removable cap was implanted and two burr holes were prepared above RSC and covered with dental cement (Extended Data Fig. After recovery from surgery, mice were put back on water restriction over the course of 1 week and reintroduced to the task. Before each experiment, mice were anesthetized briefly with isoflurane, the cap was opened temporarily and the exposed skull was wiped with lidocaine and an injection of either 50 nl of 1 μg ml−1 muscimol solution in cortex buffer per side, or the same volume of cortex solution was performed through the existing burr holes. Mice were left to recover from anesthesia for 15 min and tested on the task. After mice had undergone surgery, they were given at least 1 week to recover before water scheduling began. During this period, mice were handled by experimenters and habituated to the arena. Throughout the entire experiment mice were given water rewards for completion of the task and were given additional water to maintain their total water intake at 1.25–1.5 ml. After initial acclimation to the recording arena over 2 days, mice were trained on the task. Throughout the task we used white circular cues on the floor (referred to as landmarks) of ~30-mm diameter on a black background. Initially, mice were trained that circular visual cues on the floor of the arena indicated reward locations. One of the 16 ports was selected randomly as reward port and a cue was shown in front of this port. Visiting an incorrect port resulted in a time out (~1 s initially, increased later), during which the entire arena floor was switched to gray leading to a widespread visual stimulus. After a reward, a new reward port was chosen randomly, and the landmark was rotated together with the port, effectively performing a rotation of the entire task, and the next trial began. This meant that mice learned to not rely on any cues other than the visual landmark to locate the correct port. Mice usually completed this phase in by day 4. We then introduced a new task phase, referred to in the text as ‘dot-hunting' task: after each reward, the landmark disappeared and instead a blinking dot was shown in a random location in the arena. If the mouse walked over that dot, it disappeared and either a new dot in a new random location appeared, repeating the process, or the next trial was initiated. The number of required dots–chases was sampled uniformly from a range and was increased to six to eight by the time recordings began, and the last dot was always positioned at the arena center. Data acquired during this task phase were used during spike sorting but were not part of the main dataset in which we analyzed hypothesis representation. We analyze this task phase separately in Fig. Mice learned this task phase, with six to eight dots, by day 7 on average. Throughout phases 1 and 2, we progressively introduced a requirement for the mice to hold their snouts in the reward port for increasing durations to trigger a reward or time out. For each port visit, the required duration was drawn randomly from a uniform distribution, so on any given trial the mice did not know when exactly to expect to know the outcome of the port visit. Initially, this hold time was 500 ms, and the time range was slowly increased throughout training, depending on animal performance. By the time recordings began, a range of around 4–6 s was used. Mice were able to tolerate this holding time by day 20 on average. Next, we introduced an identical second landmark at a nonrewarded port. Initially, the two landmarks were set two ports apart (for example, ports 1 and 3), and this distance was progressively increased to four or five ports. As before, the rewarded port and landmarks were rotated randomly after each trial, but their relative positions remained stable. As a result, mice learned to visit the ‘b' port. Mice learned to make an initial distinction between the ports approximately by day 14–16. In one mouse, we maintained this training phase until overall task performance was significant over entire sessions (Extended Data Fig. We therefore transitioned subsequent mice to the next phases before a stable behavior was established. After the mice started learning to visit the port at the ‘b' landmark, we introduced a view distance limitation that made landmarks invisible from far away: the mouse's position was tracked at 30 Hz and, for each landmark, its brightness was modulated in real time as a function of the mouse's distance from it. The visibility was 0 for distances above a threshold, 1 for distances below a second threshold and transitioned linearly between the two values. For clarity, we draw only the first threshold where landmarks initially become visible in the illustrations. The second threshold was typically set to about 50% of the first, leading to a gradual brightening, but in the otherwise totally dark arena, almost any values >1 are clearly visible. Initially, thresholds were set so that both landmarks were visible from the arena center (~20 cm); they were then reduced progressively to values where, at any one time, only one of the landmarks was visible to the mouse (~10 cm). At this stage, mice that encounter a landmark after a new trial starts have no way of knowing whether this is the rewarded or nonrewarded landmark, unless they infer landmark identity via path integration (See Fig. Recordings began when mice were able to complete 100 trials per hour at a hit/miss rate >1. Mice reached this criterion level on average by total day 30–40 of training. Statistical tests were carried out in Matlab (Mathworks, v.2019) using built-in functions. Unless stated otherwise, CIs were computed at a 95% level using bootstrap, and P values were computed using a Mann–Whitney U test or Wilcoxon signed-rank test. No statistical method was used to predetermine sample sizes. Recording sessions were included once mice performed the task well enough to achieve a session average hit/miss ratio >1, indicating that mice could infer the correct port between the ‘a' and ‘b' landmarks (a correct rate of >1/16 would indicate that they can associate landmarks with rewarded ports, but not that they can infer landmark identity). Because landmarks are visible sequentially only after full training, a ratio >1 shows that mice employed a memory based strategy where they used a previous hypothesis derived from seeing or not seeing the first landmark, together with path integration, to infer the identity of the second landmark they encounter. Only sessions with at least 50 recorded single neurons, and with at least 50 min of task performance were included. For some analyses, particularly for analyses where trajectories of the mice were matched across trial types to control for potential motor and sensory confounds, additional selection criteria were applied yielding a lower number of sessions that could be used, this is stated for the respective analyses. For plots of the learning rates, we included trials where mice encountered their first landmark after 20 s or faster to exclude periods where mice were not engaged. The timepoints when landmarks became visible and the mouse transitioned from LM0 to LM1 or from LM1 to LM2, referred to as ‘landmark encounters' were defined as the timepoint when landmark visibility exceeded 50%. For analyses of the correlation of neural state and eventual behavioral outcomes, each second landmark encounter was further categorized as whether it occurred at the ‘a' or ‘b' landmark. 1f) was quantified by the Euclidian distance of their spatial tuning profiles (in an 8 × 8 map, resulting in a 64-element vector, for each comparison nonvisited ties were omitted). As an internal control, distance between tuning profiles within condition and across condition were compared using nonoverlapping 1-min segments. The control levels are different between the cases because the amount of data per session, reliability of firing, and so on, is not constant, and each control is valid only for its test data. All decoding analyses were performed on the entire neural population with no preselection. To decode the mouse position from RSC firing rates, neural firing rates were first low-pass filtered at 1 Hz with a single-pole Butterworth filter. The resulting firing rate time series were used to predict the mouse position as 100 categorical variables forming a 10 × 10 bin grid (bin width = 50 mm). The network was made up of a single long short-term memory (LSTM) layer with 20 units, and a fully connected layer into a softmax output into the 100 possible output categories. For each decoded trial, all other trials served as training set. 2a), the same analysis was repeated with training and testing data further divided by landmark state. Statistical analysis was then performed on a per session average likelihood (not weighted by number of trials per session). For the analysis of landmark state (Fig. 1e), trials with at least 0.5 s of data from all three states were used (16 sessions, 486 total trials) and individual trials were held out from training for decoding. Firing rates were low-pass filtered with a causal single-pole Butterworth filter at 0.05 Hz, and landmark state (0, 1 or 2) was decoded independently for each timepoint using a categorical linear decoder (dummy variable coding, (Nneurons + 1) × 3 parameters), or a neural network with no recurrence, using a single 20-unit layer receiving instantaneous firing rates, into a six unit layer and into three softmax outputs. For related analyses of hypothesis state decoding, see also Fig. 9j, where we decode form position-matched timepoints to account for location, motor and visual confounds, and Extended Data Fig. To show that mice can gain information by not encountering a landmark (as is shown, for example, by the ANN example in Fig. 2), we analyzed cases where the mouse first encounters a landmark, and then, in the LM1 state, encounters the position where another landmark could be, but fails to see one. We note that this analysis has unavoidable confounds, as in one condition the mouse gets salient visual input, in the other it does not. We consequently ignored these cases in the main analysis, and instead concentrated on cases where visual input was matched, but previous hypotheses differ (Fig. PCA was performed by first computing the covariance matrices of the low-pass filtered (as before) firing rates, and plotting their eigenvalue spectra, normalized by sum (Extended Data Fig. Each scaled eigenvalue corresponds to a proportion of explained variance. Spectra are plotted together with a control spectrum computed from covariances of randomly shuffled data. For a description of the method used to compute the correlation dimension of RSC rates (Extended Data Fig. For quantification of the independence of individual RSC neurons from the surrounding RSC population (Extended Data Fig. 8f,g), the firing rates of each neuron were predicted from those of all other neurons using linear regression. Rates were first filtered at 0.01–0.5 Hz with a third-order Butterworth filter, and subsampled to 3.3 Hz. Each neuron's rate was predicted with L1 regularized linear regression51 (\({\rm{lasso}},\lambda \approx 0.0001\)) from the rates of all other neurons and preceding firing rates using eight lags (~0.2.5 s). Predictions were computed both within condition (LM1, LM2 and dot-hunting phase), as well as across conditions, where the model was fit using coefficients determined from the other conditions. Entropies of empirical firing rate distributions were computed in bits according to their Shannon entropy, \(H\left(X\right)=-{\sum }_{i=1}^{n}P\left({x}_{i}\right){\log }_{2}P({x}_{i})\), relative to a uniform histogram of the same size, \(\hat{H}\left(X\right)=-\left(H\left(X\right)-H\left({\rm{uniform}}\right)\right)\). In cases where zeros appeared, a small offset term <<1 was added and all histograms were normalized to a sum of 1. Although the 8 × 8 grid is coarse enough to allow accurate capture of the spatial firing rate profile even for low-rate cells, the resulting estimates could be minimally affected by firing rate differences between neurons. For analysis of whether partial hypothesis representation in the LM1 state corresponds to trial-by-trial changes in firing rates, evident in bimodal firing rate histograms, histograms of hidden unit firing rates of the ANN, conditioned on binned 1D position are displayed (Extended Data Fig. Tuning curves were calculated using 20 bins of location/displacements and normalized individually for each neuron. The first timestep in each trial and timesteps with nonzero landmark input were excluded from the analysis. For histograms, each condition was binned in 100 column bins and neuron rates in ten row bins. Histograms were normalized to equal sum per column. For analysis of RSC firing rates (Extended Data Fig. 7b–d), we did not observe bimodal rate distributions and instead quantified the dispersion of the rate distributions according to their entropy: firing rates were low-pass filtered at 0.5 Hz to bring them into the timescale of navigation behavior, and firing rate histograms were computed with eight bins spanning from each neurons lowest to highest firing rate per neuron, for each spatial bin in a 4 × 4 grid. The dispersion of the firing rate distribution was then computed as average entropies per cell across all space bin, and compared across the two conditions. Firing rate profiles were analyzed in two reference frames, that is, global angle of the mouse in the arena, and relative angle to the last visible landmark. Firing rates were analyzed in a −π to π range in six bins by computing their entropy as described before. Recordings were split into LM[0,1,2] states as before, firing rates were low-pass filtered at 1 Hz, and the Pearson correlation coefficient between each pair of neurons was computed. This reordering has no impact on any further analyses. We observed no systematic change in the results as a function of the low-pass cutoff frequency, see Extended Data Fig. Neural firing rates were bandpass filtered as before, and an initial smoothing and dimensionality reduction step was performed by training a small LSTM with a single layer of 30 units to decode the mouse position. CIs were computed by treating median data from each session as independent samples. To study the encoding of context with minimal sensory and motor confounds (Fig. We then selected subsets of trials manually where egocentric paths just before the appearance of the second landmark are matched across the two groups. Figure 4a shows an example of such matched approach paths/trials. Sessions in which at least 16 trials could be matched were used for these analyses, yielding a total of 133 trials from six sessions (per session, 16, 23, 24, 24, 25 and 21). For each session, all of these trials were aligned to the time when the second landmark became visible, yielding a set of time ranges where the animals experienced similar visual inputs, performed similar locomotion behavior but potentially encoded different previous experience leading them to subsequently disambiguate the perceptually identical second landmark as ‘a' or ‘b.' To test whether there was consistent encoding of this context in RSC, we then compared the distances across these groups in 3D neural activity space (‘Low-dimensional embedding of neural activity') to distances within the groups (Fig. This test was performed at the point where the second landmark became visible to assess encoding of previous context, as well as 200 ms afterwards to assess how the identity of the (now visible) landmark affects encoding in RSC. To assess whether neural trajectories were determined by population dynamics that were stable across trials and could therefore serve as substrate for the computation performed by the mice, we tested whether neural trajectories behaved consistent with a laminar flow regime where neighboring particles (in our case, neural firing rate vectors) remain neighbors for a significant amount of time, or whether they decorrelate quickly (Fig. To assess temporal dynamics of the neural spiking without imposing any smoothing, we investigated raw spike counts in 750-ms windows for this analysis. For each session, an initial set of pairwise high-dimensional distances in spike counts between the trials with egocentrically similar paths (‘Analysis of context-encoding in RSC across similar motor and sensory states') was computed from the last 750 ms preceding the appearance of the second landmark. These distances were then correlated with those in a second sliding window; Extended Data Fig. An offset of 0 s was defined as the point where both windows stopped overlapping. The correlation coefficient R was then computed for increasing window offset up to 2 s. Summary statistics were computed across sessions by first shifting each session individually by its 95% level for R (from a shuffled control which removed the relationship between cells) which results in the summary plot showing a highest value for R of ~0.8 even for offsets where the windows fully overlap and the uncorrected R value is 1. Because of this offset, the null level for each trial is now at R = 0. To illustrate the joint encoding of position and task states (as sketched conceptually in Fig. 4f) using neural data, we decoded the hypothesis state, as well as x/y position from firing rates (Fig. Individual trials were held out as test set, an ANN was trained on the remaining trials and the resulting predictions in the test trial were plotted with hypothesis state in z and x/y in x/y dimensions. True LM0, 1a and 1b states were indicated with same colors as throughout the figure. Rates were low-pass-filtered with a causal third-order Butterworth filter at 0.5 Hz to bring rates into the behavioral timescale. This analysis was not used to make statistical statements. To further test whether neural trajectories were determined by population dynamics that were stable across trials, and were independent of the interpretation of the second (locally ambiguous) landmark, we tested whether neural activity evolved in similar directions across trials if it started close together in 3D neural activity space (‘Low-dimensional embedding of neural activity') (Extended Data Fig. For example, for an LM2a trial, we examined whether this trial might follow other close-by LM2b trials. We computed neural proximity in the 3D neural embedding (see above) and defined close-by trials as ones that were within 1 a.u. As a control, we also selected corresponding neurally furthest points. Similarity of neural evolution was then quantified as the angular difference between the trials in (3D) LSTM space over time, to assess coevolution independently of the initial selection by distance. Significance was computed by bootstrap across trials versus random alignments corresponding to a 90-degree difference. For the behavior prediction analysis, sessions with at least five correct and incorrect port visits after the second landmark visit were used (N = 11) and an equal number of hit and miss trials (outcome of next port visit is a time out or a correct) were selected, leading to a chance prediction level of 0.5. The spike rates from the 5 s preceding the second landmark becoming visible, binned into 1-s bins, were used to predict the behavioral outcome with a binary classification decision tree with a minimum leaf size of six, previously determined using cross-validation. Predictions for each trial were fit using all other trials. Train and test sets were split by trial, and decoding was performed with a regression tree on low-pass-filtered firing rates as before, performance was quantified as mean error on the number of landmarks. Decoding performance was compared between the within-class (for example, decode main task encounters with decoder trained on other trials in the main task) and cross-class (for example, decode dot-hunting from decoder trained on the main task, and so on). To test whether the encoding of hypothesis states in RSC is specific to task performance, we analyzed a larger number of sessions from the entire period during which two landmarks with local visibility were used (92 recording sessions in total) (Extended Data Fig. We analyzed the effect of task performance on the behavior prediction analysis (as described above; Extended Data Fig. We also analyzed the more general decoding of landmark encounter count (same method as in ‘Specificity of landmark encounter coding to the foraging task'; Fig. 1) in all of the 92 sessions with two landmarks, and correlated decoding performance with task performance on a per session level. For all of these analyses, we used an analogous method as for the nonbehavior-correlated analyses. For details of the calculation of the correlation dimension for RSC data, see ‘Correlation dimension.' We chose a simple recurrent neural network as one of the simplest architectures that can learn to maintain state over time. Unless stated in the text, the default architecture consisted of rate neurons with an input layer into 128 hidden recurrent units (tanh nonlinearity) into 80 output neurons, trained on random velocity trajectories in random environments of up to four landmarks (see ‘Network architecture and training' for details). For the analyses in the main text, landmark inputs were relayed to the ANN as a map that encoded their relative position but not identity (‘external map' ANN, 80 input neurons). The findings were replicated with an ANN that received only binary landmark presence input (‘internal map' ANN, 11 input neurons) and non-negative ANNs (Extended Data Fig. A simulated animal runs with varying velocity in a circular environment starting from a random unknown position and eventually infers its position using noisy velocity information and two, three or four indistinguishable landmarks. A trial consists of a fixed duration of exploration in a fixed environment, starting from an unknown starting location; the environment can change between trials. Environments are generated by randomly drawing a constellation of two to four landmarks, and the network must generalizably localize in any of these environments when supplied with its map. In the internal map scheme (Extended Data Fig. To make the task tractable, we limit training and testing in the internal map setting to four specific environments. 2 and 5–7), landmark locations were random and the set of locations (map) were provided to the network, whereas in the internal map task (Extended Data Fig. 10a–m) one of four landmark configurations was used, but the maps were not provided to the network. Landmarks could be observed only for a short distance. Velocity and landmark encounter information were encoded in the input layer, and all weights of the network were trained. Network performance was compared with a number of alternative algorithms: path integration plus correction integrated the noisy velocity information starting from an initial location guess and corrected this estimate by a reset to the coordinates of the nearest landmark when a landmark was encountered. Particle filters approximated sequential Bayesian inference given the available velocity and landmark information, with each particle capturing a location hypothesis whose posterior probability is given by an associated weight. The enhanced particle filter also reweights particles when a landmark is expected but not encountered, thus can infer location not only from the presence but also from the absence of landmarks. The task is defined by a simulated animal moving along a circular track of radius 0.5 m for 10 s. The animal starts at a random, unknown position along the circle at rest and starts running along a trajectory at nonconstant velocity. A trajectory is sampled every dt = 0.1 s in the following way: at each time t, acceleration at is sampled from a zero-mean Gaussian distribution with s.d. Acceleration is integrated to obtain the velocity vt and truncated if |vt | > vmax = π/2 m s−1. In a trial of the external map task, the locations of K = 2, 3 or 4 indistinguishable landmarks were determined sequentially: the first landmark was sampled from a uniform random distribution on the circle, with subsequent landmarks also sampled from a uniform random distribution but subject to the condition that the minimum angular distance from any previously sampled landmark is at least δ = π/9 rad. The internal map task involved four environments, each with a unique configuration of landmarks: two environments had two landmarks, one had three and the last had four. Landmark locations in the four environments were chosen so that pairwise angular distances were sufficiently unique to allow the inference of environment identity. After training, the networks were evaluated in different testing configurations that each consisted of a distribution over landmark configurations and trajectories: Training distribution: this test set was generated exactly as in the training set, as described in ‘Definition of environments and trajectories'. Fixed landmarks, random trajectories: the landmark configuration was given by two landmarks located at e = {0, 2π/3}, the trajectories were sampled in an identical way as in the training distribution. Note that this landmark configuration corresponds to the first environment in the internal map task. Fixed landmarks, constant velocity trajectories: the landmark configuration was given by two landmarks located at e = {0, 2π/3} and the trajectories were given by constant velocity trajectories with |vt | = vmax/2. Two variable landmarks, constant velocity trajectory: the landmark configuration was given by two landmarks located at e = {0, 2π/3 + απ/3}, where α ϵ [0, 1]. Two environments, random trajectories: the landmark configuration was given by either e1 or e2 of the internal map task, trajectories are random. The animal is considered to have encountered a landmark if it approached within dmin = vmax × dt/2 = π/40 m−2 = π/20 rad. This threshold is large enough to prevent an animal from ‘missing' a landmark even if it is running at maximum velocity. This ‘visibility radius' is smaller than the one we used for the mouse behavior experiments (Fig. Also, only trials in which the animal encountered at least two different landmarks were included. At each timestep of size dt = 0.1, the velocity input to the network corresponded to the true displacement vdt corrupted by zero-mean Gaussian noise of standard deviation σv = vmaxdt/10. In the external map task, the landmark map provided to the network and particle filter was corrupted by zero-mean Gaussian noise with standard deviation σl = π/50 rad, without changing the relative landmark positions: The map was coherently slightly rotated at a landmark encounter, and the rotation was sampled independently at each landmark encounter. First, location tuning curves were determined after the second landmark encounter using 5,000 trials from distribution 1 and using 50 location bins. Tuning curves were calculated separately for each of the four environment of the internal map task. Preferred location was determined to be the location corresponding to the tuning curve maximum. The density of preferred locations smaller than distance dmin away from a landmark was then compared with the density of preferred locations further away from landmarks. The network consisted of three layers of rate neurons with input-to-hidden, hidden-to-hidden and hidden-to-output weights. The remaining neurons (70 in the external map case and 1 in the internal map case) coded for landmark input and were activated only at the timestep of, and up to, three timesteps after a landmark encounter. The LM neurons had von Mises tuning with preferred locations xj = (j − 1) × 2π/70 rad, j = 1…70, that tiled the circle equally. Given n landmarks at locations li, i = 1…n, the firing rate of the j-th landmark input neuron was given by This mixture of von Mises activation hills produces the pattern depicted as the ‘map' input in Extended Data Fig. In the internal map case (Extended Data Fig. 10a–m), the landmark input neuron consisted of a single binary neuron that responded for four timesteps with activation 1 in arbitrary units whenever a landmark was encountered. This input encoded neither environment identity nor landmark location. The hidden layer consisted of 128 recurrently connected neurons. The activation ht of hidden layer neurons at timestep t was determined by ht = tanh(Wxxt + Whht − 1 + b), where xt are the activations of input neurons at timestep t, Wx are the input-to-hidden weights, Wh are the hidden-to-hidden weights and b are the biases of hidden neurons. The nonlinearity should be considered as an effective nonlinearity at long times; since the timestep dt = 0.1 s was large compared with a typical membrane time constant (τ ≈ 0.02 s), we did not include an explicit leak term. In the non-negative network (Extended Data Fig. 10n–t), the recurrent activation was determined by ht = tanh([Wxxt + Whht−1 + b]+), where [u]+ denotes rectification. The output layer consisted of a population of 70 neurons with activity ot given by ot = tanh(Woht + bo), where Wo are the output weights and bo the biases of the output neurons. where zα, α = 1…70 are the equally spaced preferred locations of each training target. The network was trained by stochastic gradient descent using the Adam algorithm55, to minimize the average square error between output ot and training targets \({\tilde{{o}}_{t}}\), with the average taken over neurons, time within each trial and trials. The training set consisted of 106 independently generated trials. During training, performance was monitored on a validation set of 1,000 independent trials and network parameters with the smallest validation error were selected. All results were cross-validated on a separate set of test trials to ensure that the network generalized across new random trajectories and/or landmark configurations. To ensure a fair comparison, we make sure that each alternative algorithm has access to exactly the same information as the network: the landmark identities are indistinguishable and both velocity and landmark location information are corrupted by the same small amount of sensory noise. This algorithm implements path integration and landmark correction using a single location estimate, similar to what is implemented in hand-designed continuous attractor networks that implement resets at boundaries or other landmarks15,16,56,57. The algorithm starts with an initial location estimate at y = 0 (despite the true initial location being random and unknown), and integrates the noise-corrupted velocity signal to obtain location. Particle filters implement approximate sequential Bayesian inference using a sampling-based representation of the posterior distribution. Here, the posterior distribution over location at each timepoint is represented using a cloud of weighted particles, each of which encodes through its weights a belief, or estimated probability, of being at a certain location. In the prediction step, particles are propagated independently using a random walk whose mean is the noise-corrupted velocity update and whose s.d. In the absence of a landmark encounter, particle weights remain unchanged and the particle cloud diffuses. If a landmark is encountered, the importance weights wt,β of particles β = 1…Np are multiplied by If the effective number of particles becomes too small, that is, \({N}_{\text{eff}}=1/{\sum }_{\beta }{w}_{t,\beta }^{2} < {N}_{p}/5\), the particles are resampled using low variance sampling58 and the weights equalized. This resampling step both allows for better coverage of probabilities and permits the particle cloud to sharpen again. The particle filter estimate at a given timepoint is given by the weighted circular mean \({\hat{y}}_{t}={{\arg }}({\sum }_{\beta }{w}_{t,\beta }\exp (i{y}_{t,\beta }))\) of the particle locations. This particle filter has identical initialization, prediction step and weight update at landmark encounters as the basic particle filter and proceeds in exactly the same way until the first landmark encounter. Subsequently, the enhanced particle filter can also use the absence of expected landmark encounters to narrow down its location posterior, similar to the network's ability shown in Extended Data Fig. This is implemented in the following way: if a particle comes within the observation threshold δ of a possible landmark location but no landmark encounter occurs, the particle is deleted by setting its weight to zero; afterwards the particle weights are renormalized. A complication to this implementation is that a subsequent landmark encounter only occurs if the current landmark is different than the previous one (‘landmark encounters'); to prevent the deletion of particles that correctly report a landmark at the current position but do not receive a landmark encounter signal because it is the same landmark as previously encountered, particles are deleted only if they come within the observation threshold δ to a possible landmark that is different than the last landmark and do not encounter it. In case all particles have been deleted, particles are resampled from a uniform distribution and their weights are equalized. As for the basic particle filter, particles are resampled whenever the effective number of particles becomes too small \({N}_{\text{eff}}=1/{\sum }_{\beta }{w}_{t,\beta }^{2} < {N}_{p}/5\). The timing and accuracy of location disambiguation in Extended Data Fig. If this network estimate was closer to the true than to the wrong landmark location the trial was categorized as a correct trial, otherwise it was categorized as an incorrect trial. We performed PCA on the hidden neuron states from training trials to obtain the top three principal directions. We then projected network states obtained from the distribution of testing trials 2 or 3 (Supplementary Information) onto these principal directions. The resulting reduced-dimension versions of the hidden neuron states from testing trials are shown in Fig. To calculate the correlation dimension for the ANN and RSC activity, we first performed linear dimensionality reduction (PCA) on hidden layer activations from the training trials, retaining 20 principal components. For RSC data, rates were first low-pass filtered at 0.5 Hz. In the 20-dimensional space, we randomly picked 1,000 base points (500 for RSC). From each of these base points, we estimated how the number of neighbors in a ball of radius R scales with R. The minimum ball radius was determined such that the logarithm of the number of neighbors averaged over base points was near 1. The maximum radius was set to ten times the minimum radius, and intermediate values for the radius were spaced equally on a log scale. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. The experimental data of this study are available via Figshare at https://doi.org/10.6084/m9.figshare.27890997 (ref. Code for training ANNs is available at: https://github.com/jvoigts/Voigts_Kanitscheider_et_al_2024. Vyas, S., Golub, M. D., Sussillo, D. & Shenoy, K. V. Computation through neural population dynamics. Sarafyazd, M. & Jazayeri, M. Hierarchical reasoning by neural circuits in the frontal cortex. Mante, V., Sussillo, D., Shenoy, K. V. & Newsome, W. T. Context-dependent computation by recurrent dynamics in prefrontal cortex. Smith, R. C. & Cheeseman, P. On the representation and estimation of spatial uncertainty. & Sharp, P. E. Head direction, place, and movement correlates for cells in the rat retrosplenial cortex. Alexander, A. S. & Nitz, D. A. Retrosplenial cortex maps the conjunction of internal and external spaces. & Harnett, M. T. Somatic and dendritic encoding of spatial variables in retrosplenial cortex differs during 2D navigation. Mao, D., Kandler, S., McNaughton, B. L. & Bonin, V. Sparse orthogonal population representation of spatial context in the retrosplenial cortex. Hattori, R., Danskin, B., Babic, Z., Mlynaryk, N. & Komiyama, T. Area-specificity and plasticity of history-dependent value coding during learning. Murakami, T., Yoshida, T., Matsui, T. & Ohki, K. Wide-field Ca2+ imaging reveals visually evoked activity in the retrosplenial area. Fischer, L. F., Mojica Soto-Albors, R., Buck, F. & Harnett, M. T. Representation of visual landmarks in retrosplenial cortex. & Harnett, M. T. An easy-to-assemble, robust, and lightweight drive implant for chronic tetrode recordings in freely moving animals. & Fiete, I. R. Accurate path integration in continuous attractor network models of grid cells. & McNaughton, B. L. Path integration and cognitive mapping in a continuous attractor neural network model. & Fiete, I. R. A model of grid cell development through spatial exploration and spike time-dependent plasticity. Hardcastle, K., Ganguli, S. & Giocomo, L. M. Environmental boundaries as an error correction mechanism for grid cells. & Moser, E. I. Accumulation of hippocampal place fields at the goal location in an annular watermaze task. Lee, I., Griffin, A. L., Zilli, E. A., Eichenbaum, H. & Hasselmo, M. E. Gradual translocation of spatial correlates of neuronal firing in the hippocampus toward prospective reward locations. Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. & Jazayeri, M. Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics. Finkelstein, A. et al. Attractor dynamics gate cortical information flow during decision-making. Rule Encoding in Orbitofrontal Cortex and Striatum Guides Selection. Shared mechanisms underlie the control of working memory and attention. B. et al. Fronto-parietal cortical circuits encode accumulated evidence with a diversity of timescales. Harvey, C. D., Coen, P. & Tank, D. W. Choice-specific sequences in parietal cortex during a virtual-navigation decision task. Odoemene, O., Nguyen, H. & Churchland, A. K. Visual evidence accumulation behavior in unrestrained mice. Xue, C., Kramer, L. E. & Cohen, M. R. Dynamic task-belief is an integral part of decision-making. Latent learning drives sleep-dependent plasticity in distinct CA1 subpopulations. Specific evidence of low-dimensional continuous attractor dynamics in grid cells. Gardner, R. J. et al. Toroidal topology of population activity in grid cells. McKenzie, S. et al. Preexisting hippocampal network dynamics constrain optogenetically induced place fields. Inagaki, H. K., Fontolan, L., Romani, S. & Svoboda, K. Discrete attractor dynamics underlying selective persistent activity in frontal cortex. Uria, B. et al. A model of egocentric to allocentric understanding in mammalian brains. Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. A. et al. Cortical population activity within a preserved neural manifold underlies multiple motor behaviors. Ma, W. J., Beck, J. M., Latham, P. E. & Pouget, A. Bayesian inference with probabilistic population codes. Echeveste, R., Aitchison, L., Hennequin, G. & Lengyel, M. Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference. Lu, K., Grover, A., Abbeel, P. & Mordatch, I. Pretrained transformers as universal computation engines. Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural networks. The importance of mixed selectivity in complex cognitive tasks. Fusi, S., Miller, E. K. & Rigotti, M. Why neurons mix: high dimensionality for higher cognition. Newman, J. P. et al. Twister3: a simple and fast microwire twister. Siegle, J. H. et al. Open Ephys: an open-source, plugin-based platform for multichannel electrophysiology. & Newman, J. P. et al. ONIX: a unified open-source platform for multimodal neural recording and perturbation during naturalistic behavior. Lopes, G. et al. Bonsai: an event-based framework for processing and controlling data streams. Chung, J. E. et al. A fully automated approach to spike sorting. Tibshirani, R. Regression shrinkage and selection via the lasso. & Botstein, D. Cluster analysis and display of genome-wide expression patterns. B., de Silva, V. & Langford, J. C. A global geometric framework for nonlinear dimensionality reduction. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. & Fiete, I. R. Inferring circuit mechanisms from sparse neural recording and global perturbation in grid cells. & Fiete, I. R. Grid cells: the position code, neural network models of activity, and the problem of learning. Voigts, J. Spatial reasoning via recurrent neural dynamics in mouse retrosplenial cortex. Speed cells in the medial entorhinal cortex. & Burgess, N. Grid cells and theta as oscillatory interference: electrophysiological data from freely moving rats. Fiser, J., Berkes, P., Orbán, G. & Lengyel, M. Statistically optimal perception and learning: from behavior to neural representations. Zhang, K. Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. Tsodyks, M. & Sejnowski, T. Associative memory and hippocampal place cells. Meshulam, L., Gauthier, J. L., Brody, C. D., Tank, D. W. & Bialek, W. Collective behavior of place and non-place neurons in the hippocampal network. Gothard, K. M., Skaggs, W. E. & McNaughton, B. L. Dynamics of mismatch correction in the hippocampal ensemble code for space: interaction between path integration and environmental cues. We thank E. J. Dennis, M. Jazayeri, K. Stachenfeld and E. Issa for comments on the manuscript. This work was supported by the NIH 1K99NS118112-01 and a Simons Center for the Social Brain at MIT postdoctoral fellowship (J.V. and the Center for Brains, Minds and Machines (CBMM) at MIT, funded by NSF STC award CCF-1231216 (J.P.N. is a Paul and Daisy Soros Fellow. is an HHMI Faculty Scholar and this work was partially supported by awards to I.R.F. This research was partially funded by the Howard Hughes Medical Institute at the Janelia Research Campus. Department of Brain and Cognitive Sciences, MIT, Cambridge, MA, USA Jakob Voigts, Nicholas J. Miller, Enrique H. S. Toloza, Jonathan P. Newman, Ila R. Fiete & Mark T. Harnett McGovern Institute for Brain Research, MIT, Cambridge, MA, USA Jakob Voigts, Nicholas J. Miller, Enrique H. S. Toloza, Ila R. Fiete & Mark T. Harnett HHMI Janelia Research Campus, Ashburn, VA, USA Department of Physics, MIT, Cambridge, MA, USA Picower Institute for Learning and Memory, MIT, Cambridge, MA, USA You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar wrote the paper with input from all authors. Correspondence to Jakob Voigts, Ila R. Fiete or Mark T. Harnett. Nature Neuroscience thanks Guifen Chen and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. (a) Schematic of task structure and timing. (b) Example trial schematic showing all possible task states (see Methods for more details). Landmarks were formed by white dots displayed on a screen which served as the floor of the arena. They were only made visible when mice crossed a distance threshold. Only one landmark was visible at a time in the final training stage. Nose-pokes were only registered after mice held their nose in the port for a randomly chosen delay period that was randomized for each visit and not known to the mouse. Incorrect port visits resulted in timeouts that were associated with a bright background across the entire arena. After each complete trial, which results in the reward state, mice are required to complete a separate task in which they need to ‘hunt' for a series of 4 to 8 randomly placed blinking dots on the arena floor. Each dot disappears as soon as the mouse reaches it, resulting either in a new random annulus, or initialization of the next trial. The next trial begins with a new random rotation of the landmarks and rewarded port. Mice are trained with a single landmark first, then 2 landmarks at unlimited view distance, and finally a limited view distance. (d) Top: Experimental setup for electrophysiology and real-time mouse position tracking. The arena was placed on top of a commercial flat-screen TV that was used to display visual landmarks. (e) Example excerpt of behavioral data, with state transitions. Landmark visits (black arrowheads) are defined as the point when new landmarks become visible. (f) Top: Training curves for all 4 mice. The three major training phases are indicated with shading (corresponding to panel c). Red: Proportion of time that a landmark is visible (remains 1.0 (100%) until view distance is introduced). Blue: Maximum reward port hold time for each session, the actual hold times are drawn from a uniform distribution. Black: proportion of hits / false positives (corresponds to rewards / timeouts, or proportion correct), for the 1st port visit in each trial. Values over 1/16 indicate that mice can distinguish the correct port amongst all ports. Values over 1 indicate that mice could reliably visit the correct port among the two ports indicated by locally ambiguous landmarks without excluding any other ports by trial and error (see main text and methods). Trials with 1st landmark visit after <20 sec are included in analysis. Grey: Proportion of trials in which mice see both landmarks, and then turn around to go back to the 1st landmark. If this proportion was 0, it would indicate that mice always visit the 2nd port after seeing it, which would on average lead to chance-level behavioral performance. For each individual session, significance of correct choice for the 1st port visit among the two indicated ports was tested with a binomial fit at the 95% level (two-sided, Clopper-Pearson exact method) and is indicated with a star. If the mouse also visited a large proportion of unmarked ports, this fraction can be significant despite the overall correct rate among all 16 ports being small. Bottom: latency to reward after encountering the 2nd landmark in seconds (blue), proportion of visits to ‘a' and ‘b' as fraction of all port visits (orange) and proportion correct choice between ‘a' or ‘b' with binomial 95% CI (binomial as described before, green). (g) All paths taken by the mouse in one example session, split by LM0,1,2 state (green, glue, grey). (i) Retrosplenial cortex is required for integrating egocentric sensory information and hypotheses about the animal's allocentric location, but not for visually guided navigation. To causally test the role of RSC in relating spatial hypotheses to sensory data, we used a parametric allocentric/egocentric task using the same apparatus as in the main experiment and pharmacologically inactivated RSC. Schematic of task structure: Water restricted mice had to visit the port closest to a single visual landmark for a water reward. Visits to any other port resulted in a time-out, but allowed the mice to self-correct. As in the main experiment, the landmark and rewarded port were rotated randomly after each trial, forcing mice to use only the visual landmark. These strategies all require integration of self-location hypotheses with visual landmark information. Trials with high eccentricity (right) required merely walking to the port closest to the landmark. This design allowed us to test the role of RSC in the integration of location hypotheses with egocentric visual landmark information while simultaneously determining whether simpler visually-guided navigation was also affected. (k) RSC was either 1) transiently inactivated with Muscimol, 2) sham injected with cortex buffer, or 3) not injected (see methods). Each mouse was tested in both groups, with balanced ordering. (l) Task performance (mean and 95% confidence intervals for hit rate on 1st port visits per trial, via binomial bootstrap). Mice always performed above full chance level (1/16th, assuming they cannot make use of the landmark). Performance was selectively reduced by RSC inactivation for low eccentricity conditions where integration of location hypotheses and visual landmarks was required. Performance in the visually guided condition was only minimally affected. (a) Location decoders (neural network, cross-validated per trial) do not generalize across landmark states, and LM1 carries less spatial information than LM2. Performance is measured by prediction likelihood in a 10×10 grid, means and shaded 95% CIs across sessions (N = 16 sessions). (b) Left: Example ANN neuron tuning curves (from LM2) split by travel direction, speed, or location uncertainty (corresponding to LM0,1,2 states, derived from particle filter), showing conjunctive coding. Right: Three RSC example cells showing conjunctive coding of location vs. speed, and direction (Fig. (c) Left: ANN neurons and, Right: RSC cells (N = 984 neurons) weakly preferentially fire at landmark locations. Top: distribution of locations where RSC cells fire most. Bottom: total average rates, split by LM1 and LM2. (d) Distribution of firing rates by angular position in the arena, same data as panel c. Blue: quantile of firing rates across population. Despite a small preference for the landmark locations, this effect is small compared to the overall variability on firing rates, and there is no systematic preference for cells to fire in proximity of one vs. the other landmark, even in the LM2 condition. (e) Information gain, which we study by analyzing landmark encounters, as transitions between LM0,1,2 states throughout the manuscript, can also occur when mice fail to encounter a landmark where one would be expected given some hypothesis (see Fig. These cases can also be decoded from neural activity, but cannot be directly compared to landmark encounters, as they don't offer the matched sensory input (that is no visual input vs. appearance of salient landmark) that we employ in Fig. 4 (mouse encounters 2nd landmark, but it is either ‘a' or ‘b'). These ‘virtual' landmark encounters were decoded with a cross-validated NN on a trial level and compared to real landmark encounters. (a) Tetrode drive12 implants targeting mouse retrosplenial cortex (RSC). (b) Example band-passed (100Hz-5kHz) raw voltage traces from 16 tetrodes. (d) Histograms of mean firing rates of all 984 neurons across LM0 (green), LM1 (blue), and LM2 (black) conditions. Overall rates did not shift significantly across these states. (e) Relative per-neuron changes in firing rates across conditions. Despite the lack of a population-wide shift in average rates, the firing rates of individual cells varied significantly across conditions with heterogeneous patterns of rates. Each grouping shows rates per cell, relative to the rate in LM0 (left) LM1 (middle), and LM2 (right) as individual rates (grey lines and histograms). (f) Spatial firing profiles of 42 example neurons split by hypothesis state. For clarity, missing data is that was not due to exclusion via landmark visibility in LM1 is plotted as the darkest color in each plot. (g) Spatial firing rate profiles for all neurons from one example session (52 total), from the main task phase. Profiles were computed in 25×25 bins, and individually normalized to their 99th percentile. (h) same as panel g, but from the separate trial initialization task (‘dot-hunting') in which mice had to hunt for a series of blinking dots that appeared in random positions. (i) 36 example neurons from multiple sessions and animals, chosen to represent the broad range of tuning profiles. In the dot-hunting task there is no conserved radial tuning due to the absence of consistent landmarks, however some cells retain angular spatial tuning due to olfactory cues in the arena. Tuning to eccentricity (distance to arena wall or center) is maintained across task phases in many neurons. Small numbers indicate maximum firing rates in Hz for each plot (color scale is same across the pairs). (a) Foraging and dot-hunting tasks are interleaved, allowing comparisons of how the same neural population represents hypotheses. (b) We predict the number of encountered landmarks either within condition (for example foraging from foraging, each time using one trial as test, fitting to all others), or across. Train and test sets were split by trial. Decoding was done with a regression tree on low-pass filtered firing rates. Performance was quantified as mean error on the N of landmarks. (d) Summary stats from all sessions, means and bootstrapped CIs. The prediction is significantly better when using training data from the same category than when using the neural code from the other; for example dot-hunting to predict the foraging (P= ~0 / ~ 0 within vs. across categories for predicting dot-hunting and foraging landmark state), showing that hypothesis coding is task-specific. (e) To test whether hypothesis encoding is a specific function of task learning or a general feature of RSC, we examined whether coding persisted in case when mice performed the task but were not yet performing well. We first examined the ability to predict correct vs. incorrect port choice (same as in Fig. 4) as a function of per-session task performance. We analyzed data from sessions from the entire training period where the 2 landmarks were used, with at least 5 correct and 5 incorrect choices (N = 42 sessions), due to the closely spaced recordings, neurons might be re-recorded across sessions. On average we analyzed ~15-30 port visits per session (number of trials was unaffected by behavioral performance: CI of slope = [-7.7, 2.7], p = 0.33). Predictions were made as before with a test/train split on balanced hit/miss data with a regression tree. Prediction performance was at chance level ( ~ 47%, P = 0.81 vs. chance) for low performance sessions (total correct choice ratio of 0.8 or lower), and the same as in our initial analysis (Fig. 4) for sessions with high mouse performance ( ~ 66%, P = 0.00096 vs. chance). Individual mice are indicated with colored markers. (f) We also analyzed the more general decoding of landmark encounter count (same as Fig. 1) in all of the 92 sessions with 2 landmarks, and also found a significant correlation (p = 0.0045 vs. constant model), showing that hypothesis encoding throughout the task is driven by task learning. (g) As a control experiment, we tested whether decoding the number of landmarks encountered in the interleaved dot-hunting task might also be affected by task performance, if for instance the neural encoding and performance was a function of general spatial learning, habituation to the arena, motivation, etc., and we found this correlation to be flat (P = .6, CI for slope = [-0.17, 0.29]). Input neurons encoded noisy velocity input with linear tuning curves (similar to speed cells in the entorhinal cortex60), and landmark information. If there are K landmarks (all assumed to be perceptually indistinguishable), then whenever the animal encounters a landmark, the input provides a simultaneous encoding of all K landmark locations using spatially tuned input cells. This input can be thought of as originating from a distinct brain area that identifies the current environment and provides the network with its map. (c) Activity of output neurons ordered by preferred location as a function of time in an easy trial with two nearby landmarks and a constant velocity trajectory. The network's decision on when to collapse its estimate is flexible, and dynamically adapts the decision time to task difficulty: When the task is harder because of the configuration of landmarks (the task becomes harder as the two landmarks approach a 180 degree separation because of velocity noise and the resulting imprecision in estimating distances; the task is impossible at 180 degree because of symmetry), the network keeps alive multiple hypotheses about its states across more landmark encounters until it is able to reach an accurate decision. Panels c,d,f,g show example trials from experiment configuration 4 (See Methods) with different values of landmark separation parametrized by α. (d) Same as c, but in a difficult trial with two landmarks almost opposite of each other. (e) Top: The ANN took longer to disambiguate its location in harder task configurations: average time until disambiguation as a function of landmark separation (Standard error bars are narrower than line width). Data from 10000 trials in experiment configuration 4, 1000 for each of the 10 equally spaced values of α. 2 main text) can be compared to the much poorer performance achieved by a strategy of path integration to update a single location estimate with landmark-based resets (to the coordinates of the landmark that is nearest the current path-integrated estimate), Fig. The latter strategy is equivalent to existing continuous attractor integration models13,14 combined with a landmark- or border based resetting mechanism16,56,57,61, which to our knowledge is as far as models of brain localization circuits have gone in combining internal velocity-based estimates with external spatial cues. The present network goes beyond a simple resetting strategy, matching the performance of a sequential probabilistic estimator – the particle filter (PF) – which updates samples from a multi-peaked probability distribution over possible locations over time and is asymptotically Bayes-optimal (M = 1000 particles versus N = 128 neurons in network; Fig. Notably, the network matches PF performance without using stochastic or sampling-based representations, which have been proposed as possible neural mechanisms for probabilistic computation39,62. (f) Similar to c, but in a trial where the network disambiguates its location before the second landmark encounter. Yellow arrows mark times of landmark interactions if the alternative location hypothesis had been correct. Disambiguation occurs shortly after the absence of a landmark encounter at the first yellow arrow. (g) Similar to f, but in a trial where disambiguation occurs at the first landmark location, since no landmark has been encountered at the time denoted by the first gray arrow. (j) If the ANN is instead given the landmark identity via separate input channels, it immediately identifies the correct location after the 1st landmark encounter and learns to acts as a simple path integration attractor without hypothesis states. Plots show ANN output as in c,d,g,f. (i,k) To quantify the separation of hypothesis states in the ANNs hidden states even in cases where such separation might not be evident in a PCA projection, we linearly projected hidden state activations onto the axis that separates the hypothesis states. The regular ANN shows a clear LM1 vs LM2 separation, but the ANN trained with landmark identity does not distinguish between these. (l) Population statistics for ANN with external map input. Scatter plot of enhanced particle filter (ePF) circular variance vs. estimate decoded from hidden layer of the network. 4000 trials from experiment configuration 1 were used to train a linear decoder on the posterior circular variance of the ePF from the activity of the hidden units and performance was evaluated on 1000 test trials. (m) Scatter plot of widths and heights of ANN tuning curves after the 2nd landmark encounter. Insets: example tuning curves corresponding to red dots. Unlike hand-designed continuous attractor networks, where neurons typically display homogeneous tuning across cells13,63,64, our model reproduces the heterogeneity observed in hippocampus and associated cortical areas. (n) The distribution of recurrent weights shows that groups of neurons with strong or weak location tuning or selectivity have similar patterns and strengths of connectivity within and between groups: distribution of absolute connection strength between and across location-sensitive “place cells” (PCs) and location-insensitive “unselective cells” (UCs) in the ANN. The result is consistent with data suggesting that place cells and non-place cells do not form distinct sub-networks, but are part of a system that collectively encodes more than just place information65. Location tuning curves were determined after the second landmark encounter using 5000 trials from distribution 1 and using 20 location bins. Neurons were split in two equal sets according to their location entropy, where neurons with low entropy were defined as “place cells” (PCs) and neurons with high entropy were defined as “non-place cells” (UCs). Between and across PCs and UCs absolute connection strength was calculated as the absolute value of the recurrent weight between non-identical pairs. (o) Pairwise correlation structure30 is maintained across LM[1,2] states and environments. The neurons are ordered according to their preferred locations in environment 1. Bottom: Example tuning curve pairs (normalized amplitude) corresponding to the indicated locations i-iv. (p) State-space activity of ANN is approximately 3-dimensional. Even when summed across all environments and random trajectories, the states still occupy a very low-dimensional subspace of the full state space, quantified by the correlation dimension as d ≈ 3 (left, see Methods). This measure typically overestimates manifold dimension66, and serves as an upper bound on the true manifold dimension. Data from 5000 trial, recurrent weights were sampled i.i.d. from a uniform distribution Wh,ij ~ U([ − 1, 1]), then fixed across trials. The initial hidden state across trials was sampled from ht=0,i ~ U([ − 1, 1]). Color corresponds to true location (plot shows 100 trials). (r) ANN with external map input implements a circular attractor structure: Hidden layer activity arranged by preferred location in an example trial shows a bump of activity that moves coherently. Preferred location was determined after the second landmark encounter using 5000 trials from experiment configuration 1. (s) Left: Recurrent weight matrix arranged by preferred location of neurons (determined after the second landmark encounter using 5000 trials from experiment configuration 1) indicates no apparent ring structure, despite apparent bump of activity that moves with velocity inputs (panel a). Connections between appropriate neural mixtures in the hidden layer – defined by the output projection of the neurons – therefore exhibit a circulant structure, but the actual recurrent weights do not, even after sorting neurons according to their preferred locations. The ANN thus implements a generalization of hand-wired attractor networks, in which the integration of velocity inputs by the recurrent weights occurs in a basis shuffled by an arbitrary linear transformation. Given these results, one cannot expect a connectomic reconstruction of a recurrent circuit to display an ordered matrix structure even when the dynamics are low-dimensional, without considering the output projection. Because trials in the mouse experiments typically ended almost immediately when the mouse had seen both landmarks (See Extended Data Fig. 1f for a quantification), we did not quantify the topology of the neural dynamics in RSC. (t) Low-dimensional state-space dynamics in the ANN with external map input suggests novel form of probabilistic encoding. ANN hidden layer activity was low-dimensional: Fig. 3a shows data on low-dimensional dynamics, evident in maintained pairwise correlations, and Fig. Trajectories are shown from the beginning of the trials; arrows indicate landmark encounter locations, black squares: first landmark encounter; black circles: second landmark encounter; line colors denote trajectory stage: LM0 (green), LM1 (blue), andLM2 (grey). Data in a-c is from 1000 trials from experiment configuration 3 (see Methods); sensory noise was set to zero. Trajectory starting points were selected to be a fixed distance before the first landmark. In other words, the network internally encodes single-location hypothesis states separably from multi-location hypothesis states, as we find in RSC (Fig. (u) ANN trial trajectory examples, (corresponding to Fig. Divergence of trajectories for two paths that are idiothetically identical until after the second landmark encounter. ‘a' and ‘b' denote identities of locally ambiguous identical landmarks. Disambiguation occurs at the second landmark encounter, or by encountering locations where a landmark would be expected in the opposite identity assignments. See insets for geometry of trajectories and landmark locations. LM2 state has been simplified in these plots. (v) All four trajectories from panel b plotted simultaneously, and with full corresponding LM2 state. (w) The low-dimensional state-space manifold is stable, attracting perturbed states back to it, which suggests that the network dynamics follow a low-dimensional continuous attractor and the network's computations are robust to most types of noise. Relaxations in state space after perturbations before the first (left), between first and second (middle), and after the second (right) landmark encounter. The first and second landmark encounter in this base trial is at time t = 2 s and t = 4.6 s respectively. At time t = 1 s (left), t = 4 s (middle), and t = 7 s (right) a multiplicative perturbation of size 50% was introduced at the hidden layer. 10l for same result on internal map ANNs. (a) Top: Tuning curves (mean rate) for displacement from last encountered landmark for LM1 and LM2 states in ANN. Bottom: Same data, but distribution of firing rates. The network discovers that displacement from the last landmark encounter in the LM1 period is a key latent variable, and its encoding is an emergent property. Intriguingly, a similar displacement-to-location coding switch has been observed in mouse CA167, suggesting that the empirically observed switch may be related to the brain performing spatial reasoning to disambiguate between multiple location hypotheses. (b) Same as panel a but for global location, ANN neurons became more tuned to global location rather than landmark-relative information after encountering the 2nd landmark. (c) Decoding of location, displacement, and separation between landmarks from the ANN in a 2-landmark environment by a linear decoder that remains fixed across trials and environments. As suggested by the well-tuned activity of ANN neurons, location can be linearly decoded in the LM2 state. Displacement can be best decoded in the LM1 state. Top: Performance was evaluated on 1000 trials from experiment configuration 2. For displacement, the linear decoder was trained on 4000 separate trials. Bottom: experiment configuration 1 with 4000 trials to train the linear decoder and 1000 trials to evaluate it. Thus, the network's encoding of these three critical variables is dynamic and tied to the different computational imperatives at each stage: displacement and landmark separation are not explicit inputs but the network estimates these and represents them in a decodable way at LM1, the critical time when this information is essential to the computation. After LM2, the network decodability of landmark separation drops, as it is no longer essential. (d) Neurons in RSC also became less well tuned to relative displacements from landmarks in LM2 relative to LM1: histogram across all RSC neurons of entropy of tuning curve for angular displacement from last seen landmark in RSC. For this analysis, angular firing rate distributions were analyzed relative to either the global reference frame or the last seen landmark. (e) Same as d, but for global location. (f) The absolute change in landmark-relative displacement coding (d) is larger than that of the allocentric location tuning (e), suggesting that the latter is less affected by task state. (a) Bottom: Mean spatial activity profile of 2 example ANN neurons for LM1 and LM2. Average tuning is higher for the LM2 state. Top: same data as histograms, showing that the less well-tuned LM1 state corresponds to a bimodal rate distribution (rates are high in some trials, low in others) that transitions to a unimodal distribution once the 2nd landmark has been identified in LM2. Data are from experiment configuration 2 (See Methods, section ‘Overview over experiment configurations used with ANNs'). Tuning curves were calculated using 20 bins of location/displacements and normalized individually for each neuron. The first time step in each trial and time steps with non-zero landmark input were excluded from the analysis. For histograms, each condition was binned in 100 column bins and neuron rates in 10 row bins. Histograms were normalized to equal sum per column. (b) Similarly, RSC rates are more dispersed per location in LM1. Schematic of analysis: firing rates were low pass filtered at 0.5 Hz, and for each location, the distribution of rates was computed in 8 bins, between the lowest and highest rate of that cell. Top: Rate distribution resolved by 2D-location (4×4 bins) for example RSC neuron. Bottom: the resulting 16 histograms for LM1 and LM2 each, red dotted example histograms correspond to indicated example location (red dotted circles). (d) Summary statistics showing a more dispersed rate distribution per location in LM1. In sum, this analysis shows that in addition to the explicit encoding of uncertainty by a stable rate code (conjunctive with position and other variables), as shown in Fig. 2a, where one would not expect a higher degree of trial-to-trial variability with higher uncertainty, there is still a degree of increased variability in states where the mouse might ‘take a guess' that would differ between trials. (a) Low-dimensional population structure can be probed by pairwise neural relationships30: correlations or offsets in spatial tuning between cell pairs should be preserved across environments if the dynamics across environments is low-dimensional. Example spike rates (6 sec window, low-passed at 1 Hz using a single-pole Butterworth filter) for 3 RSC neuron pairs from one example session. R values for each pair were computed across the LM1 and LM2 condition, as well as in the task-initialization phase where mice had to hunt blinking dots (Extended Data Figs. The latter provides a control condition where no landmark-based navigation was required and mice instead had to walk to randomly appearing targets. (b) Top: pairwise correlation matrices for LM1,2 and dot-hunting conditions. Example pairs are highlighted (i,ii,iii). Bottom: spatial firing rate profiles for example pairs. Proportion of variance of low-pass filtered (0.5 Hz) firing rates explained by first 45 principal components from the LM1 states. Proportion of variance explained (black, 16 sessions) drops to below that of shuffled spike trains (red) after the 6-10th principal component. We found no relationship between individual PCA components and task variables. (d) Correlation dimension in RSC is also low (same analysis as for the ANN in Extended Data Fig. This measure typically overestimates manifold dimension66, and thus serves as an upper bound on the true manifold dimension. (e) Grey/black: Summary statistics (median and quartiles) for correlation of correlations (panel b shows one example session, black dots indicate individual sessions, N = 16). Green: same analysis but spike rates were computed with a 5 Hz low-pass instead of the 1 Hz used throughout, no systematic changes were observed as function of low-pass settings. (f) Rates of individual RSC neurons can be predicted from other neurons with linear regression. In the LM2 to LM2 condition (black), the linear fit is computed for one held-out neuron's rate from other concurrent rates, and the same regression weights are then used to predict rates during LM1 (green) and dot-hunting (red) time periods. True rates of predicted neurons are plotted as solid black lines. (g) Summary statistics for the linear regression. Histograms show the proportion of explained variance for all 984 neurons, split by condition. In the LM2 to LM2 condition, the fit is computed from other concurrent rates (40.5% variance explained, median across neurons). The sequential, non-interleaved nature of this train/test split across task phases means that any consistent firing rate drifts across the conditions will lead to poor predictions, and consequently, a small number of neurons exhibit negative R2 values indicating a fit that can, for some cells, be worse than an average rate model (11.3% for LM1, 19.3% median across neurons for dot-hunting, small grey bars). However, 24.3% of variance (median across neurons) can be explained despite significant changes in spatial receptive fields (predict LM1 with LM2 weights) and even for a different task, with 16.2% when predicting dot-hunting activity from LM2 weights (red and green histogram and bars showing 95% CI of median). (h) Pairwise correlations between RSC neurons in another example session, same analysis as in panel b, and associated scatterplots. (i) Low-dimensional activity quantified via participation ratio (PR)68. This analysis does not account for noisy eigenvalue estimates from spiketrains, and consequently the shuffled spike trains where there are no prominent modes that correspond to stable sensory, motor, or latent states, yield values of PR = ~ 45. speed of largest 5 principal components, filtered at 1 Hz, CIs via bootstrap) correlates with running speed. Top right: When landmarks appear/disappear, they perturb neural activity (effect of mouse speed is regressed out). Decoding was performed using the same method as in Fig. (b) For some analyses of the low-dimensional dynamics in RSC (Fig. 4, this figure panel h), rate fluctuations related to non-spatial covariates such as speed, heading, etc. were removed: a single-layer LSTM with 20 hidden units was trained to predict the mouse position in a 10×10 grid from the RSC rates. The network learned 20 spatially relevant mixtures of input firing rates, with appropriate temporal smoothing to represent the mouse location. These activations were then embedded into 3-D space via isomap53. (c) To find trials across which mouse trajectories as they approached the 2nd landmark were similar, mouse trajectories were clustered (see Methods) leading to a subset of trials with similar locomotion and visual inputs. (d) The activity of RSC, in the low-dimensional representation, and in raw spike counts was then analyzed further. (e) Alternative hypotheses for smoothness / predictability of neural dynamics across trials (corresponding to Fig. Dynamics across trials could behave like a laminar flow, so that trials with similar neural state remain so (top), or they could shuffle, leading to a loss of the pairwise distance relationships across trials (bottom). (f) We measured this maintenance vs. loss of correlation in a sliding 750 ms window beginning at the 2nd landmark onset, versus a window just before. CIs were computed across sessions (See Methods). (g) Hypotheses for whether stable neural dynamics (Fig. 8) can determine how RSC activity encodes disambiguated landmark identity (‘a' or ‘b'). Top: trials in which the correct identity is ‘a' but that are neurally close to other trials where the answer is ‘b' might get dragged along in the wrong direction at least transiently. Bottom: alternatively, neural activity could be determined by the correct answer, even in trials that (in neural rate space) are close to trials from the opposing class. As a control, we also analyzed neurally far trials (grey). Right: Both before and after the 2nd landmark becomes visible, the classes are distinct in neural state space. (j) Whether a trial comes from LM1a or b can also be decoded from low-pass filtered (2 Hz) firing rates before the 2nd landmark onset (via regression tree, cross-validated across trials, balanced N across conditions, 5 sessions). (a) ANN with binary landmark presence input. (b) State space trajectories in the internal map network after the second landmark encounter in two different environments. Landmarks and trajectories were sampled with the same parameters as experiment configuration 1, but the duration of test trials was extended from 10 s (100 timesteps) to 50 s (500 timesteps). Only trials with low error after the second landmark encounter are shown, defined as maximum network localization error smaller than 0.5 rad, measured in a time window between 5 timesteps after the second landmark encounter until the end of the trial. (c) State space dimension is approximately 3, same analysis as in Extended Data Fig. (d) Example tuning curves, same analysis as in Extended Data Fig. (e) Linear decoding of position, displacement from last landmark and landmark separation from ANN activity, same analysis as in Extended Data Fig. A multinomial regression decoder was trained on 4000 trials from experiment configuration 1 (the training distribution of the internal map task) to predict from hidden layer activities which of the four possible environments was present. Performance was evaluated on separate 1000 test trials sampled from the training distribution. (f) Example neurons showing transition from egocentric landmark-relative displacement coding to allocentric location encoding, same analysis as in Extended Data Fig. (g) Example neurons showing conjunctive encoding, same analysis as in Extended Data Fig. Location tuning curves were determined after the second landmark encounter using 1000 trials from experiment configuration 2 using 20 location bins. Velocity and uncertainty from the posterior circular variance of the enhanced particle filter were binned in three equal bins. (h) Distribution of absolute connection strength between and across location-sensitive “place cells” (PCs) and location-insensitive “unselective cells” (UCs), same analysis as in Extended Data Fig. (i) Hidden unit activations, corresponding to Fig. (j) Trajectories from example trials, as in Fig. (k) Same trajectories as in i&j but with full LM2 state. (l) ANN is robust to perturbations, same as in Extended Data Fig. (m) ANN maintains pairwise correlation structure across states and environments, same as in Fig. Training an ANN in the external map condition but with non-negative activity replicated all key results from the other NN types: we observed similar results with respect to location and displacement tuning (r), the transition in linear decodability of displacement to location from the population and dynamically varying decodability of landmark separations within trials (p), the presence of heterogeneous and conjunctive tuning (s), lack of modularity in connectivity between cells with high and low amounts of spatial selectivity (t), and the preservation of cell-to-cell correlations across time within trials and across environments (q). The nonlinearity does affect the distribution of recurrent weights: The distribution of non-diagonal elements in the non-negative network is sparse (excess kurtosis k = 7.8), while it is close to Gaussian for the external and internal map networks with tanh-nonlinearity (k = 0.6 and k = 0.9 respectively; panel u); however, the distributions of eigenvalues of the recurrent weights have similar characteristics for all trained networks (panel v). Structure of the recurrent network: Input neurons encoded noisy velocity (10 neurons) and received external map input (70 neuron), same as the regular external map ANN. Recurrent layer rates were constrained to be non-negative. (o) Example tuning curves, same analysis as before. (p)Linear decoding of position, displacement from last landmark and landmark separation from ANN activity, same analysis as before. (q) ANN maintains pairwise correlation structure across states and environments, same as before. (r) Example neurons showing transition from egocentric landmark-relative displacement coding to allocentric location encoding, same analysis as before. (s) Example neurons showing conjunctive encoding, same analysis before. (t) Distribution of absolute connection strength between and across location-sensitive “place cells” (PCs) and location-insensitive “unselective cells” (UCs), same analysis as before. (u) Distribution of non-diagonal recurrent weights for randomly initialized (untrained), external map, internal map, and non-negative network. The k-value measured denotes excess kurtosis, a measure of deviation from Gaussianity (k = 0 for Gaussian distributions). (v) Scatterplot of real and imaginary part of complex eigenvalues of recurrent weight matrix for randomly initialized (untrained), external map, internal map, and non-negative network. The distributions of eigenvalues of the recurrent weights have similar characteristics for all trained networks. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Voigts, J., Kanitscheider, I., Miller, N.J. et al. Spatial reasoning via recurrent neural dynamics in mouse retrosplenial cortex. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41592-025-02713-3'>SpotSweeper: spatially aware quality control for spatial transcriptomics</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-06 09:28:02
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. (2025)Cite this article Quality control (QC) is a crucial step to ensure the reliability of data obtained from RNA sequencing experiments, including spatially resolved transcriptomics (SRT). Existing QC approaches for SRT that have been adopted from single-cell or single-nucleus RNA sequencing methods are confounded by spatial biology and are inappropriate for SRT data. In addition, no methods currently exist for identifying histological tissue artifacts that are unique to SRT. Here, we introduce SpotSweeper, a spatially aware QC method that leverages local neighborhoods to correct for spatial confounding in order to identify both local outliers and regional artifacts in SRT. Using SpotSweeper on publicly available data, we identify a consistent set of Visium barcoded spots as systematically low quality and demonstrate that SpotSweeper accurately identifies two distinct types of regional artifacts. SpotSweeper represents a substantial advancement in spatially resolved transcriptomics QC for SRT, providing a robust, generalizable framework to ensure data reliability across diverse experimental conditions and technologies. This is a preview of subscription content, access via your institution Get Nature+, our best-value online-access subscription cancel any time Subscribe to this journal Receive 12 print issues and online access only $21.58 per issue Buy this article Prices may be subject to local taxes which are calculated during checkout The DLPFC Visium datasets9,10 used for analyses in this manuscript can be obtained from spatialLIBD (http://research.libd.org/spatialLIBD) in SpatialExperiment format, which includes manual annotation labels from the original sources. The DLPFC samples from Huuki-Myers et al. with dryspot and hangnail artifacts were the ‘Br8325_ant' and ‘Br3942_mid' samples, respectively. Other original datasets and any manual annotations are sourced from references, including Visium ovarian cancer dataset39 through the Gene Expression Omnibus series GSE211956, the Slide-seqV2 mouse hippocampus dataset40 through an Rdata object made available on the SpatialPCA website, the Stereo-seq mouse embryonic dataset42 through the Spatial Transcriptic Omics DataBase, the MERFISH mouse colon dataset24 through the MouseColonIbdCadinu2024 function in the MerfishData R/Bioconductor package, and the STARmap mouse brain dataset25 through the STARmapPLUS_mousebrain function in the STexampleData R/Bioconductor package. All other data were sourced directly from 10x Genomics' publicly available datasets, including Visium Human Breast Cancer (https://www.10xgenomics.com/datasets/human-breast-cancer-visium-fresh-frozen-whole-transcriptome-1-standard), Visium Coronal Mouse Brain (https://www.10xgenomics.com/datasets/adult-mouse-brain-coronal-section-fresh-frozen-1-standard), Xenium 5K Coronal Mouse Brain (https://www.10xgenomics.com/datasets/xenium-prime-fresh-frozen-mouse-brain), VisiumHD Human Breast Cancer (https://www.10xgenomics.com/datasets/visium-hd-cytassist-gene-expression-human-breast-cancer-fresh-frozen), and VisiumHD Coronal Mouse Brain (https://www.10xgenomics.com/datasets/visium-hd-cytassist-gene-expression-mouse-brain-fresh-frozen). All other data supporting the findings of this study are available in the article and the Supplementary Information. Source data are provided with this paper. The code underlying these figures is deposited at https://github.com/boyiguo1/Manuscript-SpotSweeper (Zenodo: https://doi.org/10.5281/zenodo.14908155)48. The open-source software package SpotSweeper is available in the R programming language and freely available on Bioconductor (https://bioconductor.org/packages/SpotSweeper). We used SpotSweeper version 1.3.3 for the analyses in this manuscript. Du, J. et al. Advances in spatial transcriptomics and related data analysis strategies. Atta, L. & Fan, J. Computational challenges and opportunities in spatially resolved transcriptomic data analysis. Kleino, I., Frolovaitė, P., Suomi, T. & Elo, L. L. Computational solutions for spatial transcriptomics. Li, Z. et al. Benchmarking computational methods to identify spatially variable genes and peaks. Preprint at bioRxiv https://doi.org/10.1101/2023.12.02.569717 (2023). Li, H. et al. A comprehensive benchmarking with practical guidelines for cellular deconvolution of spatial transcriptomics. Yuan, Z. et al. Benchmarking spatial clustering methods with spatially resolved transcriptomics data. Murphy, A. E., Fancy, N. & Skene, N. Avoiding false discoveries in single-cell RNA-seq by revisiting the first Alzheimer's disease dataset. Bhuva, D. D. et al. Library size confounds biology in spatial transcriptomics data. Huuki-Myers, L. A. et al. A data-driven single-cell and spatial transcriptomic map of the human prefrontal cortex. Maynard, K. R. et al. Transcriptome-scale spatial gene expression in the human dorsolateral prefrontal cortex. Thompson, J. R. et al. An integrated single-nucleus and spatial transcriptomics atlas reveals the molecular landscape of the human hippocampus. Preprint at bioRxiv https://doi.org/10.1101/2024.04.26.590643 (2024). Lotfollahi, M., Hao, Y., Theis, F. J. & Satija, R. The future of rapid and automated single-cell data analysis using reference mapping. Righelli, D. et al. Spatialexperiment: infrastructure for spatially-resolved transcriptomics data in r using bioconductor. Chen, D., Lu, Chang-Tien, Kou, Y. & Chen, F. On detecting spatial outliers. & Hoaglin, D. C. How to Detect and Handle Outliers (American Society for Quality Control, 1993). Heumos, L. et al. Best practices for single-cell analysis across modalities. Amezquita, R. A. et al. Orchestrating single-cell analysis with Bioconductor. Luecken, M. D. & Theis, F. J. Current best practices in single-cell RNA-seq analysis: a tutorial. A. et al. miQC: An adaptive probabilistic framework for quality control of single-cell RNA-sequencing data. Germain, Pierre-Luc, Sonrel, A. & Robinson, M. D. pipeComp, a general framework for the evaluation of computational pipelines, reveals performant single cell RNA-seq preprocessing tools. Osorio, D. & Cai, J. J. Systematic determination of the mitochondrial proportion in human and mice tissues for single-cell rna-sequencing data quality control. Ross, M. G. et al. Characterizing and measuring bias in sequence data. Aird, D. et al. Analyzing and minimizing pcr amplification bias in illumina sequencing libraries. Cadinu, P. et al. Charting the cellular biogeography in colitis reveals fibroblast trajectories and coordinated spatial remodeling. Shi, H. et al. Spatial atlas of the mouse central nervous system at molecular resolution. Seoane, J., Varela-Centelles, P. I., Ramírez, J. R., Cameselle-Teijeiro, J. & Romero, M. A. Artefacts in oral incisional biopsies in general dental practice: a pathology audit. A review of artifacts in histopathology. Subramanian, A., Alperovich, M., Yang, Y. & Li, B. Biology-inspired data-driven quality control for scientific discovery in single-cell transcriptomics. Yates, J., Kraft, A. & Boeva, V. Filtering cells with high mitochondrial content removes viable metabolically altered malignant cell populations in cancer single-cell studies. Guo, B. et al. Integrating spatially-resolved transcriptomics data across tissues and individuals: Challenges and opportunities. Aihara, G. et al. SEraster: a rasterization preprocessing framework for scalable spatial omics data analysis. Lun, A. BiocNeighbors: Nearest Neighbor Detection for Bioconductor Packages. R package version 1.20.2. Burrus, C. S., Barreto, J. & Selesnick, I. W. Iterative reweighted least-squares design of fir filters. Venables, W. N. & Ripley, B. D.Modern Applied Statistics with S. (Springer, 2002). Some methods for classification and analysis of multivariate observations. 5th Berkeley Symposium on Mathematical Statistics and Probability/University of California Press 281–297 (University of California Press, 1967). Ester, M., Kriegel, H.-P., Sander, J. A density-based algorithm for discovering clusters in large spatial databases with noise. Second International Conference on Knowledge Discovery and Data Mining 226–231 (AAAI Press, 1996). Hahsler, M. & Piekenbrock, M. dbscan: Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Related Algorithms. R package version 1.1-12. Singhal, V. et al. Banksy unifies cell typing and tissue domain segmentation for scalable spatial omics data analysis. Denisenko, E. et al. Spatial transcriptomics reveals discrete tumour microenvironments and autocrine loops within ovarian cancer subclones. Stickels, R. R. et al. Highly sensitive spatial transcriptomics at near-cellular resolution with slide-seqv2. Shang, L. & Zhou, X. Spatially aware dimension reduction for spatial transcriptomics. Chen, A. et al. Spatiotemporal transcriptomic atlas of mouse organogenesis using dna nanoball-patterned arrays. Oliveira, M. F. et al. Characterization of immune cell populations in the tumor microenvironment of colorectal cancer using high definition spatial profiling. Preprint at bioRxiv https://doi.org/10.1101/2024.06.04.597233 (2024). Guo, B., Huuki-Myers, L. A., Grant-Peters, M., Collado-Torres, L. & Hicks, S. C. escher: unified multi-dimensional visualizations with gestalt principles. Zhao, E. et al. Spatial transcriptomics at subspot resolution with bayesspace. Chen, S. et al. Spatially resolved transcriptomics reveals genes associated with the vulnerability of middle temporal gyrus in Alzheimer's disease. Zeng, H. et al. Large-scale cellular-resolution gene profiling in human neocortex reveals species-specific molecular signatures. This project was supported by the National Institute of Mental Health (R01MH126393 to B.G. and the Chan Zuckerberg Initiative, and DAF, an advised fund of Silicon Valley Community Foundation (CZF2019-002443 to S.C.H.). All funding bodies had no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript. We thank our collaborators at the Lieber Institute for Brain Development for valuable input and feedback during the development and application of these methods to identify the nature and cause of regional artifacts during sample preparation. 's c-advisor, K. Martinowich at the Lieber Institute for Brain Development, for access to BioRender to create the schematics in Figures 1 (https://BioRender.com/u36c260) and 5 (https://BioRender.com/ceha9re). We thank the maintainers of the Joint High Performance Computing Exchange (JHPCE) compute clusters at the Johns Hopkins Bloomberg School of Public Health for providing essential computing resources. Finally, we thank J. Fan at Johns Hopkins University for the suggestion to investigate the barcodes underlying systematic outliers in the Visium platform. Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA Michael Totty, Stephanie C. Hicks & Boyi Guo Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, USA Center for Computational Biology, Johns Hopkins University, Baltimore, MD, USA Malone Center for Engineering in Healthcare, Johns Hopkins University, Baltimore, MD, USA You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar You can also search for this author inPubMed Google Scholar : conceptualization, methodology, software, validation, formal analysis, investigation, data curation, writing, visualization. : conceptualization, resources, writing (review and editing), visualization, supervision, project administration, funding acquisition. : conceptualization, writing (review and editing), visualization, supervision, project administration. Correspondence to Boyi Guo. The authors declare no competing interests. Nature Methods thanks Dharmesh Bhuva, Yi Zhao and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: Rita Strack, in collaboration with the Nature Methods team. Peer reviewer reports are available. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Cumulative distributions of local z-scores for (A) library size, (B) detected genes, and (C) mitochondrial ratio across parameter settings (that is, k=1st-5th order neighbors) using Sample 151507 from10. Dotted lines represent 3 z-score thresholds for local outlier detection and inset plots show the cumulative distribution functions zoomed in at these threshold points. (D) Spot plots of library size local outliers across parameter settings for Sample 151507. (E) Box plots showing the number of library size local outliers per sample across parameter settings. (F) Percentage of library size outliers across neighborhood sizes shared with our recommended setting of 3rd order neighbors. (G) Number spots found in addition to the local outliers detected by 3rd order neighbors. The distributions of (H) library size, (I) detected genes, and (J) mitochondrial ratio of detected outliers across parameter settings. Local outliers detected using 1st order neighbors contain a substantial number of high-quality spots that contain markedly higher library size, unique genes, and lower mitochondrial ratio. (A) The spot plot of a VisiumHD (with 8x8 um bins) dataset of one hemisphere coronal section of a mouse brain (10x Genomics) colored by spatial domains discovered using Banksy unsupervised clustering (see Methods). (B-D) Spot plots displaying the number of detected genes overlaid with the low quality spots (red) identified using (B) fixed thresholds, (C) three MADs, and (D) local outliers detected by SpotSweeper. (E-G) Ridge plots of the distribution of library size, percent of mitochondrial genes, and number of detected genes across spatial domains. Red dotted lines indicate fixed thresholds to identify outliers (< 500 sum genes, > 10% mito percent, < 500 unique genes respectively). (H) Bar plots of the total number of discarded spots across spatial domains using the four different quality control methods. (I-K) Ridge plots showing the distribution of SpotSweeper's local z scores for library size, mitochondrial ratio, and unique genes. (A) Spot plots of a Xenium dataset of one hemisphere coronal section of a mouse brain (10x Genomics) colored by Banksy spatial domains. (B-D) Spot plots displaying the number of detected genes overlaid with the low quality spots (red) identified using either fixed thresholds (C), three MADs (D), or local outliers as detected by SpotSweeper. (E-H) Ridge plots of the distribution of library size and number of detected genes across spatial domains. Red dotted lines indicate fixed thresholds to identify outliers (< 100 sum genes and < 100 unique genes, or > 3 z-scores). (I) Bar plots of the total number of discarded spots across spatial domains using the four different quality control methods, including SpotSweeper. (A) Spot plots of a MERFISH dataset of single healthy mouse colon tissue section from Cadinu et al.24 colored by Banksy spatial domains. (B-D) Spot plots displaying the number of detected genes overlaid with the low quality spots (red) identified using either fixed thresholds (C), three MADs (D), or local outliers as detected by SpotSweeper. (E-H) Ridge plots of the distribution of library size and number of detected genes across spatial domains. Red dotted ines indicate fixed thresholds to identify outliers (< 35 sum genes and < 25 unique genes, or > 3 z-scores). (I) Bar plots of the total number of discarded spots across spatial domains using the four different quality control methods, including SpotSweeper. (A) Spot plots of a STARmap dataset of one hemisphere coronal section of a mouse brain from Shi et al.25 colored by Banksy spatial domains. (B-D) Spot plots displaying the number of detected genes overlaid with the low quality spots (red) identified using either fixed thresholds (C), three MADs (D), or local outliers as detected by SpotSweeper. (E-H) Ridge plots of the distribution of library size and number of detected genes across spatial domains. Red dotted ines indicate fixed thresholds to identify outliers (< 35 sum genes and < 25 unique genes, or > 3 z-scores). (I) Bar plots of the total number of discarded spots across spatial domains using the four different quality control methods, including SpotSweeper. Cumulative distributions of local z-scores for (A) library size and (B) detected genes across various parameter settings of the Xenium 5K Coronal Mouse Brain. Dotted lines represent 3 z-score thresholds for local outlier detection and inset plots show the cumulative distribution functions zoomed in at these threshold points. (C) Bar plots showing the number of library size local outliers across parameter settings. (D) Number spots found in addition to the local outliers detected by our recommended setting of k=50 nearest neighbors. (E) Percentage of library size outliers across neighborhood sizes shared with k=50 nearest neighbors. The distributions of (F) library size and (G) detected genes across parameter settings. (A) Spot plots showing detected artifacts (red) across a range of multiscale parameter settings (that is, 1st-2nd through 1st-7th order neighborhoods). (B) The percentage of annotated spots overlapping with our recommended multiscale parameter setting (1st-5th order neighbors). (C) The number of additional annotated spots found in comparison with our recommendation parameter setting. Source data underlying plots for Fig. Source data underlying plots for Fig. Source data underlying plots for Fig. Source data underlying plots for Fig. Source data underlying plots for Fig. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions Totty, M., Hicks, S.C. & Guo, B. SpotSweeper: spatially aware quality control for spatial transcriptomics. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative © 2025 Springer Nature Limited Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            