
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - TECHNOLOGY Article Summaries - 2025-08-18</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            TECHNOLOGY
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://news.ycombinator.com/item?id=44943986'>Show HN: We started building an AI dev tool but it turned into a Sims-style game</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://news.ycombinator.com', 'title': 'Hacker News'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 19:01:14
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>It worked well enough that the system became self-improving, churning out content, and acting like a co-pilot that helped you build new agents.But when we stepped back, what we had was these endless walls of text. We were also convinced that it would be swallowed up by the next model's capabilities. We wanted to build something else—something that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?Both of us grew up on simulation games—RollerCoaster Tycoon 3, Age of Empires, SimCity—so we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we'd done before.The very first version was small: a single Unity room, an MCP server, and a chat box. Even getting two agents to take turns took weeks. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Demo video: https://www.youtube.com/watch?v=sRPnX_f2V_c.The original idea was simple: make it easy to create AI agents. It worked well enough that the system became self-improving, churning out content, and acting like a co-pilot that helped you build new agents.But when we stepped back, what we had was these endless walls of text. We were also convinced that it would be swallowed up by the next model's capabilities. We wanted to build something else—something that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?Both of us grew up on simulation games—RollerCoaster Tycoon 3, Age of Empires, SimCity—so we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we'd done before.The very first version was small: a single Unity room, an MCP server, and a chat box. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. The original idea was simple: make it easy to create AI agents. It worked well enough that the system became self-improving, churning out content, and acting like a co-pilot that helped you build new agents.But when we stepped back, what we had was these endless walls of text. We were also convinced that it would be swallowed up by the next model's capabilities. We wanted to build something else—something that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?Both of us grew up on simulation games—RollerCoaster Tycoon 3, Age of Empires, SimCity—so we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we'd done before.The very first version was small: a single Unity room, an MCP server, and a chat box. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. We were also convinced that it would be swallowed up by the next model's capabilities. We wanted to build something else—something that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?Both of us grew up on simulation games—RollerCoaster Tycoon 3, Age of Empires, SimCity—so we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we'd done before.The very first version was small: a single Unity room, an MCP server, and a chat box. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we'd done before.The very first version was small: a single Unity room, an MCP server, and a chat box. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. That unpredictability kept us building.Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Now it's a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There's community sharing built in, so you can post rooms you make.Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Under the hood, Unity's ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.We'd love feedback on scenarios worth testing and what to build next. We'd love feedback on scenarios worth testing and what to build next. LLMs with some decent harnesses could build up unpredictable - but internally consistent - strategies per each new game you play.This is close to a proof of concept for those improvements. This is close to a proof of concept for those improvements. Otherwise you run into the risk of "TOTAL NUCLEAR FINANCIAL LEGAL DESTRUCTION" ;) Same thing happened when I tried hitting the URL directly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/08/18/pixel-10-ai-capabilities-and-everything-else-we-expect-out-of-the-made-by-google-2025-event/'>Pixel 10, AI capabilities, and everything else we expect out of the Made by Google 2025 event</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 16:30:49
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Apple's hardware event is expected to take place in September. The tech giant has made its intentions clear to grow the Pixel line, likely incorporating more generative AI capabilities with Gemini. Google already dropped several AI-related updates at its developer conference in May, but the company will likely demonstrate more Gemini features during the Pixel 10 event. In a recent YouTube video promoting the new devices, Google even seemed to poke fun at Apple's failed promises with Apple Intelligence advancements on the iPhone. Rumors are buzzing about a feature called “Camera Coach,” which would let Gemini give you real-time tips for taking better photos. The AI is supposedly able to see what's going on and suggest the best angles and lighting for your shot. There's also talk of a conversational photo editing tool, where you tell Gemini what you want to tweak in a photo, such as increasing the brightness, removing objects, or changing the background. Additionally, all phones in the lineup are expected to be powered by a new Tensor G5 processor, ushering in the Gemini era and enhancing performance and power efficiency compared to the Tensor G4 chip. Aside from these updates, not many obvious changes appear to be on the table. Plus, there is talk that the Pixel 10 series could support Qi2 wireless charging. If true, this means it would be one of the few Android phones to work with magnetic accessories, such as chargers, wallets, and mounts. As for Google's upcoming foldable, there are rumors suggesting it'll have a larger cover display, measuring at around 6.4 inches, alongside an 8-inch main screen. (And according to leaked specs, this may in fact be coming.) The phone should also likely get a bump in battery life. The Pixel 10 may come in Moonstone and Jade colors. Wearables will also get their time to shine at Google's event, with the Pixel Watch 4 rumored to get a thicker design, a longer battery life, and smaller bezels. There's also a strong likelihood that the smartwatch will introduce new health and fitness-tracking capabilities, possibly including an upgraded blood oxygen (SpO2) monitoring function. Leaks of the Pixel Buds 2a indicate that the earbuds will come in new colors (Fog Light, Hazel, Iris, and Strawberry) along with features like active noise cancellation. The Pixel Buds Pro 2 will also reportedly come in a new Sterling color. This story has been updated to include additional rumors. Lauren covers media, streaming, apps and platforms at TechCrunch. Amplify your reach, spark real connections, and lead the innovation charge. How your solar rooftop became a national security issue Google CEO adds a new calendar feature at Stripe co-founder's request iOS 26 beta 6 adds new ringtones, snappy app launches, and more</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/ai-is-about-to-radically-alter-military-command-structures-that-date-back-to-napoleon-2000644380'>AI Is About to Radically Alter Military Command Structures That Date Back to Napoleon</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 16:30:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>This article is republished from The Conversation under a Creative Commons license. At the same time, military organizations have struggled to incorporate new technologies as they adapt to new domains – air, space and information – in modern war. AI agents – autonomous, goal-oriented software powered by large language models – can automate routine staff tasks, compress decision timelines and enable smaller, more resilient command posts. They can shrink the staff while also making it more effective. That need stems from the reality that today's command structures still mirror Napoleon's field headquarters in both form and function – industrial-age architectures built for massed armies. Over time, these staffs have ballooned in size, making coordination cumbersome. They also result in sprawling command posts that modern precision artillery, missiles and drones can target effectively and electronic warfare can readily disrupt. Russia's so-called “Graveyard of Command Posts” in Ukraine vividly illustrates how static headquarters where opponents can mass precision artillery, missiles and drones become liabilities on a modern battlefield. Military planners now see a world in which AI agents – autonomous, goal-oriented software that can perceive, decide and act on their own initiative – are mature enough to deploy in command systems. These AI agents can parse doctrinal manuals, draft operational plans and generate courses of action, which helps accelerate the tempo of military operations. Experiments – including efforts I ran at Marine Corps University – have demonstrated how even basic large language models can accelerate staff estimates and inject creative, data-driven options into the planning process. These efforts point to the end of traditional staff roles. There will still be people – war is a human endeavor – and ethics will still factor into streams of algorithms making decisions. These teams are likely to be smaller than modern staffs. AI agents will allow teams to manage multiple planning groups simultaneously. For example, they will be able to use more dynamic red teaming techniques – role-playing the opposition – and vary key assumptions to create a wider menu of options than traditional plans. The time saved not having to build PowerPoint slides and updating staff estimates will be shifted to contingency analysis – asking “what if” questions – and building operational assessment frameworks – conceptual maps of how a plan is likely to play out in a particular situation – that provide more flexibility to commanders. To explore the optimal design of this AI agent-augmented staff, I led a team of researchers at the bipartisan think tank Center for Strategic & International Studies' Futures Lab to explore alternatives. The team developed three baseline scenarios reflecting what most military analysts are seeing as the key operational problems in modern great power competition: joint blockades, firepower strikes and joint island campaigns. Joint refers to an action coordinated among multiple branches of a military. In the example of China and Taiwan, joint blockades describe how China could isolate the island nation and either starve it or set conditions for an invasion. Firepower strikes describe how Beijing could fire salvos of missiles – similar to what Russia is doing in Ukraine – to destroy key military centers and even critical infrastructure. Last, in Chinese doctrine, a Joint Island Landing Campaign describes the cross-strait invasion their military has spent decades refining. Any AI agent-augmented staff should be able to manage warfighting functions across these three operational scenarios. This approach – called the Adaptive Staff Model and based on pioneering work by sociologist Andrew Abbott – embeds AI agents within continuous human-machine feedback loops, drawing on doctrine, history and real-time data to evolve plans on the fly. First, they can be overly generalized, if not biased. Foundation models – AI models trained on extremely large datasets and adaptable to a wide range of tasks – know more about pop culture than war and require refinement. Second, absent training in AI fundamentals and advanced analytical reasoning, many users tend to use models as a substitute for critical thinking. No smart model can make up for a dumb, or worse, lazy user. To take advantage of AI agents, the U.S. military will need to institutionalize building and adapting agents, include adaptive agents in war games, and overhaul doctrine and training to account for human-machine teams. Second, they will need to develop additional cybersecurity measures and conduct stress tests to ensure the agent-augmented staff isn't vulnerable when attacked across multiple domains, including cyberspace and the electromagnetic spectrum. Third, and most important, the military will need to dramatically change how it educates its officers. This could include revamping some military schools to focus on AI, a concept floated in the White House's AI Action Plan released on July 23, 2025. Absent these reforms, the military is likely to remain stuck in the Napoleonic staff trap: adding more people to solve ever more complex problems. Get the best tech, science, and culture news in your inbox daily. Scientists decoded the silent inner thoughts of four people with paralysis, a breakthrough that could transform assistive speech. The AI chip arms race reportedly involves spooky surveillance tactics. We may earn a commission when you buy through links on our sites.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/08/18/perplexity-now-supports-live-earnings-call-transcripts-for-indian-stocks/'>Perplexity now supports live earnings call transcripts for Indian stocks</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 16:26:29
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>AI startup Perplexity is augmenting its Finance dashboard with live transcriptions of Indian public companies' quarterly earnings calls, as well as a calendar to show schedules for post-results conference calls. Alongside surfacing news about the markets, Perplexity's Finance dashboard shows market summaries, stock exchange charts, and top performing stocks. Until now, the dashboard only showed transcripts for U.S. stocks. Perplexity's Finance dashboard now support live earnings calls transcriptions and features earnings calls schedules for Indian stocks. Fill out this survey to let us know how we're doing and get the chance to win a prize in return! Amplify your reach, spark real connections, and lead the innovation charge. Every weekday and Sunday, you can get the best of TechCrunch's coverage. TechCrunch Mobility is your destination for transportation news and insight. Startups are the core of TechCrunch, so get our best coverage delivered weekly. Provides movers and shakers with the info they need to start their day. By submitting your email, you agree to our Terms and Privacy Notice.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.geekwire.com/2025/from-msnbc-to-ms-now-legacy-of-microsofts-news-experiment-echoes-in-networks-new-name/'>From MSNBC to ‘MS NOW': Legacy of Microsoft's news experiment echoes in network's new name</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.geekwire.com', 'title': 'GeekWire'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 16:18:22
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>It dropped its stake in the MSNBC cable channel to 18% in 2005 and fully exited two years later, when NBC acquired the remaining shares. It divested from the online side in 2012, although MSNBC.com — later NBCNews.com — still operated a newsroom in the Seattle region for a while after that. It's meant to eliminate confusion with NBC News and give the cable network a distinct identity as it enters its next phase under Versant, the new umbrella for former NBCU cable networks. But even without a stake in a major media brand, the company has found its own place in the behind-the-scenes infrastructure for digital news and information. And we've still got that MS as a reminder of the past — for NOW, at least. Have a scoop that you'd like GeekWire to cover? Microsoft alums were surprised by the sheer scale of its success, new oral history project shows Bill Gates business card up for auction, with significant date from Microsoft's early days Microsoft announces AI newsroom projects with Semafor and others, as NYT lawsuit looms Microsoft taps LinkedIn CEO for dual role leading Office in AI strategy shift</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/meta-wants-to-make-its-first-ar-glasses-with-a-display-as-cheap-as-a-flagship-phone-2000644227'>Meta Wants to Make Its First AR Glasses With a Display as Cheap as a Flagship Phone</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 16:10:35
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>If there's one thing that we've learned in the world of mixed reality over the past year, it's that price is pretty much everything. Just ask Apple, which is still struggling to find an audience for its $3,500 Vision Pro headset. I mean, it's not that there's not a lot to love about the Vision Pro (the best UI in mixed reality and a premium display are particular highlights), but I bought my first used car for a price that wasn't too far off, and that at least got me to work semi-on time. It's unclear what, if anything, Apple learned from its experiment in expensive AR hardware since there's no second-gen Vision Pro just yet, but Meta seems to have gotten the message, and not just with its newest headset, the Quest 3S, but apparently with an upcoming pair of smart glasses. According to a report from Bloomberg's Mark Gurman, Meta is planning to release its next pair of display-having smart glasses soon for $800, which is markedly less than the previous rumors that suggested a price of $1,000 or as much as $1,400. In other words, Meta seems to be taking a page from the Quest playbook, which is decidedly better for consumers. Don't get me wrong, $800 is still a sizeable amount of money for most people, but it's not as bad when you consider new flagship phones. Samsung's Galaxy S25, Nothing's Phone 3, and Google's Pixel 9 all debuted at the same price, and as important as phones are, they don't often differ wildly between generations unless you wait three to five years. For someone who's interested in the smart glasses category and has some disposable income, $800 might just be enough to justify pulling the trigger. And sure, Meta's smart glasses (codenamed Hypernova, and not to be confused with its AR prototype Orion) won't have full phone-like ability, but according to Gurman, they will have one thing that people have been begging for in a pair of Meta-made specs: a screen. With that screen (a single monocular display), Hypernova is said to have a much bigger bag of tricks compared to Meta's Ray-Bans, the only pair of smart glasses it currently offers commercially. For one, Hypernova will reportedly have the ability to use “mini-apps,” as Gurman puts it, which could mean any number of things, including (I hope) navigation and messaging. To control those apps, Gurman says Meta plans to ship the glasses with a body-sensing wristband that takes inputs from hand movements and would be a novel move in the world of smart glasses. On top of that, a screen would also give Meta's Hypernova glasses the ability to display notifications, which is something that current-gen Ray-Bans are sorely lacking. As important as phones are, I think most people would agree that skipping a generation (or two or three) isn't really going to be a big deal, and if people are presented with the opportunity to forego the next flagship phone in favor of a pair of smart glasses that actually expands their gadget wheelhouse and gives them lots of the same features in a different, useful, and maybe even exciting package, then I'm willing to wager some will take it. Get the best tech, science, and culture news in your inbox daily. "It's unacceptable that these policies were advanced in the first place." It looks like the Trump White House has been going easy on Silicon Valley. Also being racist and giving bad medical information is fine, too. We may earn a commission when you buy through links on our sites.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/tech-industry/cyber-security/report-claims-the-era-of-ai-hacking-has-arrived-good-and-bad-actors-leveraging-ai-in-cybersecurity-arms-race'>Report claims 'the era of AI hacking has arrived' — good and bad actors leveraging AI in cybersecurity arms race</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 14:40:24
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The security industry and the hackers they're supposed to defend against have both increased their use of AI as publicly available agents become more capable. When you purchase through links on our site, we may earn an affiliate commission. The future of cybersecurity is essentially a digital version of "Rock 'Em Sock 'Em Robots" that pits offensive- and defensive-minded AI against each other—or at least that's the impression given by an NBC report on how the industry views AI. "In recent months, hackers of seemingly every stripe — cybercriminals, spies, researchers and corporate defenders alike — have started including AI tools into their work," the report said. But they have become remarkably adept at processing language instructions and at translating plain language into computer code, or identifying and summarizing documents." NBC goes on to recount how Google has been discovering vulnerabilities with AI, how CrowdStrike is "using AI to help people who think they've been hacked," and how a startup called Xbow developed an AI that managed to "climb to the top of the HackerOne U.S. leaderboard" in June. (HackerOne has since divided its leaderboards into separate trackers for individual researchers and "collectives" like Xbow.) Earlier this month I reported on CrowdStrike's warning about how North Korean operatives are using generative AI to create resumes, social media accounts, and other materials that are used to trick Western tech companies into hiring them, at which point they shift to using the AI tooling to communicate with co-workers, write code, and otherwise maintain a facade of normalcy while they collect their paychecks. (Or at least better established than its ability to do sophisticated cybersecurity research on its own; it's still not particularly good at distilling facts.) Google vice president of security engineering Heather Adkins told NBC that she hasn't "seen anybody find something novel" with AI, and that it's "just kind of doing what we already know how to do." She also said "that will advance," but researchers, companies, and independent organizations alike have been assuring us that AI will "advance" far beyond its current limits since the 1960s, so we're operating on a long timeline here. Xbow rising to the top of the HackerOne leaderboard is also interesting, but it also overlooks the sheer amount of "slop" produced by similar AI tools that promise to help security researchers find vulnerabilities. Daniel Stenberg, lead developer of the open source curl project on which practically every internet-connected device relies, has repeatedly bemoaned the amount of time he's wasted on "vulnerabilities" found by AI. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. "The general trend so far in 2025 has been way more AI slop than ever before (about 20% of all submissions) as we have averaged in about two security report submissions per week," Stenberg said in a recent blog post on this problem. The valid-rate has decreased significantly compared to previous years." The lead developer of a project used in more than 20 billion devices is spending this much time calling out—to say nothing of actually dealing with—this issue. How many other maintainers of open source projects are being overwhelmed by similar problems, but without the same degree of visibility? So AI has proven useful in social engineering attacks like the North Korean tech worker scheme, sped up the rate at which Google's researchers can discover vulnerabilities, and found ways to game the HackerOne leaderboard. But it hasn't found many interesting vulnerabilities on its own, it's bombarding open source projects with irrelevant reports, and we have no idea if the AI used to find sensitive files on Russia's behalf produced worthwhile intelligence. Nathaniel Mott is a freelance news and features writer for Tom's Hardware US, covering breaking news, security, and the silliest aspects of the tech industry. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.geekwire.com/2025/washingtons-data-center-boom-fuels-tax-windfalls-and-energy-struggles/'>Washington's data center boom fuels tax windfalls — and energy struggles</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.geekwire.com', 'title': 'GeekWire'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 14:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Among Grant County's corn, hay and potato fields, 29 data centers have created an unlikely economic boom, helping drive annual property taxes up 1,277% to $54 million over nearly two decades. The town of Quincy, which hosts most of the facilities, has used its share of tax revenue to transform the Eastern Washington community's rural infrastructure — funding everything from a new school, hospital and fire station to plans for a deluxe soccer complex to draw tourists. But Quincy's windfall captures the complex tradeoffs that Washington and other states face as data center development booms nationwide. While the computing facilities generate substantial tax revenue and create jobs, they're also straining the electrical grid, gulping valuable water, and threatening efforts to ditch fossil fuel-powered energy. Bob Ferguson created a workgroup to examine the potential impacts of data centers on Washington's economy and environment. The group faces a Dec. 1 deadline for policy recommendations that could accelerate or slow future projects — a decision made more urgent by President Trump's AI Action Plan, which aims to slash regulations to speed data center construction supporting artificial intelligence. The state, which is home to two of the world's biggest cloud and AI companies — Microsoft and Amazon — now ranks 10th nationally with 128 data centers. That rate shrank to less than $1 dollar per thousand today. Ferguson creates workgroup to study impact of data centers in Washington state The company, which serves AI startups and global enterprises, cited tax breaks, clean energy access, and introductions to state universities for engineering talent development as key factors in choosing Washington. “The local business and political community has been a big source of support for us,” said Elliot Darvick, Voltage Park's senior vice president of operations and people, via email. Electricity rates in Washington have jumped 86% over nearly 20 years — outpacing the 51% national average, according to U.S. Energy Information Administration data. The facilities now consume 5.7% of Washington's total electricity production, according to a Sightline Institute study. And that share is expected to grow as AI applications expand. Public utility districts report receiving requests from data center companies that would double or even triple their current electrical loads, a preliminary workgroup document revealed. Energy demand is outstripping new clean energy deployments, and the Trump administration has recently rescinded permits for renewable projects, including a major wind farm planned in Idaho. This dynamic is pushing utilities toward continued reliance on natural gas plants, potentially undermining Washington's climate goals. Water consumption adds another layer of concern, as many data centers require substantial cooling systems that can strain limited resources. “Data centers may also have additional capital to invest in improving the grid.” Ferguson creates workgroup to study impact of data centers in Washington state Trump's mega bill blasted by Washington leaders: Clean energy cuts threaten AI boom, hike costs Climate goals vs. computing growth: How tech can expand data centers and support clean energy DOE announces $900M for next-gen reactors as Amazon launches nuclear power pursuit</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/pc-components/ssds/latest-windows-11-security-patch-might-be-breaking-ssds-under-heavy-workloads-users-report-disappearing-drives-following-file-transfers-including-some-that-cannot-be-recovered-after-a-reboot'>Latest Windows 11 security patch might be breaking SSDs under heavy workloads — users report disappearing drives following file transfers, including some that cannot be recovered after a reboot</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 13:48:44
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Some say continuous transfers exceeding 50GB can cause drives to vanish from the OS When you purchase through links on our site, we may earn an affiliate commission. Microsoft's latest security update for Windows 11 is allegedly causing SSD failures during heavy file transfers. An investigation by the user reveals a potential bug that appears during continuous file transfers on certain storage devices, particularly when the transfer exceeds 50GB and the drive is over 60% full. While restarting the PC makes the SSD visible again, the issue seems to occur again as soon as a large data transfer is performed again. It was earlier reported that the issue lies within how SSDs handle cache, especially DRAM-less models equipped with Phison NAND controllers, but it turns out the problem isn't confined to those types of drives. Out of the 21 drives tested, it was found that 12 of them became inaccessible, but only the Western Digital SA510 2TB could not be recovered even after a reboot. This indicates that the issue seems to impact SSDs with multiple types of SSD controllers. However, a separate report by Japanese outlet NichePCGamer has further identified at least eight users on X who are facing similar issues on their respective drives. While we wait for Microsoft and SSD manufacturers to acknowledge the bug, it is best to stay cautious, especially if you are dealing with large continuous file transfers on systems running the latest Windows 11 KB5063878 update. We've reached out to Phison specifically regarding the issue, but the company wasn't immediately available for comment. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Follow Tom's Hardware on Google News to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button. Kunal Khullar is a contributing writer at Tom's Hardware. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York,</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.technologyreview.com/2025/08/18/1122004/the-download-pigeons-role-in-developing-ai-and-native-artists-tech-interpretations/'>The Download: pigeons' role in developing AI, and Native artists' tech interpretations</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.technologyreview.com', 'title': 'MIT Technology Review'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 12:10:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Plus: Anthropic's AI models will start to cut off harmful conversations Why we should thank pigeons for our AI breakthroughs People looking for precursors to artificial intelligence often point to science fiction by authors like Isaac Asimov or thought experiments like the Turing test. But an equally important, if surprising and less appreciated, forerunner is American psychologist B.F. Skinner's research with pigeons in the middle of the 20th century.Skinner believed that association—learning, through trial and error, to link an action with a punishment or reward—was the building block of every behavior, not just in pigeons but in all living organisms, including human beings.His “behaviorist” theories fell out of favor with psychologists and animal researchers in the 1960s but were taken up by computer scientists who eventually provided the foundation for many of the artificial-intelligence tools from leading firms like Google and OpenAI. If you haven't already, subscribe now to receive future issues once they land. There is no word for art in most Native American languages. I've combed the internet to find you today's most fun/important/scary/fascinating stories about technology. 3 How cuts to NASA could damage public health researchIts essential tracking data is under threat. (Undark)+ 8,000 pregnant women may die because of US aid cuts to reproductive care. (WP $)+ Addictive, low-quality soap operas are rife on TikTok, too. (The Guardian)+ China's next cultural export could be TikTok-style short soap operas. 5 Stage-four cancer patients are living for longerBut they're also facing long, uncertain treatments with ongoing side effects. 6 AI is hackers' most valuable new toolIt's supercharging criminals who were already extremely proficient. 7 A tiny Californian startup now owns Europe's biggest battery giantNorthvolt's future looked bright—until it wasn't. 9 How to turn seaweed into biofuelThe Gulf of Mexico's beaches are covered in it—and these entrepreneurs have a plan. (Wired $)+ The hope and hype of seaweed farming for carbon removal. 10 The robot Olympics' athletes fell over a lotIt's all part of teaching them how to navigate the world more efficiently. (CNN)+ Some of them were more successful than others. “Pretend-me is doing better than the real me in all the years of social media that I've been trying to do this.” —Tracy Fetter, an artist and occasional stand-up comedian, explains why she has no regrets in allowing her likeness to be used in an AI TikTok avatar to the New York Times. How to fine-tune AI for prosperityPredictions abound on how the growing list of generative AI models will transform the way we work and organize our lives, providing instant advice on everything from financial investments to where to spend your next vacation.But for economists, the most critical question around our obsession with AI is how the fledgling technology will (or won't) boost overall productivity, and if it does, how long it will take. Can the technology lead to renewed prosperity after years of stagnant economic growth? A place for comfort, fun and distraction to brighten up your day. + The world's first video rental store is a lot older than you might think.+ Here's what all those unread books lying around your home are trying to tell you.+ Need more energy to get through the day? These foods can help.+ This wild hamster is just too cute. Plus: Microsoft is trying to fix a major security vulnerability Plus: what new satellite images reveal about the damage America's bombs have inflicted on Iran Plus: what went wrong with the Texas flash floods? Discover special offers, top stories, upcoming events, and more. Try refreshing this page and updating them one more time.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/how-we-test-air-purifiers/'>How We Test Air Purifiers and What You Should Consider When Buying</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-08-18 11:04:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>All products featured on WIRED are independently selected by our editors. Since I first started reviewing air purifiers for WIRED in 2018, I've tested dozens and dozens of the health-protecting appliances, usually for months at a time. I started out looking into how to improve the air quality in my apartment, and now I write about good air hygiene and test air quality monitors year-round in addition to purifiers. I've used them on bad air days, good air days, and in locations with pets, children, and lack of proper kitchen exhaust. And while cost and warranty length are all factors in what garners a WIRED recommendation, there is an overall question that each purifier needs to answer: Will users understand how to use the air purifier effectively? One might think that unboxing and hitting the power button is all there is to know, but there's more to it when choosing a good air purifier. WIRED has noted the increasingly popular trend of air purifiers cosplaying as pieces of furniture or wall art. We've covered air purifiers as sculptures and statement pieces, such as the Atem X below, but we never recommend air purifiers based on looks alone. There's a reason why so many employees are able to purchase air purifiers using their Flexible Spending Accounts (FSAs) for conditions that an air purifier will alleviate. We humans spend an awful lot of time indoors. And as the outdoor air becomes less regulated, good indoor air is an essential component to better health outcomes. While air purifiers are not guaranteed to be FSA eligible, they can be for certain medical conditions with a letter of medical necessity. Air purifiers used for the treatment or cure of an existing condition can qualify for FSA coverage, so take advantage if possible. If it's heavy, I'll ask for help or I'll put the box on its side to slide the product out of its shipping container. On more than one occasion I've had nothing to grip and there's no handle. If I put a box on its side and cannot grasp the product to lift it from its box, then that's the first strike. When I review a unit, I think about those with less upper body strength moving the appliances, and whether they'd be able to maneuver it around their home. I move air purifiers all around my apartment; I shouldn't have to bend at the knees to adjust a purifier's location. I was able to move the 50-pound ProX using its hidden handles and built-in lockable wheels. Not all the air purifiers WIRED tests have an app, but if one does, the pairing process should not be complicated, nor should it require a lot of time. An app with a simple dashboard, graph that corresponds to the AQI color codes, and a smart remote is usually all that's needed, though it's a bonus if the app gives the filter life expectancy. I prefer a simple power bar design that shows the percentage of filter life used. And while air purifier models give recommendations for when to replace the filter, like every six months, or when the replace filter indicator light glows on the appliance, nothing beats the heads-up of knowing exactly how much filter life has been used. While some air purifiers had internal air sensors when I first started testing in 2018, the majority of current purifiers we test now have them. If the purifier has internal sensors, there is usually an indicator light. Next question: Does the air purifier have an easy-to-see or read indicator light or air quality index (AQI) score? There are some companies, like Coway, that have their own take on the color codes, which can be confusing. While Coway's air purifiers are consistently WIRED's top-rated picks, if I could change one thing about them, it would be their custom color scale, where blue means good and green means moderate. Most times, air purifiers will show a reading of PM 2.5. I should be able to see that light across the room, and the numbers should be easy to read. Then there are some models, like those from Shark, that use their own scoring system. A score of 100 represents good air on the display of Shark's NeverChange, for example, but 100 PM 2.5 would be considered unhealthy. Often the square footage listed on a product is for one air exchange per hour at the purifier's highest setting. The US Centers for Disease Control and Prevention recommends five air exchanges an hour. Often, WIRED won't recommend air purifiers that can only clean unusually small spaces. And while those cute tabletop or miniature air purifiers seem like an easy solution, they are usually too tiny to effectively clean the air in an average-size room. Ideally, an air purifier should be able to clean the air in your room five times an hour without sounding like a conversation. And if an air purifier is louder—a lot louder—than the manufacturer claims, we'll include that in our review or won't recommend it. If an air purifier is too loud at its highest fan setting, users are likely to turn down that fan speed to a quieter and less effective setting, rendering the air purifier into nothing more than a wellness prop. And when those magnets are strong, usually on a flat surface, they work especially well. One of WIRED's top purifiers, RabbitAir's A3, has an especially sturdy front panel that stays in place with four magnetic fixtures. A common issue with panels that utilize magnetized closures is that when they're on curved surfaces, like those on cylindrical models, they seem to come undone to the touch. We highlight both affordable options and higher-end investments, always on the lookout for the best options for our readers. It's tempting to highlight a low price point when reviewing air purifiers, and sometimes we'll do this, but it's important to consider that a well-made air purifier will cost less in the long run, even if it costs more upfront. Unlike a game console, speaker, or air fryer, an air purifier is more than an appliance: It's preventative health care. And often, more expensive air purifiers come with longer warranties. For example, models from the IQAir series (like the MultiGas XE, above) come with a 10-year warranty, making them an excellent long-term investment. Also keep in mind that filters can be expensive. Some filters alone cost more than a small room air purifier, but anecdotally, I've noticed that the larger the air purifier's filter surface area, the less often they've needed replacing. We update our guide to the Best Air Purifiers frequently, so make sure to check back often for our latest recommendations and notes on longer-term testing. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            