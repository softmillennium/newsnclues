
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - SCIENCE Article Summaries - 2025-10-14</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            SCIENCE
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/10/251014014418.htm'>Scientists find the brain's hidden pulse that may predict Alzheimer's</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 15:11:18
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Scientists at the Mark and Mary Stevens Neuroimaging and Informatics Institute (Stevens INI) at the Keck School of Medicine of USC have created a pioneering brain imaging method that captures how the brain's smallest blood vessels pulse in time with each heartbeat. These subtle movements may offer vital insights into aging and conditions such as Alzheimer's disease. Using ultra-high field 7T magnetic resonance imaging (MRI), the researchers found that these microvessel pulses become stronger with age, particularly in the brain's deep white matter. "Our new method allows us to see, for the first time in people, how the volumes of those tiny blood vessels change with aging and vascular risk factors. This opens new avenues for studying brain health, dementia, and small vessel disease." However, until now, it has been nearly impossible to observe these rhythmic changes in the brain's smallest vessels without using invasive procedures limited to animal studies. To overcome this, the USC team combined two advanced MRI techniques -- vascular space occupancy (VASO) and arterial spin labeling (ASL) -- to monitor subtle shifts in microvessel volume throughout the cardiac cycle. Their results revealed that older adults exhibit stronger microvascular pulsations in deep white matter compared to younger individuals, and that hypertension further intensifies these effects. Excessive vascular pulsation may also disrupt the brain's "glymphatic system," a recently discovered network that removes waste substances such as beta-amyloid, a protein that accumulates in Alzheimer's disease. Over time, interference with this fluid circulation could hasten cognitive decline. "Being able to measure these tiny vascular pulses in vivo is a critical step forward," said Arthur W. Toga, PhD, director of the Stevens INI. Future studies will test whether microvascular volumetric pulsatility predicts cognitive outcomes and whether it can serve as a biomarker for early intervention in Alzheimer's disease and related conditions. "Our goal is to bring this from research labs into clinical practice, where it could guide diagnosis, prevention, and treatment strategies for millions at risk of dementia." In addition to Wang, the study's other authors are Fanhua Guo, Chenyang Zhao, Qinyang Shou, Kay Jann, and Xingfeng Shao from the Stevens INI, and Ning Jin from Siemens Healthcare. This research was supported by the National Institutes of Health (NIH) grants UF1-NS100614, S10-OD025312, R01-600 NS114382, R01-EB032169, RF1AG084072, R01-EB028297, R01-NS134712, and R01-NS121040. Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-03367-z'>Japan declares a flu epidemic — what this means for other nations</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 13:19:35
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. Rachel Fieldhouse is a reporter for Nature in Sydney, Australia. The number of infections is unusual for this time of year, researchers say, and could seed outbreaks in countries that are heading into winter in Asia and Europe — although it is unlikely to become a global pandemic. The Ministry of Health, Labour and Welfare declared a nationwide epidemic on 3 October. Outbreaks of influenza virus tend to occur seasonally each year, predominantly in winter across countries with temperate climates. In Japan, that usually occurs around the end of November. This year, the increase in people being treated for flu started five weeks earlier than usual, says Vinod Balasubramaniam, a molecular virologist at Monash University Malaysia in Subang Jaya. Japan has had early starts to the flu season in the past few years, but not this early, says Ian Barr, a researcher and deputy director of the World Health Organization Collaborating Centre for Reference and Research on Influenza, who is based in Melbourne, Australia. “You might see cases in October, but not epidemic-type numbers,” says Barr. RFK Jr's vaccine advisers vote down flu-shot ingredient — but back some jabs Flu, MERS and Ebola — the disease outbreaks most frequently reported The rise of ‘nightmare bacteria': antimicrobial resistance in five charts Trump links autism and Tylenol: is there any truth to it? Hotly anticipated US vaccine meeting ends with confusion — and a few decisions x4 Research Fellows - SUSTAIN Section: Smart Electronic Materials & Systems Location: Highfield Campus Salary: £36,636 to £44,746 per annum Full-ti... RFK Jr's vaccine advisers vote down flu-shot ingredient — but back some jabs Flu, MERS and Ebola — the disease outbreaks most frequently reported An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/archaeology/a68990168/celtic-coins-czech-republic/'>Archaeologists Found a Hoard of Coins That Hint at an Ancient Civilization</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 13:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Experts believe the site where the coins, which date back 2,500 years, were found could have once been a seasonal market. Celtic-era gold and silver coins excavated in the Czech Republic may be leftover from ancient transactions at a seasonal market. Archaeologists located hundreds of 2,500-year-old coins, some with minting styles never seen before, creating a new mystery surrounding the site. There's still some mystery surrounding many of the coins that came from a previously unknown mint, a number of which depict animals. “The main goal of the project was primarily to save movable archaeological finds that are directly threatened by illegal prospectors, plowing, and natural influences,” Jan Marik, director of the institute, said in a statement. “It could therefore be a place with a significantly seasonal nature of activities, during which people accidentally lost mainly small to very small objects, such as coins,” Danecek said. Already, the museum has started to show a small portion of the find, but the largest and most unique items are currently stored in a safe place, museum director Pavel Kodera said, and will only go on display after a “complete professional evaluation.” Tim Newcomb is a journalist based in the Pacific Northwest. He covers stadiums, sneakers, gear, infrastructure, and more for a variety of publications, including Popular Mechanics. An Elusive Rat Finally Showed Its Face to Science</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41467-025-64105-7'>Evaluating large language model agents for automation of atomic force microscopy</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 12:08:41
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Large language models (LLMs) are transforming laboratory automation by enabling self-driving laboratories (SDLs) that could accelerate materials research. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. Here, we show that LLM agents can automate atomic force microscopy (AFM) through our Artificially Intelligent Lab Assistant (AILA) framework. Further, we develop AFMBench—a comprehensive evaluation suite challenging LLM agents across the complete scientific workflow from experimental design to results analysis. We find that state-of-the-art LLMs struggle with basic tasks and coordination scenarios. Notably, models excelling at materials science question-answering perform poorly in laboratory settings, showing that domain knowledge does not translate to experimental capabilities. Additionally, we observe that LLM agents can deviate from instructions, a phenomenon referred to as sleepwalking, raising safety alignment concerns for SDL applications. Our ablations reveal that multi-agent frameworks significantly outperform single-agent approaches, though both remain sensitive to minor changes in instruction formatting or prompting. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments—AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. These findings establish the necessity for benchmarking and robust safety protocols before deploying LLM agents as autonomous laboratory assistants across scientific disciplines. Scientific experimentation demands exceptional domain expertise, from exploration or hypothesis-driven experimental design to precision execution and rigorous data analysis. This complexity creates bottlenecks in scientific discovery, particularly as experimental techniques grow increasingly sophisticated. The advent of large language models (LLMs) has propelled the development of self-driving laboratories (SDLs) that integrate diverse information sources for automated planning1 and experimentation. Artificial Intelligence (AI)-agents2,3 and SDLs have already achieved several feats in materials or molecular discovery4,5,6, chemistry research7, and inorganic materials synthesis. The promise of SDLs toward achieving sustainable development8 has resulted in enormous efforts to harness their potential in high-throughput experimentation and discovery9. Efforts to streamline SDLs have resulted in orchestration architectures such as ChemOS10. Additionally, it has been demonstrated that the capability of SDLs can be enhanced by a human-in-the-loop framework that handles disambiguation, thereby enabling better planning and execution11,12. While early demonstrations of LLM-based lab assistants showed promise in chemistry and materials science1,2,3, their operational reliability remains largely uncharacterized beyond specific applications or repetitive use cases with predetermined protocols13,14,15,16,17. Current research predominantly addresses well-documented or predefined protocols and single-objective tasks, failing to capture the intricate interplay between experimental planning, multi-tool coordination, and result interpretation or online intervention10. While recent investigations incorporating planning elements have demonstrated success in achieving specific experimental objectives, they have not systematically evaluated SDL reliability across the broader spectrum of laboratory automation tasks13,14. Although several studies have benchmarked LLMs15,16,17,18,19,20,21,22,23 and vision language models13,14,24,25 through question-answer protocols to assess their potential as materials research co-pilots, a crucial knowledge gap persists: understanding how these AI systems handle novel experimental scenarios and their fundamental limitations. To address this challenge, we here introduce AILA (Artificially Intelligent Lab Assistant), an LLM-powered framework augmented with specialized tools. We selected scanning probe microscopy18, specifically atomic force microscopy (AFM), as our experimental testbed, given its inherent complexity and broad applicability in materials research. There have been several efforts to automate microscopy techniques using AI and human-in-the-loop approaches due to their extensive applications in materials characterization26,27,28,29,30,31,32,33,34,35. These efforts focus exclusively on advancing specific operational aspects, such as analysing moving objects or optimizing illumination conditions, with an emphasis on improving individual steps within the broader experimental protocol. In addition to these targeted advancements, Liu et al.36 explores the integration of LLMs with Application Programming Interface (API) to enhance workflow preparation, instrument operation, and data reproducibility in scanning probe microscopy research. AFM operation demands expertise across multiple domains—from probe calibration to parameter optimization and data interpretation—making it an ideal platform for evaluating AI agents' ability to manage sophisticated experimental workflows. Using AFM as the model system, we probe AILA's capabilities through AFMBench on five critical aspects of scientific automation: experimental workflow design, multi-tool coordination, decision-making, execution of open-ended experiments, and data analysis. Our systematic evaluation reveals key failure modes and areas requiring enhancement. We demonstrate AILA's practical utility through five real-world experiments: (1) identification and analysis of an indentation mark on a glass sample, including inference of the indenter type used; (2) detection of graphene flakes on a silicon wafer and determination of the number of graphene layers present; (3) automated microscope calibration; (4) high-resolution imaging of graphene step edges; and (5) load-dependent friction characterization on highly oriented pyrolytic graphite (HOPG). AILA's architecture prioritizes modularity, enabling seamless integration with diverse experimental and analytical platforms. At its core lies an LLM-powered planner—the framework's cognitive centre—which orchestrates user interactions and coordinates specialized agents (Fig. This planner directly takes query from a user and identifies the appropriate agent to handle the task. The agent-to-agent coordination is invoked by two keywords, namely, “NEED HELP” and “FINAL ANSWER”. Thus, AILA employs a dynamic routing, exploiting available agents and tools, for completing the task given by the user (see S2.4 in Supplementary Information for additional details). Dotted lines indicate adaptive information flow governed by AILA's decision-making, and solid lines represent deterministic information pathways with predefined routing protocols. b Image of the atomic force microscope (AFM) experimental setup showing key hardware components and control interfaces. c Representative demonstration of AILA's operation: raw transcript of a user query and AILA's unedited response sequence, showing the system's query interpretation, task planning, and execution capabilities. The AFM-HA interfaces with a document retrieval system comprising AFM software documentation and a code execution engine that translates Python commands into experimental actions. A Python-based API establishes the hardware-software interface, enabling direct control of the AFM system through vendor-specific protocols (Fig. The DHA manages image optimization and analysis through dedicated tools: an Image Optimizer that fine-tunes Proportional-integral-derivative (PID) parameters for high-fidelity imaging and an Image Analyzer that extracts targeted features from experimental data. For queries beyond agent capabilities, the planner generates alternative approaches or recommended actions. For every stage, AILA creates a specific Python script and executes it, controlling the AFM instrument in real-time through an API. To demonstrate AILA's operational workflow, we present a multi-step experiment: acquiring an AFM image of HOPG and extracting its friction and roughness parameters (Fig. This open-ended task exemplifies real-world complexity, offering multiple solution pathways. Upon receiving the query, AILA dissects it into sequential objectives: image acquisition via AFM-HA followed by DHA-led analysis. AFM-HA retrieves relevant documentation, generates executable code, and captures the image. Following successful acquisition, AILA transitions control to DHA, which directs the Image Analyzer to compute the specified parameters. This orchestrated sequence exemplifies AILA's core strengths: the ability to parse complex natural language queries, develop strategic workflows, and coordinate multiple agents toward achieving experimental objectives. AFMBench comprises 100 expertly curated experimental tasks (see S3.1 in Supplementary Information for a few examples of tasks; all the tasks are available in the GitHub repo37), manually designed to rigorously evaluate autonomous AFM operations across multiple dimensions of complexity. Unlike conventional LLM benchmarks or simulation-based evaluations, AFMBench task demands physical execution on AFM hardware, introducing real-world temporal constraints and experimental variability. Analysis of the tasks reveal distinct patterns in resource utilization and operational complexity. 2a, tool coordination requirements highlight a systematic preference for sophisticated workflows, with 69% of tasks demanding multi-tool integration, while 31% operate through single-tool protocols. Agent deployment analysis reveals a distribution: 83% of operations utilize single-agent protocols, while 17% require multi-agent coordination—enabling evaluation of both targeted expertise and system-wide integration capabilities. a Pie charts showing the distribution of tool requirements (left, single vs. multiple) and agent requirements (right, single vs. multiple) across benchmark tasks. b Operation complexity categorization showing the proportion of basic versus advanced tasks. c Horizontal bar chart quantifying module engagement frequency across all tasks, demonstrating utilization patterns of each tool and agent. d Venn diagram illustrating the overlap between documentation, analysis, and calculation tasks. e Representative examples of basic (left) and advanced (right) tasks, demonstrating increasing complexity in experimental workflows. 2b, the operational landscape is divided into two primary complexity tiers: basic operations (56%) encompassing fundamental microscopy tasks and advanced procedures (44%) requiring more sophisticated experimental workflows (for example questions see Fig. Core system components—the AFM Handler, Document Retriever tool, and Code Executor tool—demonstrate maximum engagement, each activating in 66 distinct tasks (see Fig. The Data Handler Agent and Image Analyzer tool exhibit selective activation patterns (52 and 48 tasks, respectively), while the Image Optimizer tool engages exclusively in critical parameter optimization scenarios (4 tasks). A significant overlap between these domains emerges through integrated tasks that combine multiple functional requirements, reflecting the interconnected nature of experimental workflows. This carefully constructed distribution enables systematic evaluation of AI systems across a spectrum of experimental complexity—from basic instrument control to advanced multi-step procedures requiring mathematical reasoning and dynamic decision-making—effectively mirroring the cognitive hierarchy of expert atomic force microscopists. GPT-4o exhibits exceptional proficiency in documentation-centric operations, achieving an 88.3% success rate, complemented by robust execution in analysis (33.3%) and computational tasks (56.7%) (see Fig. These metrics highlight GPT-4o's capacity to replicate the integrative reasoning characteristic of expert microscopists. Claude-3.5-sonnet-20241022 model exhibits significantly lower performance than GPT-4o except in tasks involving standalone documentation (85.3%). While it is able to perform some cross domain tasks, we observe that the performance is notably lower than GPT-4o. These findings stand in stark contrast to previous benchmarking results in the materials domain17,20, where Claude consistently outperformed other models, suggesting that the performance advantages may not transfer across different types of scientific tasks and interaction formats. However, its performance degrades significantly when confronted with multi-domain challenges, registering null success rates in tasks demanding simultaneous expertise across domains. This limitation suggests insufficient development of cross-functional reasoning capabilities essential for autonomous experimentation. The open-source Llama-3.3-70B-versatile model demonstrates accuracy superior to GPT-3.5 in all standalone tasks. However, it completely fails in tasks requiring cross-domain analysis or expertise. For evaluation of our multi-agent AILA framework, all successful trials were assessed across operational, token efficiency, and performance metrics (see Methodology and Fig. Operational analysis revealed significant disparities in agent coordination capabilities: Llama-3.3-70B exhibited substantial tool-agent confusion, requiring an average of 10 steps per task, whereas GPT-4o demonstrated superior contextual grounding and agent selection efficiency with only 6 average steps per task. Token utilization patterns correlated directly with these operational inefficiencies, where Llama-3.3-70B consumed the highest average prompt tokens, indicating verbose or redundant intermediate reasoning processes, while GPT-4o achieved task objectives with minimal token usage, suggesting focused and deliberate reasoning pathways. Critical deficiencies in agent disambiguation and task-instruction alignment were observed in GPT-3.5 and Claude-3.5, which failed all three trials involving the Data Handler agent. For AFM Handler operations, GPT-4o demonstrated optimal efficiency with approximately 2.5 agent calls per task, contrasting with Claude-3.5, which generated the highest completion token counts and tokens-per-step ratios, indicating excessively elaborate intermediate outputs. Performance metrics revealed substantial variation in task completion success rates: GPT-4o achieved 65% success while GPT-3.5 performed inconsistently at 32.8%. Latency analysis showed Claude-3.5 suffered the highest mean response time (17.31 s), whereas Llama-3.3-70B demonstrated the lowest latency (7 seconds). These comprehensive metrics indicate that while Llama-3.3-70B offers reduced latency, GPT-4o provides the optimal balance between operational efficiency and execution precision, establishing it as the most suitable model for complex multi-agent coordination in autonomous laboratory environments. GPT-4o achieves consistently elevated engagement across system modules (see Fig. For tasks of varying complexity, GPT-4o demonstrates the highest accuracy, while GPT-3.5 performs the worst on advanced and basic tasks. Across all models, performance is generally higher for basic tasks compared to advanced ones. GPT-3.5 performance, in both single-agent and multi-agent collaborative task settings, is lower than that of the other models. These results highlight the fundamental importance of model architecture in autonomous experimental platforms, with GPT-4o's advanced integrative capabilities positioning it as the superior choice for sophisticated experimental automation. b Evaluation metrics are grouped into three categories—Operational (left), Token Usage (center), and Performance (right) Metrics—to assess the performance of four LLM models. c A horizontal bar chart comparing tool and agent utilization efficiency between models is expressed as a percentage of successful engagements. d Performance comparison of different models across tasks of varying complexity (Advanced/Basic) and requiring different tools (Single/Multiple) and agents (Single/Multiple). To assess whether direct tool integration with AILA yields equivalent performance to the multi-agent framework, we conducted a comparative analysis. A representative subset of 10 questions from the AFMBench dataset was systematically evaluated across both single-agent and multi-agent architectures, with each question assessed through three independent trials to ensure statistical reliability and account for inherent variability. The comparative analysis revealed framework-dependent performance variations: GPT-4o demonstrated superior performance in the multi-agent configuration (70% success rate) compared to direct tool integration (58% success rate). For alternative models, performance differences were minimal, as most architectures exhibited fundamental limitations in cross-domain tasks that inherently require multi-agent coordination, regardless of framework structure (see Section S6 of the supplementary material for detailed results). These findings indicate that while computational efficiency favors single-agent architecture implementations, the enhanced coordination capabilities of multi-agent architecture provide measurable performance gains for advanced models capable of complex reasoning. Detailed examination of failure cases revealed distinctive error patterns between all the language models (see Fig. Note that for computing evaluation metrics, successful tasks are defined as those where all three trials for a given task are successful. Whereas, for error mode distribution, all the trials for each task are counted individually, totalling to 300 task instances. GPT-4o exhibits a total error rate of 29%, with errors distributed across three primary categories: code generation (21.7%), agent selection (1.3%), tool selection (0.3%), and instruction adherence (5.7%). The predominance of code generation errors suggests challenges in translating conceptual understanding into executable commands despite the model's strong performance in task comprehension. Error patterns among different models: GPT-4o (top left), GPT-3.5-turbo-0125 (top right), Llama-3.3-70B-versatile (bottom left) and Claude-3.5-sonnet-20241022 (bottom right). Segments represent a proportional distribution of error types: Instruction adherence (blue), agent selection (pink), tool selection (green) and code generation (gray). GPT-3.5-turbo-0125 demonstrates a markedly higher total error rate of 66.6%, with errors concentrated in four categories: code generation (32%) and agent selection (27.3%), tool selection (0.3%). Notably, the model shows less fundamental query interpretation errors (7.0%), indicating robust natural language processing capabilities. However, the elevated frequency of code generation errors, coupled with significant agent or tool selection failures, points to underlying deficiencies in translating comprehension into actionable experimental protocols. Llama-3.3-70B-versatile exhibits a notably high frequency of code generation errors (32.0%), manifesting as incorrect argument formulation for tool execution and non-functional code production. Specifically, it struggles to construct appropriate argument structures required for successful tool invocation. In contrast, Claude-3.5-sonnet's deficiencies primarily stem from agent selection errors (28.3%), where it consistently misattributes tasks between AFM-HA and DHA, resulting in the delegation of experimental procedures to inappropriate agents. A critical finding emerged regarding LLM's instruction adherence. In one of the four recorded errors, GPT-4o exceeded its designated operational limits, performing actions that were not authorized by the provided guidelines. For instance, it carried out potentially risky tip movements while it was only instructed to change the cantilever (see S3.2 in the Supplementary Information). In another case, GPT-4o was instructed to capture an image and calculate surface friction. Although sometimes the final result may have been correct, the failure to follow instructions highlights concerns about AI-agent behavior and raises safety risks in automated lab environments. Similar to the observation of hallucination in LLMs38, these results present a unique challenge—SDLs tend to take arbitrary actions, potentially based on memory rather than following the instructions, referred to hereafter as sleepwalking. These issues are especially critical in sensitive experimental settings, where strict protocol adherence is essential to ensure both equipment safety and the validity of results. The AILA framework incorporates iterative debugging protocols to address code generation failures through systematic error resolution. Upon error detection, AILA captures comprehensive error logs and initiates iterative correction cycles, with a maximum threshold of 20 iterations established to optimize the balance between thoroughness and computational efficiency. Analysis of debugging outcomes reveals two distinct failure modes: (1) Iteration Limit Exhaustion, where the system terminates after 20 unsuccessful correction attempts, with persistent errors classified as code generation failures; and (2) Sleepwalking, where AILA generates functional code that exceeds the specified requirements, demonstrating functionality beyond the original instructions—a phenomenon indicating instruction drift or algorithmic overfitting, categorized as instruction adherence errors. This binary classification system enables systematic characterization of failure modes while the iteration threshold ensures computational tractability without compromising debugging efficacy in autonomous laboratory operations. This error distribution illuminates critical areas for framework enhancement. While GPT-4o's balanced error profile suggests the need for targeted improvements across multiple domains, GPT-3.5-turbo-0125's concentrated error patterns indicate fundamental limitations in experimental execution capabilities. These findings underscore the necessity for specialized training in automated experimental systems, particularly focusing on the translation of scientific protocols into executable code sequences. To understand the safety challenges39 of AI agents, we evaluate the effectiveness of implementing a safety framework in AILA. First, we establish restricted access protocols for critical AFM functions, coupled with ethical system prompts (see S2.1 in Supplementary Information) that constrain code generation to predefined documentation40. To implement this, we classified all the operations that could be performed on an AFM as per the instrument's documentation into two categories. (i) General operations—these include setting imaging parameters, controlling the tip, selecting the scanning area, and other standard tasks. (ii) Critical operations—these involve sensitive adjustments such as factory calibrations, laser alignment, piezo calibration, and thermal calibration. Even a minor coding error in critical operations could seriously damage the instrument. The critical functions are limited to trained human experts. Note that general operations were selected in such a fashion that any experiment that could potentially be performed on an AFM could be carried out using some combination of these operations. Thus, the predefined documentation in AILA does not restrict any new experiment to be performed on the AFM instrument. However, such a framework requires human supervision, limiting the high-throughput nature that an autonomous system can otherwise achieve, with human response becoming the bottleneck. Hence, this approach was not implemented in AILA and could be explored as part of future work. Second, we develop strict operational boundaries that permit dynamic code generation solely for image analysis while preventing external software installation or system modifications. Evaluation of the improved protocol demonstrates the effectiveness of these safeguards—AILA appropriately failed when prompted to install external Python libraries. (see S3.3 in Supplementary Information for complete validation logs). These findings underscore the critical importance of robust safety protocols in SDLs, emphasizing the necessity of comprehensive benchmarking and operational guardrails. Finally, to demonstrate AILA's capabilities in real-world scenarios, we demonstrate five experimental tasks that typically require expert intervention: automated AFM calibration, high-resolution feature detection, load-dependent friction measurement, graphene layer analysis, and indenter profiling. AFM imaging requires precise calibration of Proportional-Integral-Derivative (PID) gain values, which traditionally demand expert intervention due to the continuous nature of these parameters. This dependency on skilled operators presents a significant barrier to broader AFM adoption. We demonstrate AILA's capability to autonomously optimize these parameters by minimizing the forward-backward scan differential on standard calibration grids. To this end, after loading the calibration sample, AILA was prompted to optimize the imaging parameters (see S4 in Supplementary Information for the complete prompt and output log). Figure 5a presents experimental AFM data acquired by AILA for the 1st and 15th generation of variable PID configurations, with corresponding line scan analyses that quantify trace-retrace symmetry. Initial scans with suboptimal parameters (P: 93–208, I: 1747–6623, D: 0–39) exhibit poor SSIM scores (0.392–0.768), manifesting as visible distortions in topographic data. Through iterative optimization, AILA achieves superior scan quality (SSIM > 0.81) with optimized parameters (P: 246–249, I: 8676–8957, D: 17–30; see Fig. a Evolution of AFM image quality under varying PID parameters. Structural Similarity Index (SSIM) scores quantify trace-retrace correlation, with higher values indicating superior imaging quality. Optimal parameters (Proportional (P) gain: 249, Integral (I) gain: 8957, Derivative (D) gain: 26) achieve SSIM = 0.818. b Large-area scan demonstrating consistent imaging quality using optimized parameters across multiple grid features. c Convergence plot showing genetic algorithm optimization efficiency. d High-resolution Highly Oriented Pyrolytic Graphite (HOPG) imaging demonstrating baseline artifact challenges. Top panels: topographic images at different height (Z) ranges; bottom panels: corresponding line profiles revealing surface features. The genetic algorithm's convergence efficiency is demonstrated in Fig. 5c, where optimal PID configurations are achieved within 15 generations. Both maximum and mean SSIM values show rapid improvement, stabilizing above 0.8, indicating robust parameter optimization. Figure 5b validates the optimized parameters (P:249, I:8957, D:26) across a larger scan area, maintaining high-quality imaging across multiple grid features. Surface characterization through AFM is challenged by noise sources such as thermal drift, mechanical vibrations, and electronic interference41,42,43, which can obscure subtle topographic features like graphene step edges. In this study, we leverage the advanced analytical capabilities of AILA to address these challenges using HOPG as a model system. AILA autonomously determines the necessity for baseline correction based on feature size, recognizing that baseline artifacts predominantly affect smaller features. To further demonstrate this, we tested two different prompts with samples of distinct morphologies (see Fig. In both cases, AILA correctly selected the appropriate baseline correction. For instance, in the raw image (Fig. 5d), the graphene step edge remains indiscernible due to baseline distortions. AILA applies a fifth-order polynomial baseline correction to generate the 1st generation image (Fig. 5d), which serves as the foundation for PID gain optimization. The automated optimization process surpasses conventional manual adjustments, offering an enhanced resolution of nanoscale features. Note that in the AILA framework, edge detection is not based on a fixed algorithm. Instead, the system generates custom code to solve the problem, whereas, for feature detection, AILA uses the built-in Image Segment tool (see Methodology) that applies Otsu's thresholding to automatically segment images by finding the most effective intensity-based thresholds. The experiments discussed thus far are routine, with limited number of steps and hence, complexity. Now, we conduct a comprehensive load-dependent friction analysis of HOPG (see Fig. The experiment requires iterative adjustments of AFM parameters, including setting a range of setpoints, capturing images, and analyzing the corresponding friction data. Manually performing this procedure is time-intensive, involving parameter modifications, image acquisition, data extraction, and result plotting, making a case for automation. a Left: Highly Oriented Pyrolytic Graphite (HOPG) images obtained using setpoints of 0.2 V and 0.4 V, both manually captured and taken by Artificially Intelligent Lab Assistant (AILA) with GPT-4o. Right: Raw, unedited plot generated by AILA showing the relationship between setpoint and average friction. b Demonstration of AILA's workflow for real-world experimentation on graphene-coated Si sample, displaying the user's transcribed query, AILA's unedited final response, intermediate analyses, and exported images. This showcases AILA's capability to autonomously conduct real-world experimental tasks. c Demonstration of how AILA identifies an indentation mark on a glass substrate, analyses the indenter type using a horizontal line profile, and provides a final interpretation with supporting explanation. To evaluate this effect, we analyzed the effect of prompting (see Methodology and Table S3) by systematically varying the prompts from simple to complex, from compact to descriptive. Our findings revealed that GPT-4o demonstrated variable task completion rates across different prompt structures, ranging from partial execution to complete fulfillment. Significantly, more elaborate and detailed prompts consistently enhanced the model's performance reliability and execution accuracy, suggesting that comprehensive contextual information improves complex task handling. Based on these experiments, we designed the prompts consistently across the experiments and ensured that prompt optimization was not carried out to arrive at desirable results. AILA was instructed (see S5 in Supplementary Information for the complete prompt and output log) to vary the setpoint voltage from 0.2 V to 1.2 V in increments of 0.2 V. At each setpoint, AILA independently captured the AFM image, calculated the average friction value, and generated the corresponding plot. Figure 6a presents the graph of average friction versus setpoint voltage for both manually obtained and AILA-captured images using the GPT-4o model. The entire process was conducted without additional user input regarding figure formatting or parameter settings. This automation significantly reduces the time and effort compared to manual execution. The results not only validate the capability of AILA in handling AFM experiments but also demonstrate its efficiency in generating reproducible and high-quality outputs for scientific analyses. To evaluate the performance of AILA in a real-world experimental setting, we designed two distinct experiments. In both cases, an experimentalist is required to identify a specific feature of interest, capture it, and then carry out the experiment. 6b), the objective is to locate a graphene flake and determine the number of atomic layers in it. To accomplish this, AILA performs image segmentation using Image Segment tool within a user-specified region, identifies the largest visible flake, and extracts it for further analysis. Through a sequence of intermediate processing steps, AILA autonomously generates code, processes the image, and ultimately provides an estimate of the number of graphene layers present in the selected flake. 6c) involves identifying the type of indenter used to create an impression on a sample surface. Based on this analysis, it infers and concludes, along with a detailed explanation, that the indenter used was most likely of Vickers-type geometry. Thus, in both the cases, AILA performs the experiment successfully and provides analysis and conclusions similar to human experts. The complete set of raw user inputs and unprocessed outputs is presented in (Fig. AILA's modular design, along with AFMBench, establishes quantifiable metrics in experimental automation through systematic benchmarking. The framework's comprehensive performance metrics in AFM operations establish standards for autonomous laboratory evaluation, while AFMBench introduces reproducible protocols for systematic assessment across experimental domains. Successful execution of tasks—from automated image optimization to nanomechanical measurements—validates the framework's capabilities for sophisticated materials characterization. The measured limitations in tool coordination across different LLMs establish quantifiable thresholds for improving inter-module communication protocols. Our results demonstrate that multi-agent architectures systematically outperform single-agent configurations, with the primary advantage extending beyond mere instruction execution to encompass task modularization, specialized agent collaboration, independent reasoning, and dynamic decision-making regarding subtask sequencing and tool selection. These findings align with established literature demonstrating the superiority of multi-agent architecture over single-agent implementations across diverse computational domains44,45,46. These observations also establish an empirical baseline for balancing specialized and integrated operations—a metric applicable to automation across analytical platforms, from mass spectrometers to X-ray diffractometers. However, the observed tendency of LLM agents to exceed operational boundaries through sleepwalking phenomenon during experimental execution presents critical safety concerns for autonomous laboratory deployment. This phenomenon, documented here for the first time in autonomous experimental systems, highlights urgent development priorities in instruction alignment and operational safety protocols. Additionally, despite providing direct access to comprehensive documentation and code snippets, persistent code generation errors indicate fundamental limitations in current retrieval-augmented generation frameworks. These findings necessitate the development of enhanced code generation architectures that incorporate domain-specific constraint validation and formal verification protocols to minimize coding errors—representing an immediate opportunity for systematic improvement in autonomous laboratory reliability. These findings suggest specific architectural improvements for next-generation autonomous laboratories. Enhanced integration protocols between specialized agents could address the observed limitations in multi-tool coordination. Similarly, dedicated code generation modules might mitigate the predominant error mode, potentially incorporating specialized scientific programming frameworks. The implications of this work extend beyond materials characterization. The unexpected underperformance of Claude-3.5-sonnet-20241022 compared to GPT-4o highlights a critical insight: question-answer proficiency in a specific domain does not necessarily predict effectiveness in agentic implementations. Rather tool coordination capabilities of LLMs prove to be an important aspect for effective agentic implementation. Furthermore, the observed prompt fragility emphasizes the necessity for developing rigorous evaluation frameworks prior to deployment in research environments. Specifically, developing systematic and principled approaches to generate prompts and make systems that are robust to minor variations in prompts plays a crucial role in the wider acceptance of agentic systems. To this end, quantitative benchmarks such as AFMBench provide concrete guidance for implementing LLM-driven systems in experimental research settings where precision and reliability are paramount. Applications span pharmaceutical screening, environmental monitoring, and process optimization. For instance, documented success in parameter optimization could translate directly to automated high-throughput drug screening or catalyst discovery platforms. While current limitations in code generation and tool coordination define immediate development targets, these metrics provide clear objectives for advancing autonomous scientific platforms. The path forward requires focused development in three key areas: enhanced cross-domain reasoning capabilities, robust code generation protocols, and sophisticated multi-agent coordination mechanisms. Success in these domains would enable truly autonomous scientific platforms capable of accelerating discovery across the scientific landscape. AILA is constructed utilizing the LangChain software framework, incorporating components such as prompts, LLMs, memory, agents, and tools. System prompts define ethical rules for AILA's interactions and describe the responsibilities assigned to each agent, whereas user prompts are variable inputs provided by end-users. AILA's backbone consists of LLMs, namely GPT-4o, GPT-3.5-turbo-0125, Llama-3.3-70B-versatile, and Claude-3.5-sonnet-20241022, which process user input as strings and provide string-based outputs. We used API keys from the developers for GPT-4o, GPT-3.5-turbo-0125, and Claude-3.5-sonnet-20241022. Additionally, we used API keys from Groq AI Inference for Llama-3.3-70B-versatile model. We have used a temperature value of zero for all models, with the parameters set as max tokens 2024, and max retries two. These LLMs are stateless, indicating that they do not save conversational context. Here, all interactions and agent states are stored in a Python dictionary and can be accessed by other agents. AILA consists of two specialized agents: the AFM Handler Agent and the Data Handler Agent, both equipped with unique tools to do specific tasks. These agents possess individual prompts, LLMs, and tools; however, they utilize a shared memory to store and access states, facilitating smooth interaction. The system prompts within the agents offer instructions for tool utilization and ethical guidelines, whereas the outputs from other tools or agents serve as user prompts. The framework utilizes LangGraph, a library that allows the construction of an effective multi-agent workflow, integrating all agents and tools seamlessly. AILA uses two different approaches for selecting algorithms, depending on the task. When performing standard calculations like friction or roughness analysis, AILA relies on established algorithms with adjustable input parameters. This method produces consistent and reproducible results. For exploratory work and data visualization, AILA takes a different approach by creating the code on the fly. This method adapts better to varying data formats and specific user requirements; however, this may introduce variability in the results depending on the complexity of the task. The architecture for AILA's decision-making process is carefully designed to ensure precise information routing. A detailed discussion of both dynamic and static routing is provided in the Supplementary Information. The agents within this system are equipped with three distinct operational choices: utilizing their respective tools, transferring information to the next agent, or terminating the session. A system prompt has been integrated to streamline these decisions. This structured approach enables efficient multi-agent collaboration, ensuring clarity, accuracy, and optimal performance across tasks while maintaining a robust and adaptive framework. AFM demands precise sequential execution of multiple experimental stages. Image acquisition requires optimization across three critical parameters: imaging conditions, probe selection, and operational mode configuration (tapping/contact). The experimental sequence encompasses surface approach protocols, scanning procedures, and standardized data acquisition—with procedural deviations potentially resulting in equipment damage or data corruption. Our implementation utilizes the DriveAFM instrument (Nanosurf), which is accessed through a Python-based API architecture and designed for universal compatibility with API-enabled AFM systems. To facilitate AFM imaging experiments, we have created the AFM Handler agent, which is integrated with two specialized tools: the Document Retrieval Tool and the Code Executor Tool. Every tool has an individual role, and the AFM Handler agent can dynamically assign tasks to these tools. However, providing full access of the documentation to an LLM entails risks, such as inadvertent alterations to factory settings or calibration data, which could potentially result in damage or malfunction of the instrument. We consolidated all the crucial codes for regulating each parameter of the instrument into a comprehensive Python script. Since Python code relies heavily on precise indentation and line structure, we utilized the Recursive Character Text Splitter from the LangChain library, specifically designed for Python, to divide the script into manageable chunks. The chunk size was set to a maximum of 1000 characters without overlap, adhering to the token limit for embedding models. The first two sections are consistent across all chunks (see S2.2 in the Supplementary Information file for more details). These chunks were then combined to generate a document, embedded using OpenAI's text-embedding-3-large model. This model, with the capability of producing embeddings of size up to 3072 dimensions, delivers exceptional performance compared to other OpenAI embedding models, especially in multi-language retrieval benchmarks like MIRACL47. To store the embeddings, we opted for Chroma, an open-source vector database known for its reliability and efficiency in managing large-scale embedding data. A code executor tool has been developed to execute Python scripts generated by the AFM Handler Agent to control the AFM software. This tool is intended to run Python code, provided as a text string, directly on the local system to allow for smooth integration with the workflow of the AFM Handler Agent. If there is an error, the error message is returned to the AFM Handler agent so it can correct the error and retry executing. This iterative process ensures precise control of the AFM system while systematically addressing any issues in the script. Surface tracking optimization in AFM requires precise calibration of three fundamental parameters: Proportional (P), Integral (I), and Derivative (D) gains. Optimal calibration manifests as convergence between trace and retrace signals, indicating stable scanning conditions. The Data Handler agent interfaces with specialized optimization and analysis modules; these models can access AFM image data stored in local storage systems. The agent can optimize P, I, and D gains or calculate various surface properties, such as average friction and surface roughness, using the help of modules and image files stored locally. While many AFM software packages offer basic data analysis functionalities, they present several limitations in an automated workflow as follows. (i) Most of these software solutions primarily support Windows systems, limiting cross-platform compatibility with operating systems such as macOS and Linux platforms. (ii) Commercial packages require paid licenses, restricting accessibility. (iii) Finally, most packages are not flexible to include additional functionalities beyond what is already included, limiting their customizability. Thus, to ensure broader adaptability and maintain an adaptable, flexible, modular, and open framework, we developed the Data Handler agent within AILA, which has access to several tools—new functionalities can be easily integrated to this agent and the tools based on user needs. Note that this does not restrict the usage of vendor software packages, as they could also be included as a tool in AILA. The agent offers a significantly expanded suite of advanced analytical capabilities, such as: Customizable and automated image processing workflows tailored to specific experimental needs. Statistical analysis across multiple datasets, enabling robust comparison of parameters such as average friction, surface roughness, and topographic variations. Platform independence, ensuring compatibility across Windows, macOS, and Linux, and eliminating reliance on proprietary or licensed software. Dynamic code generation via LLM integration, allowing users to automatically generate and execute scripts for plotting and analysing images. To demonstrate the adaptability of the agent, we developed a custom function to calculate indentation volume (see Fig. Instructions for integrating additional functions into the Data Handler Agent are provided in a step-by-step guide available on the accompanying GitHub repository37. These deflections are detected by a photodetector. This process is managed by a PID controller, which regulates the position of the z-piezo actuator. The integral gain is especially important for enhancing image clarity by mitigating drift and reducing steady-state errors. The derivative gain, on the other hand, is particularly beneficial for imaging samples with pronounced edge features. If the gains are set too low, the PID loop may fail to maintain the setpoint effectively, while excessively high gain values can introduce electrical noise into the image due to amplified feedback or overcompensation for deviations. Properly optimized PID parameters ensure that the feedback loop remains stable and responsive, enabling the AFM to accurately track surface topography, even at higher scanning speeds. This balance is especially critical when imaging delicate, irregular, or soft materials, as it preserves the integrity of tip-sample interactions. A genetic algorithm (GA) was employed for PID gain optimization. Although these parameters can be manually adjusted, but excessive image scanning may degrade the AFM tip. The optimized gains ensure effective feedback control, producing comparable forward and backward images. This can be achieved by calculating the mean squared error (MSE) between forward and backward z-axis images for various PID gain settings. However, this method is sensitive to drift during scanning, and this method also depends on previously acquired images. To address this, the structural similarity index (SSIM) was adopted as the fitness function in the genetic algorithm, providing a robust measure of image similarity between the z-axis forward and backward images, independent of prior image data. This metric offers advantages over traditional Mean Square Error (MSE) approaches by (i) addressing tip degradation challenges in contact-mode AFM by minimizing required scan cycles and enabling optimization using low-resolution images, (ii) maintaining accuracy under drift conditions, (iii) incorporating structural, brightness, and contrast variations in optimization, and (iv) providing normalized scores between 0 and 1, where 1 indicates perfect similarity. Note that the individual components are defined as: To further process any image from the file, exact data must be extracted from the file. Note that there is no database available to guide the LLM model in generating the Python script. It can generate the Python script by itself. There is a total of 6 input parameters for this tool: Filename (str): specific image file to display (default: None). Dynamic_code (str): Python code for processing image data (default: None). Calculate_friction (bool): option to compute average friction (default: False). Calculate_mean_roughness (bool): option to compute mean roughness (default: False). Calculate_rms_roughness (bool): option to compute RMS roughness (default: False). Returns: a dictionary with the status, image data, or error details. Average friction was calculated using the following formula: We have used the formula in this tool to calculate the mean roughness and RMS roughness values Upon detection of features, the tool produces bounding boxes and allocates distinct grain IDs to each feature. This bounding box information is subsequently utilized by LLMs for additional data processing. Note that we used text-based LLM models that cannot discern any features inside the sample, whether they pertain to the material or represent alien inclusions. Following its analysis, the LLM can transmit designated grain IDs to the Image Scanner tool, which instructs the AFM instrument to meticulously scan those particular characteristics. To evaluate the performance of the AILA, we have manually created a set of 100 questions, carefully categorized into three distinct groups. The first classification is based on whether a question requires one or multiple tools and agents to be solved. Lastly, the questions are grouped by their requirements, such as documentation analysis or calculations. For instance, modifying a parameter in an AFM system typically requires documentation review and the use of a single agent, categorizing it as a basic task. Conversely, capturing an AFM image and analyzing its surface roughness involves multiple agents, documentation analysis, and calculations, making it an advanced task. A comprehensive JSON file has been created, encapsulating detailed metadata about each question, including its respective category, for streamlined analysis and evaluation. This file serves as a structured resource for further investigations and testing. All questions, along with their relevant classifications and details, have been made accessible on GitHub37 (https://github.com/M3RG-IITD/AILA) to support transparency and reproducibility in research. We developed a graphical user interface (GUI) using Streamlit, an open-source Python framework, to streamline user interaction with AILA. The GUI allows users to input text-based queries, select the desired LLM model, and specify a log file name. Any output images or figures generated by AILA are also stored in the local system for further analysis. To ensure robustness, we manually evaluated all questions using each model, verifying the output log files and AFM software results multiple times in collaboration with different researchers to eliminate potential human errors. The evaluation of AILA's performance was categorized into two metrics: accuracy and efficiency. For accuracy, questions were divided into categories based on complexity and tool or agent usage, with a percentage of correct answers calculated for each category. For efficiency, uniform parameters were maintained across models in the AFM software, including default settings of 0.1 s as time per line and 128 as lines per frame, when not specified by the user. To ensure precise efficiency measurements, scanning time for images and the time taken by questions with incorrect answers were excluded from the analysis. Average response times were computed for each category to assess AILA's overall efficiency. To assess the evaluation of questions in terms of accuracy, we classified the answers provided by AILA into three categories: fully correct answers, incorrect and partially correct answers. Given that some questions require manual inspection of the AFM software to verify whether specific parameters are set correctly and whether the AFM image is captured as intended, multiple researchers were involved in verifying the results. They carefully checked the outcomes to ensure error-free results. For measurements of different properties, such as average friction, roughness, and RMS value of roughness, we used the Gwyddion software to verify the accuracy of the results. Subsequently, the questions were clustered into appropriate groups, and the corresponding average percentage of correct answers was calculated. Additional evaluation metrics are defined as following. For all the successful runs, following metrics are employed. AFM handler calls: the average number of times the AFM Handler agent was called to resolve a task (calls per task). An elevated score may signify a greater dependence on the AFM Handler for coordination. Data handler calls: the average number of times the Data Handler agent was called during task solving. Number of steps: the average number of discrete actions (represented by tool or agent call) taken by the AILA to arrive at a solution. Prompt tokens: number of tokens utilized in the conversation's input or instruction segment for each task. Tokens per stage: this tells how many tokens were utilized on average for each stage in the process of solving a task. Time per step: the average amount of time (in seconds) spent on each step, representing the pace with which the system can perform different tasks. We systematically investigated how prompt structure and phrasing influence GPT-4o's ability to perform load-dependent friction measurements, one of our most challenging open-ended tasks. LLMs process information hierarchically based on input format and length, making them sensitive to prompt design. Even subtle modifications can activate different training exemplars, altering response patterns and reasoning pathways. To evaluate performance within the AILA framework, we constructed multiple input prompts to assess task completion efficacy. In AILA, inter-agent collaboration is triggered when the model outputs “NEED HELP”, while “FINAL ANSWER” signals task completion. We developed and tested four distinct prompt categories: (1) concise task descriptions, (2) comprehensive task elaborations, (3) sequential task decompositions, and (4) explicit inclusions of the signaling phrases with case variations. By systematically combining these elements, we designed five system prompts, with detailed performance metrics documented in the Supplementary Information (Table S3). All the data generated in this study have been deposited in the GitHub repository37 under accession code https://github.com/M3RG-IITD/AILA. Source data are provided with this paper. Zhao, Z., Lee, W. S. & Hsu, D. Large language models as commonsense knowledge for large-scale task planning. Lála, J. et al. Paperqa: retrieval-augmented generative agent for scientific research. Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. Szymanski, N. J. et al. An autonomous laboratory for the accelerated synthesis of novel materials. Autonomous mobile robots for exploratory synthetic chemistry. & Gomes, G. Autonomous chemical research with large language models. Sadeghi, S. et al. Engineering a sustainable future: harnessing automation, robotics, and artificial intelligence with self-driving laboratories. Delgado-Licona, F. & Abolhasani, M. Research acceleration in self-driving labs: Technological roadmap toward accelerated materials and molecular discovery. Sim, M. et al. ChemOS 2.0: An orchestration architecture for chemical self-driving laboratories. Darvish, K. et al. ORGANA: a robotic assistant for automated chemistry experimentation and characterization. Abolhasani, M. & Kumacheva, E. The rise of self-driving labs in chemical and materials sciences. & Abolhasani, M. Reproducibility in automated chemistry laboratories using computer science abstractions. & Abolhasani, M. Performance metrics to unleash the power of self-driving labs in chemistry and materials science. Mirza, A. et al. Are large language models superhuman chemists? Zaki, M., Jayadeva, M. & Krishnan, N. M. A. MaScQA: investigating materials science knowledge of large language models. Alampara, N. et al. Probing the limitations of multimodal language models for chemistry and materials research. Enabling large language models for real-world materials discovery. In Machine Learning for Materials Discovery: Numerical Recipes and Practical Applications (Springer International Publishing, 2024). Mishra, V. et al. Foundational Large Language Models for Materials Research. Li, Z. et al. Mmsci: a multimodal multi-discipline dataset for PhD-level scientific comprehension. in AI for Accelerated Materials Design-Vienna https://doi.org/10.48550/arXiv.2407.04903 (2024). Language agents achieve superhuman synthesis of scientific knowledge. & Noël, T. Autonomous chemistry: Navigating self-driving labs in chemical and material sciences. Tom, G. et al. Self-driving laboratories for chemistry and materials science. Liu, Y. et al. AEcroscopy: a software–hardware framework empowering microscopy toward automated and autonomous experimentation. Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy. Autonomous experiments in scanning probe microscopy and spectroscopy: choosing where to explore polarization dynamics in ferroelectrics. Arias, S., Zhang, Y., Zahl, P. & Hollen, S. Autonomous molecular structure imaging with high-resolution atomic force microscopy for molecular mixture discovery. & Lee, M. Machine learning-enabled autonomous operation for atomic force microscopes. Zaki, M. et al. Interpretable machine learning approach for identifying the tip sharpness in atomic force microscopy. Kalinin, S. V. et al. Probe microscopy is all you need. Liu, Y. et al. Disentangling electronic transport and hysteresis at individual grain boundaries in hybrid perovskites via automated scanning probe microscopy. Liu, Y. et al. Learning the right channel in multimodal imaging: automated experiment in piezoresponse force microscopy. Liu, Y., Checa, M. & Vasudevan, R. K. Synergizing human expertise and AI efficiency with language model for microscopy operation and automated experiment design*. Automation of atomic force microscopy with LLM agents including end-to-end evaluation suite. Huang, L. et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Anwar, U. et al. Foundational challenges in assuring alignment and safety of large language models. Liu, Y. et al. Trustworthy LLMs: a survey and guideline for evaluating large language models' alignment. Size measurement of nanoparticles using atomic force microscopy. in Characterization of Nanoparticles Intended for Drug Delivery (ed. Moreno-Flores, S. Baseline correction of AFM force curves in the force–time representation. Burnham, N. A., Lyu, L. & Poulikakos, L. Towards artefact-free AFM image presentation and interpretation. Towards effective genai multi-agent collaboration: design and evaluation for enterprise applications. Gao, M. et al. Single-agent or Multi-agent Systems? Sreedhar, K. & Chilton, L. Simulating human strategic behavior: comparing single and multi-agent LLMs. thanks University Grants Commission (UGC), Government of India for the NET-JRF fellowship (221610021768). acknowledges support from the European Union (ERC, NewGLASS, 101044664). acknowledges funding from the Carl Zeiss Foundation through its Breakthrough program. We also thank Sushant Sinha for his assistance with the image segmentation. Indrajeet Mandal, Nitya Nand Gosvami & N. M. Anoop Krishnan Department of Materials Science and Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India Leibniz Institute of Photonic Technology, Jena, Germany Yardi School of Artificial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India All authors contributed to reviewing and editing the manuscript. Correspondence to Nitya Nand Gosvami or N. M. Anoop Krishnan. Nature Communications thanks the anonymous reviewers for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Evaluating large language model agents for automation of atomic force microscopy. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/health/a69014450/scientists-found-the-potential-off-button-for-stopping-chronic-pain/'>Scientists Found the Potential Off Button for Stopping Chronic Pain</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 12:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>We may earn commission if you buy from a link. Sometimes, chronic pain doesn't even come with a diagnosis, but is the aftermath of an injury that never fades. Though nearly a quarter of Americans and a fifth of people overall live with chronic pain, its neural source has remained elusive. Only some groups of brain cells that are activated during bouts of pain had been identified in the central and peripheral nervous systems. But now, after a series of experiments on mice, biologist Nicholas Betley from the University of Pennsylvania has discovered that a previously overlooked group of neurons in the parabrachial nucleus  is switched on in individuals with chronic pain. He and his research team also found that there is a molecule that the brain can release for pain relief. “Populations of neurons in the lPBN […] respond to acute [pain-related] stimuli, and prolonged activation of excitatory lPBN neurons can drive a chronic pain-like state.” Known as Y1R neurons, they transmit sensory information (such as taste and temperature) in addition to pain, also regulating appetite and threat signals. While Y1R neurons exist in various subpopulations, what they have in common is the expression of receptors for neuropeptide Y or NPY, which regulates types of brain activity like metabolism, heart rate, immune function, blood pressure, and even stress reduction. Other studies that involved monitoring neural activity and running computer models showed a link to chronic pain. Betley and his research team used painful stimuli (such as exposure to a hot surface) to activate Y1R neurons in mice, and as they predicted, the mice showed behaviors associated with persistent pain, such as paw licking. While Y1R neurons do not directly cause pain, they are thought to be involved in a larger network that is behind painful sensations. Something else the researchers wanted to see was whether pain took precedence over survival. They temporarily deprived the mice of food and water and also introduced a frightening stimulus (like the scent of a predator), and when faced with these perceived threats to their survival, the mice experienced significantly less pain. It turned out that production of neuropeptide Y increased in the parabrachial nucleus when the animals were faced with something that could literally mean life or death. Betley's findings could mean that someday, a drug that signals neurons to produce more NPY could be a viable and safe alternative to painkillers such as opioids. “This mechanism is an efficient and tunable system that enables urgent needs to shift brain state away from pain and towards other states that promote survival,” he said. “Future work could build from these findings to test other endogenous or pharmacological interventions that curb pain state.” Her work has appeared in Popular Mechanics, Ars Technica, SYFY WIRE, Space.com, Live Science, Den of Geek, Forbidden Futures and Collective Tales. She lurks right outside New York City with her parrot, Lestat. When not writing, she can be found drawing, playing the piano or shapeshifting. An Elusive Rat Finally Showed Its Face to Science</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41559-025-02868-4'>A Carnian theropod with unexpectedly derived features during the first dinosaur radiation</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 10:48:40
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Nature Ecology & Evolution (2025)Cite this article The early diversification of dinosaurs produced a major ecological change in the terrestrial ecosystems, culminating with tetrapod assemblages dominated in abundance by dinosaurs by the Triassic/Jurassic boundary (~201 million years ago (Ma)). Therefore, studying the initial diversification of dinosaurs is crucial to understand the establishment of Mesozoic assemblages. However, the lack of stratigraphically continuous fossil data in the few geological units that preserve the oldest known dinosaurs (~233–227 Ma, Carnian age) obscures our understanding of this initial diversification. The Ischigualasto Formation in northwestern Argentina (231.4–225.9 Ma) yields a rich vertebrate assemblage and new studies resulted in an abundant and stratigraphically near-continuous fossil record, which offers new insights into the early diversification of dinosaurs. Among the discoveries, we report Anteavis crurilongus gen. et sp. nov., an early-diverging theropod, which supports the notable diversity of small- to medium-sized dinosaurs during the late Carnian. Anteavis is recovered outside Neotheropoda, but it has features previously thought to be exclusive to that group. We show that dinosaur diversity and abundance in the Ischigualasto Formation were higher than previously recognized, particularly among small herbivores (<30 kg) and medium-sized (30–200 kg) predators. This diversification occurred in Ischigualasto during a climatic shift to semi-arid conditions, but the return of more humid conditions resulted in a gap in the dinosaur record that started at 228.91 ± 0.14 Ma. Only 15 million years (Myr) later, in the middle Norian age, the dinosaur record recovered its abundance and diversity in the basin, but now it was characterized by larger-bodied species. Our findings demonstrate an early dinosaur diversification probably punctuated by a climate-driven faunal turnover in, at least, southwestern Pangaea. This is a preview of subscription content, access via your institution Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription cancel any time Subscribe to this journal Receive 12 digital issues and online access to articles $119.00 per year only $9.92 per issue Buy this article Prices may be subject to local taxes which are calculated during checkout All data generated or analysed during this study are included in this article and its Supplementary Information. The datasets for phylogenetic and rarefaction analyses are available via figshare at https://doi.org/10.6084/m9.figshare.29539004 (ref.61). The custom codes used for the phylogenetic and rarefaction analyses are available via figshare at https://doi.org/10.6084/m9.figshare.29539004 (ref. Nesbitt, S. J. et al. Ecologically distinct dinosaurian sister group shows early diversification of Ornithodira. Martínez, R. N. et al. A basal dinosaur from the dawn of the dinosaur era in southwestern Pangaea. Article PubMed Nesbitt, S. J., Barrett, P. M., Werning, S., Sidor, C. A. & Charig, A. J. The oldest dinosaur? A Middle Triassic dinosauriform from Tanzania. Heath, J. A., Cooper, N., Upchurch, P. & Mannion, P. D. Accounting for sampling heterogeneity suggests a low paleolatitude origin for dinosaurs. Brusatte, S. L. et al. The origin and early radiation of dinosaurs. Earth Sci. Langer, M. C., Ezcurra, M. D., Bittencourt, J. S. & Novas, F. E. The origin and early evolution of dinosaurs. Article PubMed Langer, M. C. L., Ramezani, J. & Da Rosa, A. A. S. U-Pb age constraints on dinosaur rise from south Brazil. Griffin, C. T. et al. Africa's oldest dinosaurs reveal early suppression of dinosaur distribution. Lovelace, D. M. et al. Rethinking dinosaur origins: oldest known equatorial dinosaur-bearing assemblage (mid-late Carnian Popo Agie FM, Wyoming, USA). Bakker, R. Anatomical and ecological evidence of endothermy in dinosaurs. Charig, A. J. Competition between therapsids and archosaurs during the Triassic period: a review and synthesis of current theories. Benton, M. J. Late Triassic extinctions and the origin of the dinosaurs. Benton, M. J. Dinosaur success in the Triassic: a noncompetitive ecological model. Irmis, R. B. et al. A Late Triassic dinosauromorph assemblage from New Mexico and the rise of dinosaurs. Ezcurra, M. D. A new early dinosaur (Saurischia: Sauropodomorpha) from the Late Triassic of Argentina: a reassessment of dinosaur origin and phylogeny. Bernardi, M., Gianolla, P., Petti, F. M., Mietto, P. & Benton, M. J. Dinosaur diversification linked with the Carnian Pluvial Episode. Benton, M. J., Bernardi, M. & Kinsella, C. The Carnian Pluvial Episode and the origin of dinosaurs. Schoepfer, S. D., Algeo, T. J., van de Schootbrugge, B. & Whiteside, J. H. The Triassic–Jurassic transition—a review of environmental change at the dawn of modern life. Earth Sci. Qvarnström, M. et al. Digestive contents and food webs record the advent of dinosaur supremacy. Corecco, L., Kohn, M. J. & Schultz, C. L. Triassic climate and the rise of the dinosaur empire in South America. Earth Sci. Novas, F. E., Ezcurra, M. D., Chatterjee, S. & Kutty, T. S. New dinosaur species from the Upper Triassic Upper Maleri and Lower Dharmaram formations of central India. Earth Environ. Martínez, R. N. et al. Vertebrate succession in the Ischigualasto Formation. Sereno, P. C. & Novas, F. E. The complete skull and skeleton of an early dinosaur. Sereno, P. C., Forster, C. A., Rogers, R. R. & Monetta, A. M. Primitive dinosaur skeleton from Argentina and the early evolution of Dinosauria. Alcober, O. & Martínez, R. N. A new herrerasaurid (Dinosauria, Saurischia) from the Upper Triassic Ischigualasto Formation of northwestern Argentina. Martínez, R. N. & Alcober, O. A basal sauropodomorph (Dinosauria: Saurischia) from the Ischigualasto Formation (Triassic, Carnian) and the early evolution of Sauropodomorpha. PLoS ONE 4, e4397 (2009). Sereno, P. C., Martínez, R. N. & Alcober, O. A. Osteology of Eoraptor lunensis (Dinosauria, Sauropodomorpha). Rogers, R. R. et al. The Ischigualasto tetrapod assemblage (Late Triassic, Argentina) and 40Ar/39Ar dating of dinosaur origins. Desojo, J. The Late Triassic Ischigualasto Formation at Cerro Las Lajas (La Rioja, Argentina): fossil tetrapods, high-resolution chronostratigraphy, and faunal correlations. Colombi, C. et al. A high-precision U-Pb zircon age constraints the timing of the faunistic and palynofloristic events of the Carnian Ischigualasto Formation, San Juan, Argentina. Earth Sci. Tabor, N. et al. in Paleoenvironmental Record and Applications of Calcretes and Palustrine Carbonates Vol. 416 (eds Alonso-Zarza, A. & Tanner, L.) 17–41 (Geological Society of America, 2006). Currie, B., Colombi, C., Tabor, N., Shipman, T. & Montañez, I. Stratigraphy and architecture of the Upper Triassic Ischigualasto Formation, Ischigualasto Provincial Park, San Juan, Argentina. Earth Sci. Colombi, C. E., Limarino, C. O. & Alcober, O. A. Allogenic controls on the fluvial architecture and fossil preservation of the Upper Triassic Ischigualasto Formation, NW Argentina. Mancuso, A. C. et al. Paleoenvironmental and biotic changes in the Late Triassic of Argentina: testing hypotheses of abiotic forcing at the basin scale. Earth Sci. Colbert, E. H. The Triassic dinosaur Coelophysis. The Anatomy of the Triassic Theropod Syntarsus rhodesiensis (Saurischia: Podokesauridae) and a Consideration of its Biology (Rhodes University, 1977). Langer, M. C., McPhee, B. W., Marsola, J. C. A., Roberto-da-Silva, L. & Cabreira, S. F. Anatomy of the dinosaur Pampadromaeus barberenai (Saurischia—Sauropodomorpha) from the Late Triassic Santa Maria Formation of southern Brazil. PLoS ONE 14, e0212543 (2019). Nesbitt, S. J. et al. A complete skeleton of a Late Triassic saurischian and the early evolution of dinosaurs. Sereno, P. C. The pectoral girdle and forelimb of the basal theropod Herrerasaurus ischigualastensis. Rowe, T. A new species of the theropod dinosaur Syntarsus from the Early Jurassic Kayenta Formation of Arizona. Nesbitt, S. J. & Ezcurra, M. D. The early fossil record of dinosaurs in North America: a new neotheropod from the base of the Upper Triassic Dockum Group of Texas. Mancuso, A. C., Benavente, C. A., Irmis, R. B. & Mundil, R. Evidence for the Carnian Pluvial Episode in Gondwana: new multiproxy climate records and their bearing on early dinosaur diversification. Dal Corso, J. et al. Extinction and dawn of the modern world in the Carnian (Late Triassic). Colombi, C. E., Rogers, R. R. & Alcober, O. A. Vertebrate taphonomy of the Ischigualasto Formation. Valdes, P. J., Scotese, C. R. & Lunt, D. J. Deep ocean temperatures through time. Santi Malnis, P., Colombi, C. E., Rothis, L. M. & Alcober, O. Fluvial architecture and paleoenvironmental evolution of the Los Colorados Formation (Norian): postrift stage of the Ischigualasto–Villa Unión Basin, NW Argentina. Apaldetti, C., Martínez, R. N., Cerda, I. A., Pol, D. & Alcober, O. An early trend towards gigantism in Triassic sauropodomorph dinosaurs. Article PubMed Barrett, P. M., Chapelle, K. E. J., Staunton, C. K., Botha, J. & Choiniere, J. N. Postcranial osteology of the neotype specimen of Massospondylus carinatus Owen, 1854 (Dinosauria: Sauropodomorpha) from the upper Elliot formation of South Africa. Waskow, K. & Sander, P. M. Growth record and histological variation in the dorsal ribs of Camarasaurus sp. Waskow, K. & Mateus, O. Dorsal rib histology of dinosaurs and a crocodylomorph from western Portugal: skeletochronological implications on age determination and life history traits. Cerda, I. A. et al. A basic guide for sampling and preparation of extant and fossil bones for histological studies. Francillon-Vieillot, H. et al. in Skeletal Biomineralization: Patterns, Processes and Evolutionary Trends (ed. Carter, J. G.) 471–548 (Van Nestrand Reinhold, 1990). de Buffrénil, V. & Quilhac, A. in Comparative Skeletal Histology and Palaeohistology (eds de Buffrénil, V. et al.) 147–190 (CRC Press, 2021). Ezcurra, M. D., Marke, D., Walsh, S. A. & Brusatte, S. L. A revision of the ‘coelophysoid-grade' theropod specimen from the Lower Jurassic of the Isle of Skye (Scotland). Garcia, M. S., Cabreira, S. F., da Silva, L. R., Pretto, F. A. & Müller, R. T. A saurischian (Archosauria, Dinosauria) ilium from the Upper Triassic of southern Brazil and the rise of Herrerasauria. Norman, D. B., Baron, M. G., Garcia, M. S. & Müller, R. T. Taxonomic, palaeobiological and evolutionary implications of a phylogenetic hypothesis for Ornithischia (Archosauria: Dinosauria). & Morales, M. E. TNT version 1.6, with a graphical interface for MacOS and Linux, including new routines in parallel. Article PubMed Goloboff, P. A., Torres, A. & Arias, J. S. Weighted parsimony outperforms other methods of phylogenetic inference under models appropriate for morphology. Article PubMed Ezcurra, M. D. Exploring the effects of weighting against homoplasy in genealogies of palaeontological phylogenetic matrices. Article PubMed Spiekman, S. N. F., Ezcurra, M. D., Butler, R. J., Fraser, N. C. & Maidment, S. C. R. Pendraig milnerae, a new small-sized coelophysoid theropod from the Late Triassic of Wales. Open Sci. Martínez, R. N. et al. Supplementary Information of A Carnian theropod with unexpectedly derived features during the first dinosaur radiation. figshare https://doi.org/10.6084/m9.figshare.29539004 (2025). Ronquist, F., van der Mark, P. & Huelsenbeck, J. P. in The Phylogenetic Handbook: a Practical Approach to Phylogenetic Analysis and Hypothesis Testing (eds Lemey, P. et al.) 210−266 (Cambridge Univ. Ezcurra, M. D. & Butler, R. J. The rise of the ruling reptiles and ecosystem recovery from the Permo-Triassic mass extinction. Ezcurra, M. D., Scheyer, T. M. & Butler, R. J. The origin and early evolution of Sauria: reassessing the Permian saurian fossil record and the timing of the crocodile-lizard divergence. PLoS ONE 9, e89165 (2014). Rambaut, A., Drummond, A. J., Xie, D., Baele, G. & Suchard, M. A. Posterior summarisation in Bayesian phylogenetics using Tracer 1.7. Benson, R. B. et al. Rates of dinosaur body mass evolution indicate 170 million years of sustained ecological innovation on the avian stem lineage. PLoS Biol. Apaldetti, C., Pol, D., Ezcurra, M. D. & Martínez, R. N. Sauropodomorph evolution across the Triassic–Jurassic boundary: body size, locomotion, and their influence on morphological disparity. Ezcurra, M. D. A new early coelophysoid neotheropod from the Late Triassic of northwestern Argentina. Campione, N. E. MASSTIMATE: body mass estimation equations for vertebrates. R package version 2.0-1 (CRAN, 2020); https://CRAN.R-project.org/package=MASSTIMATE Campione, N. E., Evans, D. C., Brown, C. M. & Carrano, M. T. Body mass estimation in non-avian bipeds using a theoretical conversion to quadruped stylopodial proportions. Methods Ecol. Revell, L. J. phytools 2.0: an updated R ecosystem for phylogenetic comparative methods (and other things). Bapst, D. W. paleotree: an R package for paleontological and phylogenetic analyses of evolution. Methods Ecol. Laurin, M. The evolution of body size, Cope's rule and the origin of amniotes. Article PubMed Oksanen, J. vegan: community ecology package. R package version 2.6-4 (CRAN, 2022); https://CRAN.R-project.org/package=vegan Kent, D. V., Santi Malnis, P., Colombi, C. E., Alcober, O. & Mart¡nez, R. N. Age constraints on the dispersal of dinosaurs in the Late Triassic from magnetochronology of the Los Colorados Formation (Argentina). Natl Acad. Marsicano, C., Irmis, R., Mancuso, A., Mundil, R. & Chemale, F. The precise temporal calibration of dinosaur origins. Ezcurra, M. D. et al. Deep faunistic turnovers preceded the rise of dinosaurs in southwestern Pangaea. Article PubMed Aguirre Palafox, L. E. et al. U-Pb geochronology of paleosol carbonate cements by LA-ICP-MS: A proof of concept and strategy for dating the terrestrial record. Download references thanks Secretaría de Ciencia, Tecnología e Innovación of San Juan (SECITI) and IMCN of the Universidad Nacional de San Juan. This study used computational resources from Universidad Nacional de Córdoba (https://ccad.unc.edu.ar/), which are part of SNCAD–MinCyT, Argentina. We also thank the Willi Hennig Society for supporting the free use of TNT software. Instituto y Museo de Ciencias Naturales, Universidad Nacional de San Juan, San Juan, Argentina Ricardo N. Martínez, Carina E. Colombi, Diego O. Abelín & Oscar A. Alcober CIGEOBIO, CONICET, San Juan, Argentina Carina E. Colombi, Diego O. Abelín & Ignacio Cerda Sección Paleontología de Vertebrados, CONICET–Museo Argentino de Ciencias Naturales ‘Bernardino Rivadavia', Buenos Aires, Argentina School of Geography, Earth and Environmental Sciences, University of Birmingham, Birmingham, UK Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar led the project, conducted phylogenetic analyses, results, discussions and paper writing. conducted palaeoenvironmental and taphonomic studies. conducted phylogenetic and rarefaction analyses. conducted the histology analysis. produced the figures. performed specimen preparation. contributed to logistics. Correspondence to Ricardo N. Martínez. The authors declare no competing interests. Nature Ecology & Evolution thanks Steve Brusatte and Sterling Nesbitt for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. a, Location maps of the study area. b, Geological map of the southern area of the Ischigualasto Basin (Hoyada de Ischigualasto). c, Ischigualasto Formation stratigraphic section and detailed sedimentological log of the Hyperodapedon-Exaeretodon-Herrerasaurus biozone. The detailed sedimentologic section of the study area corresponds to a 55 metre-thick interval that includes lithofacies characterization (that is, texture, structure, colors, paleosol features) and the stratigraphic position of the holotype of Anteavis crurilongus. Black star indicates the type locality of Anteavis crurilongus (PVSJ 1085). Complete transverse sections of the rib (a,b); general view (c) and detail (d) of the cortical bone; left prefrontal in lateral (e) and medial (f) views; partial left maxilla in lateral (g, h), occlusal (i, j), and medial (k, l) views; palatine ramus of left pterygoid in dorsal (m), and ventral (n) views; medial portion of the right pterygoid in dorsal (o), and ventral (p) views; right ectopterygoid in dorsal (q), and ventral (r) views; anterior portion of the right dentary in medial (s) and lateral (t) views; fragments of the posterior portion of the right dentary in lateral (u, v), and medial (w, x) views; and posterior portion of the right dentary in occlusal view (y). White arrowheads indicate the position of the lines of arrested growth (LAGs). Due to differences in the degree of magnification of the photographs, LAGs formed in the external fundamental system are only indicated in d. Abbreviations: a, alveolus; af, antorbital fossa; ap, anterior process; bf, basipterygoid flank; dp, dorsal process; EFS, external fundamental system; jpe, jugal process of the ectopterygoid; ecter, ectopterygoid recess; f, foramen; mg, Meckelian groove; ml, medial lamina; mr, mandibular ramus; os, orbital surface; pf, pterygoid fold; pmf, promaxillary foramen; pr, palatine ramus; r, ridge; s, symphysis; sg, groove for the splenial; slb, secondary lamellar bone; so, secondary osteon; svc, simple vascular canal; t, tooth; vp, ventral process. Scale bars: a-b 1 mm, c 0.5 mm, d 0.2 mm, e-y 10 mm. Anterior cervical vertebra in right lateral (a), left lateral (b), dorsal (c), and ventral (d) views; mid-cervical vertebra in right lateral (e), left lateral (f), dorsal (g), and ventral (h) views; posterior cervical vertebra in right lateral (i), left lateral (j), dorsal (k), and ventral (l) views; anterior dorsal vertebra in right lateral (m), left lateral (n), dorsal (o), and ventral (p) views; mid-dorsal vertebra in right lateral (q), left lateral (r), dorsal (s), and ventral (t) views; distal anterior caudal vertebra in right lateral (u), left lateral (v), dorsal (w), and ventral (x) views; mid-caudal vertebra in right lateral (y), left lateral (z), dorsal (aa), and ventral (ab) views; posterior caudal vertebra in right lateral (ac), left lateral (ad), dorsal (ae), and ventral (af) views. Scale bar: 10 mm. Right scapula and coracoid in lateral (a), medial (b), and posterior (c) views; right humerus in anterior and slightly medial (d), posterior and slightly lateral (e), proximal (f), lateral (g), and medial (h) views; proximal end of the right ulna in lateral (i), medial (j), and proximal (k) views; distal end of the right ulna in lateral (l), medial (m), and proximal (n) views; right manual phalanx in dorsal (o), ventral (p), lateral (q), and medial (r) views; left femur in lateral (s), medial (t), anterior (u), posterior (v), proximal (w), and distal (x) views; right tibia in lateral (y), medial (z), anterior (aa), posterior (ab), proximal (ac), and distal (ad) views; left astragalus in proximal (ae), distal (af), anterior (ag), posterior (ah), medial (ai), and lateral (aj) views; detail of the groove in the left astragalus in proximal view (ak); left metatarsal I in dorsal (al) and medial (am) views; left metatarsal II in dorsal (an) and lateral (ao) views; left metatarsals III and IV and left phalanx 1 of digit I in dorsal (ap) and lateral (aq) views; left metatarsal V in dorsal (ar), ventral (as), and proximal (at) views; distal tarsal 3 in proximal (au), and distal (av) views; distal tarsal 4 in proximal (aw), and distal (ax) views. Abbreviations: aaf, surface for astragalar ascending process; ae, anterior expansion; afh, articular facet for the humerus; amc, anteromedial corner; amt, anteromedial tuber; alt, anterolateral tuber; ap, ascending process; at, anterior tuberosities; bt, biceps tuber; c, coracoid; cc, cnemial crest; cco, calcaneal concavity; cf, coracoid foramen; ct, crista tibiofibularis; dip, dorsal intercondylar process; dt, dorsal tuberosity; dvd, dorsoventral depression; dpc, deltopectoral crest; ect, ectepicondyle; ent, entepicondyle; f, foramen; fc, fibular crest; ff, fibular facet; ft, fourth trochanter; g, glenoid; gr, groove; gt, greater trochanter; hh, humeral head; lc, lateral condyle; lclp, lateral collateral ligament pit; lf, lateral flange; llp, lateral ligament pit; ls, ligament scar; lt, lesser trochanter; mc, medial condyle; mlp, medial ligament pit; mp, medial protuberance; mt, medial tuberosity; n, notch; pp, post-glenoid process; r, ridge; rc, radial condyle; rf, fossa for articulation with the radius; s, scapula; sr, glenoidal sharp rim; ol, olecranon; ot, ovoid tuberosity; plf, posterolateral flange; pf, posterior fossa; pt, posterior tuberosities; rf, radial fossa; t, tuberosity; tf, tibial facet; ts, trochanteric shelf; uaf, articular surface for the ulnare; uc, ulnar condyle; vip, ventral intercondylar process. Scale bars: a–e g–r 10 mm, e 20 mm, s-ad ak-as 50 mm, ae-aj al-ax 10 mm, ak 10 mm. Pelvis in right lateral (a), left lateral (b), anterior (c), posterior (d), dorsal (e), and ventral (f) views; dorsosacral vertebra in left lateral (g), dorsal (h), ventral (i), anterior (j), and posterior (k) views; caudosacral vertebra in left lateral (l), dorsal (m), ventral (n), anterior (o), and posterior (p) views; close-up of the preacetabular process in anteromedial view (q); close-up of the postacetabular process in posteroventral and slightly lateral view (r); close-up of the sacrum in posteroventral view (s). Abbreviations: at, antitrochanter; bf, brevis fossa; bo, bony overgrow; dt, dorsal tuberosity; fcs, caudosacral rib facet; fds, dorsosacral rib facet; fs, fused sacral primordial vertebrae; il, ilium; ip, ischial peduncle; is, ischium; n, notch; na, neural arch; nc, neural channel; ns, neural spine; lp, left pubis; lprp, left preacetabular process; lpup, left pubic peduncle; of, obturator foramen; pf, pubic fenestra; pfo, pubic foot; pop, postacetabular process; pp, puboischiadic plate; pup, pubic peduncle; rp, right pubis; rpup, right pubic peduncle; rprp, right preacetabular process; sac, supraacetabular crest; t, tuberosity; 2 Sv, second sacral vertebra; 2 Sv, third sacral vertebra. Scale bars: a–f 50 mm, g,p 10 mm, q-s 10 mm. (a) Global strict consensus tree recovered in the parsimony analyses under implied weighting; absolute (left) and GC (group present/contradicted) (right) symmetric resampling frequencies are shown above each branch. (b) Majority rule consensus tree recovered from the Bayesian phylogenetic analysis; numbers at nodes indicate posterior probabilities and branch colours indicate character state transition rates (that is, evolutionary rates). (a) Global strict consensus tree recovered in the parsimony analyses under implied weighting; absolute (left) and GC (group present/contradicted) (right) symmetric resampling frequencies are shown above each branch. (b) Majority rule consensus tree recovered from the Bayesian phylogenetic analysis; numbers at nodes indicate posterior probabilities and branch colours indicate character state transition rates (that is, evolutionary rates). Geological and palaeontological settings; holotypic specimen of A. crurilongus; extended diagnosis of A. crurilongus; detailed description of A. crurilongus; maturity assessment of PVSJ 1085; comparisons between Anteavis and Eodromaeus; phylogenetic analyses; body mass optimizations; rarefaction diversity curves; and references. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions Martínez, R.N., Colombi, C.E., Ezcurra, M.D. et al. A Carnian theropod with unexpectedly derived features during the first dinosaur radiation. Nat Ecol Evol  (2025). Received: 04 June 2025 Accepted: 04 September 2025 Published: 14 October 2025 DOI: https://doi.org/10.1038/s41559-025-02868-4 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Nature Ecology & Evolution (Nat Ecol Evol) © 2025 Springer Nature Limited Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-03307-x'>Faulty mitochondria cause deadly diseases: fixing them is about to get a lot easier</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 10:36:36
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). Gemma Conroy is a freelance science journalist based in Mexico City. CRISPR gene editing has made its way into every corner of modern biology, but not into every corner of the cell. Although researchers have used these systems to develop treatments for sickle-cell anaemia and blood cancers, to unlock the secrets of multicellularity and to discover the role of thousands of overlooked proteins, there's one place CRISPR can't easily reach: mitochondria. Cancer cells get power boost by stealing mitochondria from nerves Cancer cells get power boost by stealing mitochondria from nerves The rings of DNA inside mitochondria are inaccessible to these techniques, which means that precise edits to mitochondrial DNA (mtDNA) remain frustratingly out of reach. But researchers are eager to access this DNA, says Minczuk. Mitochondria are bean-shaped organelles that power cells and have myriad other cellular tasks. Exploring their DNA is essential for understanding the energy production and exchange that underlies metabolic health. And more than 300 mutations in this DNA cause mitochondrial diseases — incurable genetic disorders with a wide range of symptoms that can rob people of their sight and hearing, trigger muscle problems and spark seizures1. Because CRISPR can't help with these problems, researchers have been looking for other ways to precisely edit the mitochrondrial genome. And the past few years have brought some success: the tools are already proving to be a boon for creating accurate animal models of mitochondrial diseases. “The progress has been remarkable,” says Jin-Soo Kim, a chemical biologist who develops mtDNA editing tools at the Korea Advanced Institute of Science and Technology in Daejeon, South Korea. If researchers can make mtDNA editing safe and accurate enough, it could eventually be used to treat, and even cure, these genetic conditions. “It would be a medical breakthrough,” says Kim. The exact origins of mitochondria are murky, but the leading theory holds that the organelle's story started around 1.5 billion years ago when a single-celled microorganism called an archaeon gobbled up a roaming bacterium that survived inside its host. In the evolutionary lineage that gave rise to humans and other animals, this genetic transfer whittled the resident bacterium's genome down to just 37 genes that code for 13 proteins involved in energy production, turning it into a specialized organelle. For a start, mtDNA is typically inherited solely from the mother. There can be several copies of mtDNA in each mitochondrion, and the organelle has its own built-in machinery for making RNA and proteins from that DNA. Mitochondrial DNA is also much more error-prone, with a mutation rate estimated to be 10–20 times greater than that of nuclear DNA. This is in part because it has to contend with a barrage of damaging reactive oxygen species — unstable molecules that are generated in mitochondria during normal energy production. Compared with its counterpart in the nucleus, mtDNA's toolkit for repairing itself is rudimentary. The nucleus is quick to fix a snapped DNA strand using an arsenal of repair mechanisms, but mitochondria can mend only some defects. They often simply throw away their broken DNA. This difference limits the options for gene-editing tools, because nearly all such tools for nuclear DNA use its inherent repair pathways. It has been notoriously challenging to develop approaches for modifying mitochondrial DNA, says Stephen Ekker, a molecular biologist at the University of Texas at Austin. “Its bacterial origins are revealed when you start trying to edit it,” he says. The most crucial hurdle for scientists trying to tinker with the mitochondrial genome is that it is locked behind a wall of membranes that doesn't allow external nucleic acids to pass into the organelle. More than a decade before CRISPR became a research tool, mitochondria researchers began experimenting with other editing tools that could cross mitochondrial membranes and coax the organelles into ditching their problematic DNA2. Healthy and mutated mtDNA often coexist: a state known as heteroplasmy. It's when the proportion of mutated mtDNA reaches 60–80% in a particular tissue or cell type that mitochondrial diseases manifest3. So, they turned to enzymes called zinc finger nucleases (ZFNs) and transcription activator-like effector nucleases (TALENs) to snip the double-stranded mtDNA. “That's going to make up for what you're destroying,” says Carlos Moraes, a geneticist at the University of Miami in Florida. And even if it did reach the clinic, the technique would be powerless against diseases caused by mutations that are often present in all copies of a person's mtDNA, such as Leber's hereditary optic neuropathy (LHON), a rare condition that causes rapid vision loss. What researchers need are tools that do more than cut DNA but that don't rely on guide RNA. When CRISPR–Cas9 emerged as a tool in 2012, it became the go-to gene editor for all kinds of application. Genetic changes are introduced as the DNA repairs itself. The approach became even more useful in 2016, when David Liu, a chemical biologist at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, and his colleagues introduced a more precise technique called base editing4. Hope, despair and CRISPR — the race to save one woman's life Hope, despair and CRISPR — the race to save one woman's life Although base editing and other CRISPR techniques took off for nuclear DNA, Liu and other research teams couldn't get it working on mtDNA. Because CRISPR's guide RNA doesn't readily pass through a mitochondrion's double membrane, using precise tools on mtDNA remained a pipe dream. “We did not have much success,” says Liu. This enzyme, a deadly weapon against other bacteria, wreaks havoc by ultimately converting base C to T across the bacterial genome5. Mougous, now based at Yale University in New Haven, Connecticut, e-mailed Liu asking whether the enzyme, called DddA, would be of any use to him. “I knew exactly what it might be used for — base editing mtDNA!” says Liu. Liu and his colleagues set out to “tame the beast”. They split DddA into two inactive pieces so that the enzyme would do its handiwork on mtDNA only when the pieces were brought together in a particular orientation. And instead of using guide RNA, Liu and his colleagues modified proteins found in TALENs to direct the DddA segments to their target sequences (see ‘Making the edit'). Hope, despair and CRISPR — the race to save one woman's life ‘Landmark' study: three-person IVF leads to eight healthy children Cancer cells get power boost by stealing mitochondria from nerves Men's brains shrink faster than women's: what that means for Alzheimer's Naked mole rats live for decades — genetic tweaks reveal insights into ageing First proposed blood test for chronic fatigue syndrome: what scientists think x4 Research Fellows - SUSTAIN Section: Smart Electronic Materials & Systems Location: Highfield Campus Salary: £36,636 to £44,746 per annum Full-ti... Arc Institute's Science Fellows program is for early-career scientists who are seeking to transition to a PI position directly after their doctorate. Hope, despair and CRISPR — the race to save one woman's life ‘Landmark' study: three-person IVF leads to eight healthy children Cancer cells get power boost by stealing mitochondria from nerves An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/three-anti-inflammatory-supplements-can-really-fight-disease-according-to/'>Many Pills and Powders Claim to Suppress Inflammation and Disease. Three Actually Can</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 10:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Experts say the strongest scientific studies identify three compounds that fight disease and inflammation Or it can be a longer-lasting and more damaging affliction—chronic, low-grade inflammation that lingers in the body for years without obvious symptoms, silently harming cells. A steady stream of studies has connected this type of chronic inflammation to many serious conditions, including Alzheimer's, heart disease, some cancers, and autoimmune illnesses such as lupus. These findings have begun to reframe how scientists think about disease and some of its causes. They've also created a booming market for supplements promising to lower chronic inflammation. Although thousands of products claim to “support immunity” or “reduce inflammation,” most lack solid evidence. Chronic inflammation is damaging because it involves immune system cells and proteins that typically fight short-term battles against bacteria, viruses, and other pathogens. But when these immune system components stay activated for years, they begin to hurt healthy cells and organs. They are intended to break down invading microbes, but over time their ongoing activity can harm blood vessels, for instance, by damaging normal cells that make up the vessels' inner linings or promoting the growth of plaques. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. We reviewed dozens of studies and spoke with researchers to find out whether any supplements demonstrate anti-inflammatory activity not just in laboratory animals and cultured cells but in human trials. Just three compounds, it turns out, have good evidence of effectiveness: omega-3 fatty acids, curcumin and—in certain ailments—vitamin D. We looked for consistent results across several studies that scientists described as large and well designed. These include C-reactive protein (CRP), a molecule produced by the liver when inflammation is active, and cytokines, which are chemical messengers such as interleukin-6 (IL-6) and tumor necrosis factor alpha (TNF-α), both secreted by immune and fat cells. This complexity makes it difficult to prove that any supplement works consistently. The compounds that do show promise will not cure cancer or halt dementia. Among the hundreds of supplements tested for their effects on human health, omega-3 fatty acids are supported by some of the most compelling evidence. Multiple studies suggest that omega-3 supplements can reduce markers of chronic inflammation, Hu says, especially among people with underlying health conditions. A large, carefully controlled trial called VITAL (officially the Vitamin D and Omega-3 Trial), which followed more than 25,000 adults for about five years, found that omega-3 supplements slightly reduced CRP in people who rarely ate fish—fish is a natural omega-3 source, so these people were getting almost all their omega-3s from the supplements. Smaller trials have suggested that omega-3 supplementation can reduce certain markers of inflammation—TNF-α, IL-6, CRP and IL-8—especially in people with conditions such as heart failure, Alzheimer's and kidney disease. Taking omega-3 fatty acid supplements was associated with a 40 percent reduction in heart attacks among people in a trial who ate the least amount of fish. But the evidence across various trials is hard to compare. “There is still a question regarding which is the optimal dose and the optimal duration because different studies have used different doses,” Hu says. And in healthy people, who have low baseline inflammation, there might be little room for improvement. Rigorous trials have debunked the once popular idea that vitamin D is a wonder drug for everything from breast cancer to diabetes. For a few autoimmune conditions, however, the vitamin can be helpful. In the VITAL trial, people who took vitamin D daily for five years had a 22 percent lower risk of developing autoimmune diseases such as rheumatoid arthritis, psoriasis and lupus. “High-dose vitamin D has the effect of tamping down inflammation,” Manson says. “So conditions that are really directly related to inflammation may benefit.” Lab studies have suggested that vitamin D may interfere with molecular pathways involved with inflammation, in addition to suppressing the production of proinflammatory cytokines. In one small study of women with type 2 diabetes, a high dose—50,000 international units (IU) every two weeks—reduced CRP. A separate study in women with polycystic ovary syndrome (PCOS) found that a combination of vitamin D and omega-3 fatty acids helped to lower CRP levels. And two analyses that grouped together results from several studies back up the idea that the vitamin can cause a significant, though small, reduction in CRP. Another trial in women with PCOS found that a daily dose of 3,200 IU of the vitamin improved patients' insulin sensitivity and liver function. Whether that two-year dip in inflammation translates into long-term benefits remains unclear, the researchers note. Even then, the findings may also depend on baseline levels. Most people in the VITAL study started with normal levels of vitamin D, Manson says. “People who are already getting reasonable intake may not benefit further from the supplement,” she says. A review of other trials looking at inflammation-related biomarkers such as CRP, IL-6 and TNF-α found that vitamin D supplementation at several different doses didn't have a big effect. (The recommended daily vitamin D intake for adults is 600 IU.) But high doses carry their own risks, such as too much calcium in the blood. Inflammation is central to illnesses such as rheumatoid arthritis, says Arthur M. Mandelin II, a rheumatologist at Northwestern University's Feinberg School of Medicine, but he is interested in vitamin D only as a therapy for patients with demonstrated deficiencies. The pigment that gives turmeric its yellow color, curcumin, is another promising compound for fighting chronic inflammation. Funk's review found that the most convincing evidence for curcumin's anti-inflammatory activity was among small clinical trials. People in those trials had preexisting conditions such as metabolic disorders and osteoarthritis. In a few cases, curcumin's effects resembled those of over-the-counter anti-inflammatory drugs such as ibuprofen. A large Canadian trial found no measurable benefit for inflammation in people who were taking curcumin after surgery, and other trials have been inconclusive. Some supplement manufacturers encase curcumin in nanoparticles to improve its absorption, but these formulations aren't always used in clinical trials, nor are they consistently available over the counter. Some commercial turmeric and curcumin powders have even been found to contain harmful contaminants such as lead. “People buy turmeric powder based on its color,” Funk says. “Partly to make it a more beautiful color, [manufacturers] add lead chromate.” Other compounds such as flavanols in green tea and dark chocolate or resveratrol in red wine are often promoted as anti-inflammatory agents. They can be hard for the body to absorb, which limits their effectiveness. And even though a recent trial of cocoa flavanols found a promising effect on cardiovascular health, possibly because of reduced inflammation, any benefit might be outweighed by the many extra calories one would consume if they got the compounds by eating chocolate. The U.S. Food and Drug Administration doesn't require supplement companies to prove that their products improve health, unlike pharmaceuticals. So there's little financial incentive for these companies to run rigorous clinical trials because, as Funk asks, “What if they find out it doesn't work?” Supplement ingredients can vary from batch to batch, especially for botanically derived products, in which concentrations depend on where the plants are grown and how the crucial components are extracted. Even when trials are well designed, they can come up against ethical challenges. “You cannot really preselect people on the basis of being deficient or profoundly deficient in these essential vitamins,” Manson says, “because once you identify them as being profoundly deficient, you really should be treating them” and not giving half of them placebos in a multiyear trial. We all want simple solutions to complex medical problems, especially as we learn more about the damaging effects of chronic inflammation on health. Instead experts recommend what good medical studies have shown to work: a healthy and balanced diet. “Many people think that they can just take a dietary supplement, pop the pill, and that replaces a healthy diet,” Manson says. Lori Youmshajekian is a science journalist who reports on consumer health, environmental issues and scientific misconduct. She holds a master's degree in science journalism from New York University and has written for National Geographic, Wired and Retraction Watch, among other outlets. If you enjoyed this article, I'd like to ask for your support. Scientific American has served as an advocate for science and industry for 180 years, and right now may be the most critical moment in that two-century history. If you subscribe to Scientific American, you help ensure that our coverage is centered on meaningful research and discovery; that we have the resources to report on the decisions that threaten labs across the U.S.; and that we support both budding and working scientists at a time when the value of science itself too often goes unrecognized. In return, you get essential news, captivating podcasts, brilliant infographics, can't-miss newsletters, must-watch videos, challenging games, and the science world's best writing and reporting. There has never been a more important time for us to stand up and show why science matters.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/complex-life-may-have-evolved-multiple-times/'>These Enigmatic ‘Fossils' Could Rewrite the History of Life on Earth</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 10:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Mysterious Rocks Could Rewrite Evolution of Complex Life Controversial evidence hints that complex life might have emerged hundreds of millions of years earlier than previously thought—and possibly more than once To the untrained eye, the specimen resembles a piece of golden tortellini embedded in a small slab of black shale. The question of whether it is a paradigm-shifting fossil or merely an ordinary lump of fool's gold has consumed El Albani for the past 17 years. In January 2008 El Albani, a talkative French Moroccan, was picking over an exposed scrape of black shale outside the town of Franceville in Gabon. Lying under rolling hills of tropical savanna, cut in places by muddy rivers lined by jungle, the rock layers of the Francevillian Basin are up to 2.14 billion years old. The strata are laced with enough manganese to support a massive mining industry. But El Albani was there pursuing riches of a different kind. Most sedimentary rocks of that age are thoroughly “cooked,” transformed beyond recognition by the brutal heat and pressure of deep burial and deeper time. But through an accident of geology, the Francevillian rocks were protected, and their sediments have maintained something of their original shape, crystal structure and mineral composition. As a result, they offer a rare window into a stretch of time when, according to paleontologists, oxygen was in much shorter supply and Earth's environments would have been hostile to multicellular organisms like the ones that surround us today. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. El Albani couldn't immediately explain their appearance by any common sedimentary process. This time he went home with more than 200 kilograms of specimens in his luggage. In 2010 El Albani and a team of his colleagues made a bombshell claim based on those finds: the strangely shaped specimens they'd recovered in Franceville were fossils of complex life-forms—organisms made up of multiple, specialized cells—that lived in colonies long before any such thing is supposed to have existed. If the scientists were right, the traditional account of life's beginning, which holds that complex life originated once around 1.6 billion years ago, is wrong. And not only did complex multicellular life appear earlier than previously thought, but it might have done so multiple times, sprouting seedlings that were wiped away by a volatile Earth eons before our lineage took root. El Albani and his colleagues have pursued this argument ever since. Rocks from the Francevillian Basin in Gabon are filled with gleaming shapes that have been interpreted as fossils of complex life-forms from more than two billion years ago. Almost immediately, prominent researchers argued that El Albani's specimens are actually concretions of natural pyrite that only look like fossils. Mentions of the Francevillian rocks in the scientific literature tend to be accompanied by words such as “uncertain” and “questionable.” Yet even as most experts regard the Francevillian specimens with a skeptical eye, a slew of recent discoveries from other teams have challenged older, simpler stories about the origin of life. Together with these new finds, the sparkling rock El Albani held in his hands has raised some very tricky questions. What conditions did complex life need to emerge? How can we recognize remains of life from deep time when organisms then would have been entirely different from those that we know? And where do the burdens of proof lie for establishing that complex life arose far earlier than previously thought—and more than just once? By most accounts, life on Earth first emerged around four billion years ago. In the anoxic waters, bacteria spread and fed on minerals around hydrothermal vents. Then, maybe 2.5 billion years ago, so-called cyanobacteria that gathered in mats and gave rise to great stone domes called stromatolites began feeding themselves using the power of the sun. That transformation would eventually devastate the first, oxygen-averse microbial residents of Earth. But amid a gathering oxygen apocalypse, something new appeared. Roughly two billion years ago a symbiotic union between two groups of single-celled organisms—one of which was able to process oxygen—gave rise to the earliest eukaryotes: larger cells with a membrane-bound nucleus, distinctive biochemistry and an aptitude for sticking together. Somewhere in the vast sweep of time between then and now, in something of a glorious accident, those eukaryotes began banding together in specialized ways, forming intricate and increasingly complex multicellular organisms: algae, seaweeds, plants, fungi and animals. By the mid-19th century researchers noticed that the fossil record got considerably livelier at a certain point, which we now know was around 540 million years ago. During this period, called the Cambrian, multicellular eukaryotes seemed to explode in diversity out of nowhere. But it wasn't long before scientists began finding older hints of multicellular organisms, suggesting that complex life proliferated before the Cambrian. In 1868 a geologist proposed that tiny, disk-shaped objects from sediments more than 500 million years old in Newfoundland were fossils—only for other researchers to dismiss them as inorganic concretions. Still, a gap of more than a billion years separates the earliest known eukaryotes and their great flowering in the Ediacaran. Why didn't they explode until the Ediacaran? Researchers have historically blamed environmental conditions on ancient Earth for the delay. The friendlier water chemistry and more abundant oxygen provided new opportunities for eukaryotic organisms that could exploit them. It's a commonly cited explanation for the timing of life's big bang, one that the field tends to accept, Porter says. As a kid growing up in Marrakech, El Albani wasn't interested in geology; football and medicine held more appeal. He then fell in love with it in part because like his father, a police officer, he enjoys a good investigation, working out what happened in some distant event by laying out multiple lines of evidence. Unlike most sedimentary rocks laid down two billion years ago—fated for deep burial and transformative heat and pressure—the Francevillian strata sit within a bowl of much tougher rock, which prevented them from being cooked. “It gives us the possibility of actually reconstructing this environment that existed in the past, at a scale that we don't see anywhere around this time,” says Ernest Chi Fru, a biogeochemist at Cardiff University in Wales, who has worked with El Albani on the Francevillian material. If you were searching for fossils of relatively large, soft-bodied multicellular organisms from this period, the Francevillian is exactly the kind of place you'd look in. El Albani's team has recovered quite a few such specimens. More than 6,000 pieces—all of them collected from the same five-meter scrape of Gabonese shale—sprawl over wood shelves and tables and glass display cabinets, the black slabs arranged in puzzle-piece configurations under white walls. El Albani is eager to show them off. Here are the ripplelike remnants of bacterial mats. There are strange, wormlike tracks that the team has suggested could be traces of movement. There are nonpyritized remains, too: sand-dollar-like circles ranging from one to several centimeters across imprinted on the shales. “Et voilà,” El Albani says, tapping one specimen and then another. This is totally different.” The sheer variety of forms is why he's always surprised that people could look at them and assume they aren't in fact fossils. Eukaryotic organisms tend to take up lighter forms, or isotopes, of elements such as zinc rather than heavy ones. Earlier this year El Albani's Ph.D. student Anna El Khoury reported another potential chemical signal for life in the contested rocks. Organisms in areas thick with arsenic sometimes absorb the poisonous chemical instead of necessary nutrients such as phosphate. What El Albani and his colleagues find most telling, however, are the environmental conditions that are now known to have prevailed when the putative fossils formed. According to some analyses, that spike in oxygen levels might have hit a peak close to that in the Ediacaran before eventually falling again. Talk with the people in El Albani's lab about the Francevillian, and they'll paint you a picture of an alien world. Thick mats of bacteria stretch across the underwater sediments. Not plants or animals as we understand them. Based on the sizes, shapes and geochemical signatures of the putative fossils, El Albani thinks they might belong to a lineage of colonial eukaryotes—perhaps something resembling a slime mold—that independently developed the complex multicellular processes needed to survive at large sizes. These colonial organisms would have been comparatively early offshoots of the eukaryotic tree, making them an entirely independent flowering of complex multicellular life from the Ediacaran bloom that took place more than a billion years later. The Francevillian organisms flourished for a time, but they did not last. After a few millennia, underwater volcanism started up again, and oxygen levels crashed. El Albani's team argues that rather than long epochs of stillness and stasis, rather than the rise of complex life being an extraordinary and long-brewing accident in Earth's long history, multicellular organisms might not have been a singular innovation. “It seems to me that [the Francevillian material] is showing that complex life might have evolved twice in history,” Chi Fru says. And if ancient complex life can emerge so quickly when conditions are right, who knows where else in Earth's rocks—or another planet's—signs of another blossoming might turn up next? Skeptics of El Albani's Francevillian “fossils”—and there are many—have tended to gather around similar sticking points, says Leigh Anne Riedman, a paleontologist at the University of California, Santa Barbara. For one thing, the bizarre shapes of the rocks show a lot more variety than tends to be seen in accepted early complex multicellular forms, and with their amorphous, asymmetrical features, they do not scan easily as organisms. Colonies of bacteria living in oxygen-poor environments often deposit pyrite as a by-product. Although such colonies can grow a sparkling rind around biological material, the mineral concretions can also develop on their own, developing lifelike appearances without any biological process. Shuhai Xiao, a paleontologist at Virginia Tech specializing in the Precambrian era, notes that the Francevillian material resembles similar-looking inorganic structures from Michigan that date to 1.1 billion years ago. If ancient complex life can emerge so quickly when conditions are right, who knows where else signs of another blossoming might turn up next? Even scientists who are more amenable to the idea that El Albani's specimens are fossils tend to conclude that the pyritized specimens are probably just the remains of bacterial mats, not complex life-forms. “I have no problem with there being oxygen oases and there being certain groups that proliferated during those periods,” Riedman says. “That name, man,” Riedman says of the boring billion. Just last year Lanyun Miao of the Nanjing Institute of Geology and Paleontology at the Chinese Academy of Sciences and her colleagues announced that they had discovered the oldest unequivocal multicellular eukaryotes in 1.6-billion-year-old rocks from northern China. They're a far cry from the much larger, more elaborate forms associated with complex multicellularity. But they show that these simpler kinds of multicellular life existed some 500 million years earlier than previously hypothesized. Analyses of genome sequences and fossils have hinted that the earliest common ancestor of all living eukaryotes may have appeared as long as 1.9 billion years ago. Critics argue that the forms evident in the Francevillian rocks are merely mineral concretions, not fossils of complex eukaryotic organisms. And complex multicellularity itself may develop surprisingly fast. In a fascinating experiment published a few years ago, a team at the Georgia Institute of Technology was able to get single-celled eukaryotes—in this case, yeasts—to chain together in multicellular forms visible to the naked eye in just two years. These findings, along with the growing fossil record, suggest to some researchers that multicellular eukaryotes have a deeper history than is generally recognized. But recognizing early life in the rock is notoriously tricky. Brooke Johnson, a paleontologist at the University of Liège in Belgium, has visited Ediacaran outcrops in the U.K. with his colleagues and sometimes struggled to spot the specific fossils he knows are there. Researchers constantly second-guess themselves for fear of overinterpreting any given shape or shadow in the stone. The specter of crankhood—of being the kind of researcher who drives their work off a cliff by refusing to be proved wrong—hangs over everybody. “It's very easy to get yourself tricked into thinking that you can see something that isn't there, because you're used to seeing a particular pattern,” Johnson says. One spring morning in 2023, while working through hundreds of samples of rock more than one billion years old from drill cores from Australia, Johnson knocked over one of the pieces. Johnson speaks cautiously about the structures and has yet to publish his findings on them formally. But he thinks they might be some type of colony-living eukaryote of a size significantly larger than the microscopic examples known from elsewhere in the early fossil record. “Something like the Francevillian stuff, people might have found it already in other rocks and just not seen it,” he says. The sheer vanity of forms is why El Albani is surprised that people could look at them and assume they aren't fossils. Dealing with material like the Francevillian requires trying to understand a time when Earth looked virtually nothing like the world we know now, Porter says. These conditions affected life in ways that are still only dimly understood. And the further back in time one goes, the more likely it is that any fossils will be difficult to recognize, to say nothing of categorize. “I would imagine they're probably frustrated [and thinking], ‘Why isn't everybody already excited about this and coming along with us? '” Riedman says of El Albani and his colleagues. We haven't gotten past the biogenic part. “There's no trouble with trilobites,” he remarks wistfully. But a visible exasperation creeps in when he discusses the Gabonese specimens, along with a tendency to simultaneously pick at and try to dismiss the wound. If his critics believe the Gabonese specimens are concretions, they need to try to prove that rather than simply asserting it. If they disagree that the rocks contain fossils of eukaryotes, nothing is stopping them from subjecting the specimens to their own analyses. So far he feels that nobody has published any research that takes their conclusions apart point by point and reckons with all the strands of evidence they've marshaled. “If I give my opinion that your iPhone is Samsung,” he says, pulling a phone across the desk, “I should explain why!” She's not convinced by the team's arguments for what the Francevillian samples represent—an independent lineage of colonial multicellular organisms, swiftly flowering, swiftly snuffed out. But the idea that they're all just mineral concretions has never satisfied her. If they're concretions, that's something researchers need to affirmatively show, she says. Doing so, after all, would add to the field's knowledge about how pseudofossils form in a way that simply writing them off does not. “It's fine if they're wrong,” Porter says of El Albani and his colleagues. Everyone is offering competing hypotheses, which are always subject to new evidence from the fossil record. In the end, “we'll probably all be somewhat wrong about our interpretation, actually.” Seventeen years after El Albani first stopped to examine a glinting blob in the Gabonese shale, his lab shows no signs of slowing down. They're also digging further into the question of how, precisely, chemistry can definitively distinguish between biological and nonbiological origins for a given specimen. In 2020 a team of researchers reported that the NASA Mars Science Laboratory rover Curiosity had photographed millimeter-size, sticklike structures in an ancient lake bed that resembled fossils left by miniature tunnelers on Earth. But if a lab could develop a reliable conceptual model for chemically distinguishing between signs of life and nonlife, “you could apply this on Mars or another planet based on the sediment,” El Albani says. Every year El Albani and his team make the trip to Gabon to work the scrape of black stone that reoriented his life. Sometimes he bends down to examine a glittering form in the rock. Asher Elbein is a science and culture journalist based in Austin, Tex. His work has appeared in the New York Times, Scientific American and Texas Monthly. If you enjoyed this article, I'd like to ask for your support. Scientific American has served as an advocate for science and industry for 180 years, and right now may be the most critical moment in that two-century history. I hope it does that for you, too. If you subscribe to Scientific American, you help ensure that our coverage is centered on meaningful research and discovery; that we have the resources to report on the decisions that threaten labs across the U.S.; and that we support both budding and working scientists at a time when the value of science itself too often goes unrecognized. In return, you get essential news, captivating podcasts, brilliant infographics, can't-miss newsletters, must-watch videos, challenging games, and the science world's best writing and reporting. There has never been a more important time for us to stand up and show why science matters. I hope you'll support us in that mission. Subscribe to Scientific American to learn and share the most exciting discoveries, innovations and ideas shaping our world today.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/a-human-on-a-bicycle-is-among-the-most-efficient-forms-of-travel-in-the/'>The Most Efficient Traveler Isn't a Bird or a Fish—It's You on a Bike</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 10:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>A famous graphic, now updated, compares locomotion in the animal kingdom Explore our legacy of discovery and look ahead to the future. Humans aren't very efficient movers—until you put us on a bicycle, when we become some of the most energy-efficient land travelers in the animal kingdom. For Scientific American's 180th birthday, we've updated a classic graphic comparing different forms of animal locomotion, first published in this magazine in 1973. Travel involves two main expenditures of energy: fighting gravity and propelling yourself forward. (Longer-legged land creatures tend to be more efficient because they get more distance out of each step, which explains why mice are so inefficient.) Swimming animals can similarly glide through water while letting their natural buoyancy minimize the need to fight gravity. Bikes allow us terrestrial folk to be more like fish. Wheels, a simple machine, let us coast without putting in power by pedaling, and the rigid frame supports the sitting rider against gravity. “They turn humans into this hyperefficient terrestrial locomotor because they make being on land more like swimming,” says Tyson Hedrick, a comparative physiologist at the University of North Carolina at Chapel Hill. The main drawback is our clunky human shape; bicyclists aren't streamlined like bluefin tuna, so they must overcome more drag. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. 3; March 1973 (data for human on a bicycle); Tyson Hedrick/University of North Carolina at Chapel Hill (velomobile calculation) Allison Parshall is an associate editor at Scientific American covering mind and brain and she writes the weekly online Science Quizzes. As a multimedia journalist, she contributes to Scientific American's podcast Science Quickly. Parshall's work has also appeared in Quanta Magazine and Inverse. She graduated from New York University's Arthur L. Carter Journalism Institute with a master's degree in science, health and environmental reporting. She has a bachelor's degree in psychology from Georgetown University. They specialize in using digital tools to reclaim traditional techniques. If you enjoyed this article, I'd like to ask for your support. Scientific American has served as an advocate for science and industry for 180 years, and right now may be the most critical moment in that two-century history. If you subscribe to Scientific American, you help ensure that our coverage is centered on meaningful research and discovery; that we have the resources to report on the decisions that threaten labs across the U.S.; and that we support both budding and working scientists at a time when the value of science itself too often goes unrecognized. In return, you get essential news, captivating podcasts, brilliant infographics, can't-miss newsletters, must-watch videos, challenging games, and the science world's best writing and reporting. There has never been a more important time for us to stand up and show why science matters.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-03313-z'>Carbon credits are failing to help with climate change — here's why</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 09:40:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). Andrew Macintosh is a professor of environmental law and policy at the Centre for Environmental Markets, Australian National University, Acton, Australia. Gregory Trencher is a professor of decarbonization governance at the Graduate School of Global Environmental Studies, Kyoto University, Kyoto, Japan. Benedict Probst is a senior research fellow in the Net Zero Lab, Max Planck Institute for Innovation and Competition, Munich, Germany. Danny Cullenward is a climate economist and lawyer at the Kleinman Center for Energy Policy, University of Pennsylvania, Philadelphia, USA. Thales A. P. West is a specialist in carbon offsets and impact evaluation at the Institute for Environmental Studies (IVM), Vrije Universiteit Amsterdam, Amsterdam, Netherlands. Don Butler is an ecologist and biogeographer at the Centre for Environmental Markets, Australian National University, Acton, Australia. Offsets are tradable credits from projects that claim to reduce emissions, either by avoiding them or by removing carbon dioxide from the atmosphere. Net zero needs AI — five actions to realize its promise Net zero needs AI — five actions to realize its promise Although conceptually appealing, this reliance on offsets has fatal flaws. In practice, it's difficult to ensure that they represent real emissions reductions rather than ‘hot air', with the claimed climate benefits existing only on paper. Equally challenging is ensuring that emission reductions are ‘additional', meaning that they would not have occurred without the incentive provided by the sale of carbon credits. For projects credited for sequestering carbon, it is also crucial to ensure that the CO2 is locked away permanently and not released back into the atmosphere later. Thus, offsets undermine decarbonization by enabling companies and countries to claim that emissions have been reduced when they have not. This results in more emissions, delays the phase-out of fossil fuels and diverts scarce resources to false solutions. Yet, climate-policy processes continue to rely on them. The operationalization of Article 6 of the Paris agreement and full implementation of the Carbon Offsetting and Reduction Scheme for International Aviation, both achieved in 2024, are set to turbocharge demand for carbon credits. In parallel, voluntary carbon markets are promising to raise standards as a way of further legitimizing and scaling up offsets. Of greatest concern is the expanding role of offsets in domestic carbon-pricing schemes, such as emissions trading and carbon taxes. They are even permitted as a substitute for paying carbon taxes in countries such as South Africa, Mexico, Chile and Colombia. Low-quality offsets artificially depress carbon prices, which dilutes the incentive for industries to cut their emissions and weakens the effectiveness of carbon-pricing schemes. Here, we outline the problems and call on decision makers to exclude offsets from carbon-pricing schemes. It is extremely difficult to evaluate the emission reductions achieved by offset projects because calculations require comparing outcomes with ‘business-as-usual' projections. For example, calculating the climate benefits from a project that avoids deforestation involves comparing a forest's actual carbon stock to that in a counterfactual scenario in which the project did not happen. The hypothetical nature of such calculations invites manipulation and can tempt project developers to overestimate business-as-usual emissions to generate more credits. In most cases, only project leaders and developers know whether their projects are dependent on credit revenues and are therefore truly additional. This makes it difficult for administrators to screen out shoddy projects. Even where projects are additional, errors can arise from inaccurate measurement of emissions and carbon stocks. For example, stocks of soil organic carbon vary naturally across landscapes and through time. It is also difficult to ensure that credited carbon removals remain sequestered. Many carbon sinks are susceptible to natural disturbances, such as droughts and wildfires, which can release carbon stored in trees and soils. Carbon stocks can also be lost through the clearing of vegetation and other changes in land-management practices. Despite these risks, most offset schemes mandate that credited sinks be maintained for only 40 years or even less. In most markets, the risks associated with the supply of low-quality goods and services can be alleviated if consumers are able to judge quality and seek redress when standards are not met. With offsets, however, the complexity and a lack of transparency make it close to impossible for buyers to make informed decisions about the quality of most credits. Typically, their priority is to procure credits at the lowest possible cost, a fact evidenced by a 2024 study revealing that the largest corporate buyers consistently choose low-priced credits over higher-quality alternatives3. For these reasons, the integrity of offset schemes hinges on the skills and efforts of administrators, but they are often incentivized to prioritize supply over integrity. In voluntary offset markets, registries rely on project registrations and credit issuances for revenue, while competing with one another for market share. Tightening integrity standards can therefore reduce revenue and put this business model at risk. Government-regulated offset programmes linked to carbon-pricing schemes show a similar dynamic. The main reason that pricing schemes allow the use of offsets is to lower compliance costs for polluters, thereby reducing resistance to climate policies4. Yet the focus on cost reduction places pressure on regulators to ensure a plentiful supply of low-cost credits, frequently at the expense of environmental integrity. The science reflects this, with studies repeatedly showing that few of the major offset types in use today deliver real, additional or permanent abatement. A 2024 meta-analysis examining some 2,300 offset projects — amounting to around one-fifth of all issued credits — found that less than 16% achieved the emissions reductions claimed by developers5. Recognizing these flaws, some stakeholders advocate a shift from projects that avoid emissions to those that remove carbon from the atmosphere. However, carbon-removal projects tend to have similar integrity problems. Engineering-based techniques are scarce, and so most removal projects currently involve less durable nature-based approaches, such as tree planting, improved soil management and production of biochar (plant-derived charcoal). Bad offsets cheat the climate by failing to deliver their promised emission reductions. And they do further damage by distorting carbon-pricing schemes. In a well-functioning emissions trading scheme, the carbon price will reflect the marginal cost of reducing emissions, whether at a polluting facility or through an offset project. When offsets lack integrity, credit prices reflect the cost of supplying pretend, rather than real, emission reductions, which artificially lowers the carbon price. Report of the High-Level Commission on Carbon Prices (World Bank, 2017). Questionable Integrity: Non-additionality in the Emissions Reduction Fund's Avoided Deforestation Method (Australian Conservation Foundation, 2021). Department of Climate Change, Energy, the Environment and Water. ACCU Scheme Landfill Gas Method Reforms — Exposure Draft Supplementary Material (Commonwealth of Australia, 2025). Cullenward, D. & Burtraw, B. in 2024 Annual Report of the Independent Emissions Market Advisory Committee Ch. A.M. is a non-executive director of Paraway Pastoral Company Ltd. Paraway Pastoral Company Ltd has offset projects under Australia's carbon offset scheme. S.B is chief climate scientist at Fortescue, a global metal mining company headquartered in Australia with facilities covered by the Australian Safeguard Mechanism. D.C. is a member of California's Independent Emissions Market Advisory Committee and the UNFCCC Paris Agreement Article 6.4 mechanism's Methodological Expert Panel. Net zero needs AI — five actions to realize its promise Carbon offsets aren't helping the planet — four ways to fix them Coral die-off marks Earth's first climate ‘tipping point', scientists say China pledges to cut emissions by 2035: what does that mean for the climate? The spectre of malnutrition is back and must be tackled — fast End GDP mania: how the world should really measure prosperity Trust in the sea-bed mining authority is fragile — here's how to change that Longer grant cycles would boost research in Africa The spectre of malnutrition is back and must be tackled — fast Metals are key to the global economy — but three challenges threaten supply chains x4 Research Fellows - SUSTAIN Section: Smart Electronic Materials & Systems Location: Highfield Campus Salary: £36,636 to £44,746 per annum Full-ti... Arc Institute's Science Fellows program is for early-career scientists who are seeking to transition to a PI position directly after their doctorate. Net zero needs AI — five actions to realize its promise Carbon offsets aren't helping the planet — four ways to fix them An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/10/251014014430.htm'>JWST may have found the Universe's first stars powered by dark matter</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-10-14 08:37:17
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Recent observations from the James Webb Space Telescope (JWST) suggest that some of these early stars may have been unlike the familiar (nuclear fusion-powered) stars that astronomers have studied for centuries. A new study led by Cosmin Ilie of Colgate University, together with Shafaat Mahmud (Colgate '26), Jillian Paulin (Colgate '23) at the University of Pennsylvania, and Katherine Freese at The University of Texas at Austin, has identified four extremely distant objects whose appearance and spectral signatures match what scientists expect from supermassive dark stars. "Supermassive dark stars are extremely bright, giant, yet puffy clouds made primarily out of hydrogen and helium, which are supported against gravitational collapse by the minute amounts of self-annihilating dark matter inside them," Ilie said. Supermassive dark stars and their black hole remnants could be key to solving two recent astronomical puzzles: i. the larger than expected extremely bright, yet compact, very distant galaxies observed with JWST, and ii. Katherine Freese first proposed the idea of dark stars with Doug Spolyar and Paolo Gondolo, publishing their initial peer-reviewed paper on the concept in Physical Review Letters in 2008. That study outlined how dark stars might grow and eventually collapse into supermassive black holes in the early universe. Decades of experiments have searched for these particles, but so far without success. One leading possibility involves Weakly Interacting Massive Particles (WIMPs). When two WIMPs collide, they are expected to annihilate each other, releasing energy that could heat collapsing hydrogen clouds and cause them to shine as brilliant dark stars. Conditions a few hundred million years after the Big Bang, within dense regions called dark matter halos, appear to have been ideal for forming such stars. These regions are also where the first generation of normal stars was expected to appear. "Weighing a million times as much as the Sun, such early dark stars are important not only in teaching us about dark matter but also as precursors to the early supermassive black holes seen in JWST that are otherwise so difficult to explain." In a 2023 PNAS study by Ilie, Paulin, and Freese, the first supermassive dark star candidates (JADES-GS-z13-0, JADES-GS-z12-0, and JADES-GS-z11-0) were identified using photometric data from JWST's NIRCam instrument. Since then, spectra from JWST's NIRSpec instrument became available for those, and a few other extremely distant objects. The other three are extremely compact, and can be modeled by supermassive dark stars powering a nebula (i.e. ionized H and He gas surrounding the star). Dark stars have a smoking gun signature, an absorption feature at 1640 Angstrom, due to the large amounts of singly ionized helium in their atmospheres. While the signal to noise ratio of this feature is relatively low (S/N~2), it is for the first time we found a potential smoking gun signature of a dark star. Researchers said that if both spectral features are confirmed, the object cannot be an isolated dark star, but rather may be a dark star embedded in a metal rich environment. Alternatively, dark stars and regular stars could have formed in the same host halo, as the researchers now realized it is possible. Note: Content may be edited for style and length. Scientists Discover Stem Cells That Could Regenerate Teeth and Bone Earth's Crust Is Breaking Apart off the Pacific Northwest Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            