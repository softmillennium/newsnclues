
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - SCIENCE Article Summaries - 2025-06-23</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            SCIENCE
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/archaeology/a65105504/odysseus-hero-cult-site-discovered/'>Archaeologists Believe They've Found a Lost Cult Site of Odysseus</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 18:45:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The idea of such a place was already known of by researchers, at least in theory, thanks to an inscription “from around 207 BC from Magnesia in Asia Minor that references a hero shrine and games (the Odysseia) held in his honor in Ithaca.” But thanks to this most recent project, which began in 2018, the scholars involved now feel confident that they've identified Odysseus's real hero-cult site. While this particular discovery doesn't prove that a real Odysseus truly did brave cyclops and sirens to journey home to his patient Penelope back in Ithaca, it does prove that many devotees believed it to be true and made their own possibly perilous journeys to this same site. With a new, big-budget retelling of Homer's Odyssey on the horizon, perhaps a whole new cadre of devotees will make their way to Ithaca once more. And now, after centuries of wondering, they'll know exactly where to go. Michale Natale is a News Editor for the Hearst Enthusiast Group. As a writer and researcher, he has produced written and audio-visual content for more than fifteen years, spanning historical periods from the dawn of early man to the Golden Age of Hollywood. His stories for the Enthusiast Group have involved coordinating with organizations like the National Parks Service and the Secret Service, and travelling to notable historical sites and archaeological digs, from excavations of America' earliest colonies to the former homes of Edgar Allan Poe. The Laser That Could Make Iran's Missiles Obsolete The Secret War to Take out Iran's F-14 Fighters The World's Most Powerful Laser Beam Is Coming This AI Model Could One Day Read Your Mind</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41586-025-09289-0'>Gating and noelin clustering of native Ca2+-permeable AMPA receptors</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 16:21:26
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Please note there may be errors present which affect the content, and all legal disclaimers apply. While extensive structural studies have been conducted on recombinant AMPARs and native calcium impermeable (CI)-AMPARs alongside their auxiliary proteins2-5, the molecular architecture of native calcium permeable (CP)-AMPARs has remained undefined. To elucidate the subunit composition, physiological architecture, and gating mechanisms of CP-AMPARs, here we present the first visualization of these receptors, immunoaffinity purified from rat cerebella, and resolve their structures using cryo-electron microscopy (cryo-EM). Notably, Noe 1 stabilizes the amino-terminal domain (ATD) layer without affecting receptor gating properties. Noe 1 contributes to AMPAR function by forming dimeric-AMPAR assemblies that likely engage in extracellular networks, clustering receptors within synaptic environments and modulating receptor responsiveness to synaptic inputs. This is a preview of subscription content, access via your institution Prices may be subject to local taxes which are calculated during checkout Chengli Fang, Cathy J. Spangler, Jumi Park, Natalie Sheldon, Laurence O. Trussell & Eric Gouaux Howard Hughes Medical Institute, Oregon Health & Science University, Portland, OR, USA Fang, C., Spangler, C.J., Park, J. et al. Gating and noelin clustering of native Ca2+-permeable AMPA receptors. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41557-025-01863-3'>New-to-nature biocompatible chemistry for plastic waste upcycling</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 15:40:27
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Synthetic and biological chemistry are traditionally seen as separate fields. Now, a biocompatible chemical reaction enables an engineered microbe to convert plastic waste into valuable compounds under mild, cell-friendly conditions. This is a preview of subscription content, access via your institution Get Nature+, our best-value online-access subscription Subscribe to this journal Receive 12 print issues and online access Prices may be subject to local taxes which are calculated during checkout A., Johnson, N. W. & Wallace, S. RSC Chem. Johnson, N. W. et al. Nat. Thomas, M. et al. Org. Bao, T., Qian, Y., Xin, Y., Collins, J. J. Wei Long Soon, Hui Qing Chong, Jee Loon Foo & Matthew Wook Chang Synthetic Biology Translational Research Programme, Yong Loo Lin School of Medicine, National University of Singapore, Singapore, Singapore Wei Long Soon, Hui Qing Chong, Jee Loon Foo & Matthew Wook Chang Department of Biochemistry, Yong Loo Lin School of Medicine, National University of Singapore, Singapore, Singapore Wei Long Soon, Hui Qing Chong, Jee Loon Foo & Matthew Wook Chang Wei Long Soon, Hui Qing Chong, Jee Loon Foo & Matthew Wook Chang Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Correspondence to Matthew Wook Chang. The authors declare no competing interests. et al. New-to-nature biocompatible chemistry for plastic waste upcycling. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: Translational Research newsletter — top stories in biotechnology, drug discovery and pharma.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.popularmechanics.com/science/environment/a65105278/ghost-plume-discovery/'>Geologists Accidentally Found a Ghost Plume Rising From Earth's Mantle</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.popularmechanics.com', 'title': 'Popular Mechanics'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 14:45:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Mantle plumes are one of the most dynamic geologic processes on Earth. As their name suggests, these plumes move hot rock near the core-mantle boundary toward the surface, creating new landmasses (such as Hawai'i) or causing powerful geothermal activity (Yellowstone). Of course, moving all that magma beneath the Earth comes with a healthy dose of volcanism for those who live above these mantle superhighways—unless you live in Oman, that is. In a new paper published in the journal Earth and Planetary Science Letters, an international team of scientists—led by seismologist Simone Pilia of King Fahd University of Petroleum and Minerals in Saudi Arabia—claims to have found an amagmatic mantle plume, also known as a ‘ghost plume,' resting beneath eastern Oman. While analyzing some of these waves, Pilia noticed a cylindrical area beneath eastern Oman where they moved more slowly and the rock they moved through appeared to be less rigid. But eastern Oman doesn't display the surface volcanism that's typical of such areas. Taking a closer look with independent measurements, Pilia confirmed that a mantle plume—nicknamed “Dani” after his son—likely existed roughly 660 kilometers below the surface. Although there is no volcanic activity on the surface above this plume, there are other pieces of evidence that point to some sort of geologic anomaly in the region. For example, Oman continues to rise in elevation long after the impacts of tectonic compression—a process that squeezes the Earth's crust together. If this “Dani plume” truly is a ghost plume, it would be the first one ever detected, and could possibly lead scientists to re-examine just how much heat is moving from the core-mantle boundary—especially if more ghost plumes exist around the world. Not only could that change Earth's geologic history, but mantle plumes provide many real-world benefits. If a variety of ‘ghost plumes' are also at work around the world, it's important that we learn as much as we can. Past CO2 Rise Can't Even Compare to Climate Change Are There More Humans on Earth Than We Thought? All Life on Earth Follows This One Simple Rule</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/06/250623072802.htm'>Recycled plastic is a toxic cocktail: Over 80 chemicals found in a single pellet</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 14:14:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>A single pellet of recycled plastic can contain over 80 different chemicals. A new study with researchers from University of Gothenburg and Leipzig shows that recycled polyethylene plastic can leach chemicals into water causing impacts in the hormone systems and lipid metabolism of zebrafish larvae. However, as plastics contain thousands of chemical additives and other substances that can be toxic, and these are almost never declared, hazardous chemicals can indiscriminately end up in recycled products. In a new study, researchers bought plastic pellets recycled from polyethylene plastic from different parts of the world and let the pellets soak in water for 48 hours. After which zebrafish larvae were exposed to the water for five days. The experimental results show increases in gene expression relating to lipid metabolism, adipogenesis, and endocrine regulation in the larvae. "These short leaching times and exposure times are yet another indicator of the risks that chemicals in plastics pose to living organisms. Some chemicals used as additives in plastics and substances that contaminate plastics are known to disturb hormones, with potential impacts on fertility, child development, links to certain cancers, and metabolic disorders including obesity and diabetes. And there is also a significant risk of chemical mixing events occuring, which render the recycled plastic toxic," says Bethanie Carney Almroth, professor at the University of Gothenburg and principal investigator on the project. These may have contaminated the plastics during their first use phase, prior to becoming waste and being recycled. The authors of the work stress that negotiators and decision-makers must include provisions to ban or reduce hazardous chemicals in plastics, and to increase transparency and reporting along plastics value chains. "This work clearly demonstrates the need to address toxic chemicals in plastics materials and products, across their life cycle," says Professor Bethanie Carney Almroth. "We cannot safely produce and use recycled plastics if we cannot trace chemicals throughout production, use and waste phases." Polyethylene, abbreviated PE, is a type of plastic used in a lot of packaging materials like bottle caps, plastic bags, agricultural mulch films, insulation for wiring and cables, pipes, ropes, toys and household items. It is the most widely produced and used polymer. Note: Content may be edited for style and length. Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/06/250623072753.htm'>Artificial intelligence isn't hurting workers—It might be helping</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 14:03:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>As artificial intelligence reshapes workplaces worldwide, a new study provides early evidence suggesting AI exposure has not, thus far, caused widespread harm to workers' mental health or job satisfaction. In fact, the data reveals that AI may even be linked to modest improvements in worker physical health, particularly among employees with less than a college degree. But the authors caution: It is way too soon to draw definitive conclusions. "So far, we find little evidence that AI adoption has undermined workers' well-being on average. If anything, physical health seems to have slightly improved, likely due to declining job physical intensity and overall job risk in some of the AI-exposed occupations." "We may simply be too early in the AI adoption curve to observe its full effects," Stella emphasized. "AI's impact could evolve dramatically as technologies advance, penetrate more sectors, and alter work at a deeper level." The co-authors noted that outcomes may differ in more flexible labor markets or among younger cohorts entering increasingly AI-saturated workplaces. "This research is an early snapshot, not the final word," said Pitt's Giuntella, who previously conducted significant research into the effect of robotics on households and labor, and on types of workers. "As AI adoption accelerates, continued monitoring of its broader impacts on work and health is essential. Technology alone doesn't determine outcomes -- institutions and policies will decide whether AI enhances or erodes the conditions of work." Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/06/250623072757.htm'>This triple-layer sunlight catalyst supercharges green hydrogen by 800%</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 14:02:35
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The chemical reaction to produce hydrogen from water is several times more effective when using a combination of new materials in three layers, according to researchers at Linköping University in Sweden. "Passenger cars can have a battery, but heavy trucks, ships or aircraft cannot use a battery to store the energy. For these means of transport, we need to find clean and renewable energy sources, and hydrogen is a good candidate," says Jianwu Sun, associate professor at Linköping University, who has led the study published in the Journal of the American Chemical Society. The LiU researchers are working on developing materials that can be used to produce hydrogen (H2) from water (H2O) by using the energy in sunlight. The research team has previously shown that a material called cubic silicon carbide (3C-SiC) has beneficial properties for facilitating the reaction where water is split into hydrogen and oxygen. In their current study, the researchers have further developed a new combined material. The new material has eight times better performance than pure cubic silicon carbide for splitting water into hydrogen," says Jianwu Sun. A challenge in the development of materials for this application is to prevent the positive and negative charges from merging again and neutralising each other. Today, there is a distinction between "grey" and "green" hydrogen. Almost all hydrogen present on the market is "grey" hydrogen produced from a fossil fuel called natural gas or fossil gas. "Green" hydrogen is produced using renewable electricity as a source of energy. Most materials under development today have an efficiency of between 1 and 3 per cent, but for commercialisation of this green hydrogen technology the target is 10 per cent efficiency. Being able to fully drive the reaction using solar energy would lower the cost of producing "green" hydrogen, compared to producing it using supplementary renewable electricity as is done with the technology used today. Jianwu Sun speculates that it may take around five to ten years for the research team to develop materials that reach the coveted 10 per cent limit. Note: Content may be edited for style and length. Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/d41586-025-01973-5'>First images from world's largest digital camera leave astronomers in awe</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 11:01:12
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. Major telescope hosts world's largest digital camera: how it will transform astronomy Major telescope hosts world's largest digital camera: how it will transform astronomy The latest images were put together mainly for aesthetic impact and to showcase how the observatory's digital camera can scan large swaths of sky with high sensitivity and in a short time. And yet, they are a reminder of what drives astronomers in the first place. Major telescope hosts world's largest digital camera: how it will transform astronomy Landmark Webb telescope releases first science image — astronomers are in awe Major telescope hosts world's largest digital camera: how it will transform astronomy Why the Royal Greenwich Observatory was chosen to mark zero degrees longitude A Postdoctoral position is available at an NIH-funded lab to study the role of inflammasome, nucleic acid sensing and interferon signaling in neutr... Assistant Professor with tenure-track for Quantitative Cell Biology (80-100%) at the University of Bern, Switzerland. Join us to build next-gen AGI, focusing on LLM, AIGC, and core model infrastructure. Major telescope hosts world's largest digital camera: how it will transform astronomy An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s42256-025-01041-7'>Dimensions underlying the representational alignment of deep neural networks with humans</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 10:51:11
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. A preprint version of the article is available at arXiv. Determining the similarities and differences between humans and artificial intelligence (AI) is an important goal in both computational cognitive neuroscience and machine learning, promising a deeper understanding of human cognition and safer, more reliable AI systems. Much previous work comparing representations in humans and AI has relied on global, scalar measures to quantify their alignment. However, without explicit hypotheses, these measures only inform us about the degree of alignment, not the factors that determine it. To address this challenge, we propose a generic framework to compare human and AI representations, based on identifying latent representational dimensions underlying the same behaviour in both domains. Applying this framework to humans and a deep neural network (DNN) model of natural images revealed a low-dimensional DNN embedding of both visual and semantic dimensions. In contrast to humans, DNNs exhibited a clear dominance of visual over semantic properties, indicating divergent strategies for representing images. Although in silico experiments showed seemingly consistent interpretability of DNN dimensions, a direct comparison between human and DNN representations revealed substantial differences in how they process images. By making representations directly comparable, our results reveal important challenges for representational alignment and offer a means for improving their comparability. Deep neural networks (DNNs) have achieved impressive performance, matching or surpassing human performance in various perceptual and cognitive benchmarks, including image classification1,2, speech recognition3,4 and strategic gameplay5,6. In addition to their excellent performance as machine learning models, DNNs have drawn attention in the field of computational cognitive neuroscience for their notable parallels to cognitive and neural systems in humans and animal models7,8,9,10,11. These similarities, observed through different types of behaviour or patterns of brain activity, have sparked a growing interest in determining both factors underlying these similarities and differences between human and DNN representations. From the machine learning perspective, understanding where DNNs exhibit a limited alignment with humans can support the development of better and more robust artificial intelligence (AI) systems. From the perspective of computational cognitive neuroscience, DNNs with stronger human alignment promise to be better candidate computational models of human cognition and behaviour12,13,14,15. Much previous research on the alignment of human and artificial visual systems has compared behavioural strategies (for example, classification) in both systems and has revealed important limitations in the generalization performance of DNNs16,17,18,19,20. Other work has focused on directly comparing cognitive and neural representations in humans to those in DNNs, using methods such as representational similarity analysis (RSA21) or linear regression22,23,24,25. This quantification of alignment has led to a direct comparison of numerous DNNs across diverse visual tasks26,27,28,29, highlighting the role of factors such as architecture, training data or learning objective in determining the similarity to humans25,26,29,30. Despite the appeal of summary statistics, such as correlation coefficients or explained variance, for comparing the representational alignment of DNNs with humans, they only quantify the degree of representational or behavioural alignment. However, without explicit hypotheses about potential causes for misalignment, such scalar measures are limited in their explanatory scope of which properties determine this degree of alignment, that is, which representational factors underlie the similarities and differences between humans and DNNs. Although diverse methods for interpreting DNN activations have been developed at various levels of analysis, ranging from single units to entire layers31,32,33,34,35, a direct comparability to human representations has remained a key challenge. Inspired by recent work in cognitive sciences that has revealed core visual and semantic representational dimensions underlying human similarity judgements of object images36, here we propose a framework to systematically analyse and compare the dimensions that shape representations in humans and DNNs. In this work, we apply this framework to human visual similarity judgements and representations in a DNN trained to classify natural images. Our approach reveals numerous interpretable DNN dimensions that appear to reflect both visual and semantic image properties and that appear to be well aligned to humans. In contrast to humans, who showed a dominance of semantic over visual dimensions, DNNs exhibited a striking visual bias, demonstrating that downstream semantic behaviour is driven more strongly by different, primarily visual, strategies. Although psychophysical experiments on DNN dimensions underscored their global interpretability, a direct comparison with human dimensions revealed that DNN representations, in fact, only approximate human representations but lack the consistency expected from property-specific visual and semantic dimensions. Together, our results reveal key factors underlying the representational alignment and misalignment between humans and DNNs, shed light on potentially divergent representational strategies, and highlight the potential of this approach to identify the factors underlying the similarities and differences between humans and DNNs. To improve the comparability of human and DNN representations, we aimed to identify the similarities and differences in core dimensions underlying human and DNN representations of images. To achieve this aim, we treated the neural network analogously to a human participant carrying out a cognitive behavioural experiment and then derived representational embeddings using a recent variational embedding technique37 from both human similarity judgements and a DNN on the same behavioural task (Methods). This approach ensured direct comparability between human and DNN representations. As a behavioural task, we chose a triplet odd-one-out similarity task, where from a set of three object images i, j and k, participants have to select the most dissimilar object (Fig. 1a; Supplementary Section D provides an analysis of the role of task instructions on triplet choice behaviour). In this task, the perceived similarity between two images i and j is defined as the probability of choosing these images to belong together across varying contexts imposed by a third object image k. By virtue of providing minimal context, the odd-one-out task highlights the information sufficient to capture the similarity between object images i and j across diverse contexts. In addition, it approximates human categorization behaviour for arbitrary visual and semantic categories, even for fairly diverse sets of objects36,37,38. Thus, by focusing on the building blocks of categorization that underlies diverse behaviours, this task is ideally suited for comparing object representations between humans and DNNs. a, The triplet odd-one-out task in which a human participant or a DNN is presented with a set of three images and is asked to select the image that is the most different from the others. b, Sampling approach of odd-one-out decisions from DNN representations. First, a dot-product similarity space is constructed from the DNN features. Next, for a given triplet of objects, the most similar pair in this similarity space is identified, making the remaining object the odd one out. For humans, this sampling approach is based on observed behaviour, which is used as a measure of their internal cognitive representations. c, Illustration of the computational modelling approach to learn a lower-dimensional object representation for human participants and the DNN, optimized to predict behavioural choices made in the triplet task. d, Schematic of the interpretability pipeline that allows for the prediction of object embeddings from pretrained DNN features. The displayed images ginger, granola and iron are sourced from publicly available datasets and are licensed under a public domain license76. Images in a and c reproduced with permission from ref. For human behaviour, we used a set of 4.7 million publicly available odd-one-out judgements39 over 1,854 diverse object images, derived from the THINGS object concept and image database40. For the DNN, we collected similarity judgements for 24,102 images of the same objects used for humans (1,854 objects with 13 examples per object). We used a larger set of object images since the DNN was less limited by constraints in dataset size than humans. This allowed us to obtain more precise estimates of their representation. To derive DNN representations, we chose a pretrained VGG-16 model41, given its common use in the computational cognitive neurosciences. Specifically, this network has been shown to exhibit good correspondence to both human behaviour17 and measured neural activity9,27,42 and performs well at predicting human similarity judgements24,25,30,43,44,45. VGG-16 was trained on the 1,000-class ImageNet dataset46, which contains a diverse range of image categories, such as animals, everyday objects and scenes. However, for completeness, we additionally ran similar analyses for a broader range of neural network architectures (Supplementary Section A). We focused on penultimate layer activations as they are the closest to the behavioural output, and they also showed closest representational correspondence to humans (Supplementary Section B). For the DNN, we generated a dataset of behavioural odd-one-out choices for the 24,102 object images (Fig. To this end, we first extracted the DNN layer activations for all the images. Next, for a given triplet of activations zi, zj and zk, we computed the dot product between each pair as a measure of similarity, then identified the most similar pair of images in this triplet and designated the remaining third image as the odd one out. Given the excessively large number of possible triplets for all 24,102 images, we approximated the full set of object choices from a random subset of 20 million triplets47. From both sets of available triplet choices, we next generated two representational embeddings, one for humans and one for the DNN, where each embedding was optimized to predict the odd-one-out choices in humans and DNNs, respectively. In these embeddings, each object is described through a set of dimensions that represent interpretable object properties. To obtain these dimensions and for comparability to previous work in humans36,37,38, we imposed sparsity and non-negativity constraints on the optimization, which support their interpretability and provide cognitively plausible criteria for dimensions36,39,48,49,50,51. Sparsity constrained the embedding to consist of fewer dimensions, motivated by the observation that real-world objects are typically characterized by only a few properties. Non-negativity encouraged a parts-based description, where dimensions cannot cancel each other out. Thus, a dimension's weight indicated its relevance in predicting an object's similarity to other objects. During training, each randomly initialized embedding was optimized using a recent variational embedding technique37 (see the ‘Embedding optimization and pruning' section). The optimization resulted in two stable, low-dimensional embeddings, with 70 reproducible dimensions for DNN embedding and 68 for human embedding. Having identified stable, low-dimensional embeddings that are predictive of triplet odd-one-out judgements, we first assessed the interpretability of each identified DNN dimension by visualizing object images with large numeric weights. In addition to this qualitative assessment, we validated these observations for the DNN by asking 12 (6 female and 6 male) human participants to provide labels for each dimension separately (see the ‘Labelling dimensions and construction of word clouds' section). Similar to the core semantic and visual dimensions underlying odd-one-out judgements in humans described previously36,37,39, the DNN embedding yielded many interpretable dimensions, which appeared to reflect both semantic and visual properties of objects. The semantic dimensions included taxonomic membership (for example, related to food, technology and home) and other knowledge-related properties (for example, softness), whereas the visual dimensions reflected visual-perceptual attributes (for example, round, green and stringy), with some dimensions reflecting a composite of semantic and visual properties (for example, green and organic) (Fig. Of note, the DNN dimensions also revealed a sensitivity to basic shapes, including roundness, boxiness and tube shape. This suggests that in line with earlier studies52,53, DNNs indeed learn to represent basic shape properties, an aspect that might not be apparent in their overt behaviour54. a, Visualization of example dimensions from human- and DNN-derived representational embeddings, with a selection of dimensions that had been rated as semantic, mixed visual–semantic and visual, alongside their dimension labels obtained from human judgements. Note that the displayed images reflect only images with a public domain license and not the full image set76. b, Rating procedure for each dimension, which was based on visualizing the top k images according to their numeric weights. Human participants labelled each of the human and DNN dimensions as predominantly semantic, visual, mixed visual–semantic or unclear (unclear ratings are not shown; 7.35% of all dimensions are for humans and 8.57%, for VGG-16). c, Relative importance of dimensions labelled as visual and semantic, where VGG-16 exhibited a dominance of visual and mixed dimensions relative to humans that showed a clear dominance of semantic dimensions. Images in a and b reproduced with permission from ref. Despite the apparent similarities, there were, however, striking differences found between humans and the DNN. First, overall, DNN dimensions were less interpretable than human dimensions, as confirmed by the evaluation of all dimensions by two independent raters (Supplementary Section C). This indicates a global difference in how the DNN assigns images as being conceptually similar to each other. Second, although human dimensions were clearly dominated by semantic properties, many DNN dimensions were more visual perceptual in nature or reflected a mixture of visual and semantic information. We quantified this observation by asking the same two independent experts to rate human and DNN dimensions according to whether they were primarily visual perceptual, semantic, reflected a mixture of both or were unclear (Fig. To confirm that the results were not an arbitrary byproduct of the chosen DNN architecture, we provided the raters with four additional DNNs for which we had computed additional representational embeddings. The results revealed a clear dominance of semantic dimensions in humans, with only a small number of mixed dimensions. By contrast, for DNNs, we found a consistently larger proportion of dimensions that were dominated by visual information or that reflected a mixture of both visual and semantic information (Fig. This visual bias is also present across intermediate representations of VGG-16 and even stronger in early to late convolutional layers (Supplementary Fig. This demonstrates a clear difference in the relative weight that humans and DNNs assign to visual and semantic information, respectively. We independently validated these findings using semantic text embedding and observed a similar pattern of visual bias (Supplementary Section E indicates that the results were not solely a product of human rater bias). Despite the overall differences in human and DNN representational dimensions, the DNN also contained many dimensions that appeared to be interpretable and comparable to those found in humans. Next, we aimed at testing to what degree these interpretable dimensions truly reflected specific visual or semantic properties, or whether they only superficially appeared to show this correspondence. To this end, we experimentally and causally manipulated images and observed the impact on dimension scores. Beyond general interpretability, these analyses further establish which visual properties in each image drive individual dimensions and, thus, determine image representations. Image manipulation requires a direct mapping from input images to the embedding dimensions. However, the embedding dimensions were derived using a sampling-based optimization based on odd-one-out choices inferred from penultimate DNN features. Consequently, this approach does not directly map these features to the learned embedding. To establish this mapping, we applied ℓ2-regularized linear regression to link the DNN's penultimate layer activations to the learned embedding. This mapping then enables the prediction of embedding dimensions from the penultimate feature activations in response to novel or manipulated images (Fig. Penultimate layer activations were indeed highly predictive of each embedding dimension, with all dimensions exceeding an R2 of 75%, and the majority exceeding 85%. Thus, this allowed us to accurately predict the dimension values for novel images. Having established an end-to-end mapping between the input image and individual object dimensions, we next used three approaches to both probe the consistency of the interpretation and identify dimension-specific image properties. First, to identify image regions relevant for each individual dimension, we used Grad-CAM55, an established technique for providing visual explanations. Grad-CAM generates heat maps that highlight the image regions that are the most influential for model predictions. Unlike the typical use of Grad-CAM, which focuses on generating visual explanations for model classifications (for example, dog versus cat), we used Grad-CAM to reveal which image regions drive the dimensions in the DNN embedding. The results of this analysis are illustrated with example images in Fig. Object dimensions were indeed driven by different image regions that contain relevant information, in line with the dimension's interpretation derived from human ratings and suggesting that the representations captured by the DNN's penultimate layer allow us to distinguish between different parts of the image that carry different functional importance. a, General methodology of the approach. We used Grad-CAM55 to visualize the importance of distinct image parts based on the gradients of the penultimate DNN features that we initially used to sample triplet choices. The gradients were obtained in our fully differentiable interpretability model with respect to a dimension w in our embedding. b, We visualize the heat maps for three different images and dimensions. Each column shows the relevance of parts of an image for that dimension. For this figure, we filtered the embedding by images available in the public domain76. Credit: torch in b, Cezary Borysiuk under a Creative Commons license CC BY 2.0; wineglass in b, Wojtek Szkutnik under a Creative Commons license CC BY-SA 2.0. Images in a and b reproduced with permission from ref. As the second image explanation approach, to highlight which image properties drive a dimension, we used a generative image model to create novel images optimized for maximizing the values of a given dimension31,56,57. Unlike conventional activation maximization targeting a single DNN unit or a cluster of units, our approach aimed to selectively amplify activation in dimensions of the DNN embedding across the entire DNN layer, using a pretrained generative adversarial neural network (StyleGAN-XL58). To achieve this, we applied our linear end-to-end mapping to predict the embedding dimensions from the penultimate activations in response to the images generated by StyleGAN-XL. The results of this procedure are shown in Fig. The approach successfully generated images with high numerical values in the dimensions of our DNN embedding. Indeed, the properties highlighted by these generated images appear to align with human-assigned labels for each specific dimension, again suggesting that the DNN embedding contained conceptually meaningful and coherent object properties similar to those found in humans. b, Visualizations for different dimensions in our embedding. For this figure, we filtered the embedding by images available in the public domain76. Images in a and b reproduced with permission from ref. As the third image explanation approach, given that different visual properties naturally co-occur across images, and to unravel their respective contribution, we causally manipulated individual image properties and observed the effect on the predicted DNN dimensions. We exemplify this approach with manipulations in colour, object shape and background (Supplementary Section F), largely confirming our predictions, showing specific activation decreases or increases in dimensions that appeared to be representing these properties. The previous results have confirmed the overall consistency and interpretation of the DNN's visual and semantic dimensions based on common interpretability techniques. However, a direct comparison with human image representations is crucial for identifying which representational dimensions align well and which do not. Traditional RSA provides a global metric of representational alignment, revealing a moderate correlation (Pearson's r = 0.55) between the representational similarity matrices (RSMs) of humans and the DNN (Fig. Although this indicates some degree of alignment in the object image representations, it does not clarify the factors driving this alignment. To address this challenge, we directly compared pairs of dimensions from both embeddings, pinpointing which dimensions contributed the most to the overall alignment and which dimensions were less well aligned. a, RSMs reconstructed from human and VGG-16 embedding. Each row represents an object, with rows sorted into 27 superordinate categories (for example, animal, food and furniture) from ref. 40 to better highlight similarities and differences in representation. b, Pairwise correlations between human and VGG-16 embedding dimensions. c, Cumulative RSA analysis that shows the amount of variance explained in the human RSM as a function of the number of DNN dimensions. For this figure, we filtered the embedding by images from the public domain76. For three images without a public domain version, visually similar replacements were used. Images in d–f reproduced with permission from ref. For each human dimension, we identified the most strongly correlated DNN dimension, once without replacement (unique) and once with replacement, and sorted the dimensions based on their fit (Fig. This revealed a close alignment, with Pearson's reaching up to r = 0.80 for a select few dimensions, which gradually declined across other representational dimensions. To determine whether the global representational similarity was driven by just a few well-aligned dimensions or required a broader spectrum of dimensions, we assessed the number of dimensions needed to explain human similarity judgements. The analysis revealed that 40 dimensions were required to capture 95% of the variance in representational similarity with the human RSM (Fig. Although this number is much smaller than the original 4,096-dimensional VGG-16 layer, these results demonstrate that the global representational similarity is not solely driven by a small number of well-aligned dimensions. Given the imperfect alignment of DNN and human dimensions, we explored the similarities and differences in the stimuli represented by these dimensions. For each dimension, we identified which images were the most representative of both humans and the DNN. Crucially, to highlight the discrepancies between the two domains, we then identified which images exhibited strong dimension values for humans but weak values for the DNN, and vice versa (Fig. Although the results indicated similar visual and semantic representations in the most representative images, they also exposed clear divergences in dimension meanings. For instance, in an animal-related dimension, humans consistently represented animals even for images in which the DNN exhibited very low dimension values. Conversely, the DNN dimension strongly represented objects that were not animals, such as natural objects, cages or mesh (Fig. Similarly, a string-related dimension maintained a string-like representation in humans but included other objects in the DNN that were not string like, potentially reflecting properties related to thin, curvy objects or specific image properties (Fig. Since internal representations do not necessarily translate into behaviour, we next addressed whether this misalignment would translate to downstream behavioural choices. To this end, we used a jackknife resampling procedure to determine the relevance of individual dimensions for odd-one-out choices. For each triplet, we iteratively pruned dimensions in both human and DNN embeddings and observed changes in the predicted probabilities of selecting the odd one out, yielding an importance score for each dimension for the odd-one-out choice (Fig. The results of this analysis showed that although humans and DNNs often aligned in their representations and choices, a sizable fraction of choices exhibited the same behaviour despite strong differences in representations (Fig. For behavioural choices, the semantic bias in humans was enhanced, as evidenced by an even stronger importance of semantic relative to visual or mixed dimensions in humans compared with DNNs. Individual triplet choices were affected not only by semantic dimensions but also by visual dimensions (Fig. Together, these results demonstrate that the differences in how humans and DNNs represent object images not only translate into behavioural choices but are also further amplified in their categorization behaviour. a, Overview of the approach. For one triplet, we computed the original predicted softmax probability based on the entire representational embedding for each object image in the triplet. We then iteratively pruned individual dimensions from the representational embedding and stored the resulting change in the predicted softmax probability—relative to that of the full embedding—as a relevance score for that dimension. b, We calculated the relevance scores for a random sample of 10 million triplets and identified the most relevant dimension for each triplet. We then labelled the 10 million most relevant dimensions according to human-labelled visual properties as semantic, mixed visual–semantic, visual or unclear. Semantic dimensions are the most relevant for human behavioural choices, whereas for VGG-16, visual and mixed visual–semantic properties are more relevant. c–f, We rank the sorted changes in softmax probability to find triplets in which human and the DNN maximally diverge. Each panel shows a triplet with the behavioural choice made by humans and the DNN. We visualized the most relevant dimension for that triplet alongside the distribution of relevance scores. Each dimension is assigned its human-annotated label. For this figure, we filtered the embedding by images from the public domain76. Images in a and c–f reproduced with permission from ref. A key challenge in understanding the similarities and differences in humans and AI lies in establishing ways to make these two domains directly comparable. Overcoming this challenge would allow us to identify strategies to make AI more human like17 and for using AI as an effective model of human perception and cognition. In this work, we propose a framework to identify interpretable factors that determine the similarities and differences between human and AI representations. In this framework, these factors can be identified by using the same experiment to probe behaviour in humans and AI systems and applying the same computational strategy to natural and artificial responses to infer their respective interpretable embeddings. We applied this approach to human similarity judgements and representations of DNNs trained on natural images with varying objectives, with a primary focus on an image classification model. This allowed for a direct, meaningful comparison of the representations underlying human similarity judgements with the representations of the image classification model. Our results revealed that the DNN contained representations that appeared to be similar to those found in humans, ranging from visual (for example, ‘white', ‘circular/round' and ‘transparent') to semantic properties (for example, ‘food related' and ‘fire related'). However, a direct comparison with humans showed largely different strategies for arriving at these representations. Although human representations were dominated by semantic dimensions, the DNN exhibited a pronounced bias towards visual or mixed visual–semantic dimensions. In addition, a direct comparison of seemingly aligned dimensions revealed that DNNs only approximated the semantic representations found in humans. These different strategies were also reflected in their behaviour, where similar behavioural outcomes were based on different embedding dimensions. Thus, despite seemingly well-aligned human and DNN representations at a global level, deriving dimensions underlying the representational similarities provided a more complete and more fine-grained picture of this alignment, revealing the nature of the representational strategies that humans and DNNs use12,14,59. Although approaches like RSA21,60 are particularly useful for comparing one or multiple representational spaces, they typically provide only a summary statistic of the degree of alignment and require explicit hypotheses and model comparisons to determine what it is about the representational space that drives human alignment. By contrast, other approaches have focused specifically on the interpretability of DNN representations31,32,34,35,61,62,63, but either provide very specific local measures about DNN units or have limited direct comparability with human representations, as the same interpretability methods can typically not be applied to understand human mental representations. Our framework combines the strengths of the comparability gained from RSA and existing interpretability methods to understand image representations in DNNs. We applied common interpretability methods to show that our approach allows for detailed experimental testing and causal probing of DNN representations and behaviour across diverse images. Yet, only the direct comparison with human representations revealed the diverging representational strategies of humans and DNNs and, thus, the limitations of the visualization techniques we used64. Our results are consistent with previous work indicating that DNNs make use of strategies that deviate from those used in humans65,66. Beyond previously discovered biases, here we found a visual bias in DNNs that diverges from a semantic bias in humans for similarity judgements. In particular, even the highest layers in DNNs retained strong visual biases for solving the tasks they had been trained on, including image classification or linking images with text, both of which can be described as semantic tasks with different degrees of richness. This visual strategy may, of course, reflect how our visual system solves core object recognition67. Indeed, it is an open question to what extent human core object recognition relies on a similar visual bias68 and whether this bias is also found in the anterior ventral–temporal cortex69, which is known to be involved in high-level object processing70 However, even if humans used a primarily visual strategy for solving core object recognition, our findings would demonstrate a significant limitation of DNNs in capturing human mental representations as measured with similarity judgements, despite similar representational geometries71. Interestingly, CLIP, a more predictive model of human cortical visual processing26,29, retained a visual bias despite training on semantic image descriptions, showing that the classification objective alone is not sufficient for explaining visual bias in DNNs. At the same time, the visual bias in CLIP was smaller (Supplementary Fig. 1b), indicating that better models of high-level visual processing may also be models with a larger semantic bias and pointing towards potential strategies for improving their alignment with humans, which may involve multimodal pretraining or larger, more diverse datasets29. Future work would benefit from a systematic comparison of different DNNs to identify what factors determine visual bias and their alignment with human brain and behavioural data. Although these results indicate that studying core dimensions of DNN representations can improve our understanding of the factors required to identify better models of human mental representations, it has also been demonstrated recently that aligning DNNs with human representations can improve DNN robustness and performance at out-of-distribution tasks28,72. This work highlights that identifying visual bias may be useful not only for understanding representational and behavioural differences between humans and DNNs but also for guiding future work determining the gaps in human–AI alignment and identifying adjustments in architecture and training needed to reduce this bias59. Further work is needed to clarify the role of task instructions in human–AI alignment across diverse tasks and instructions73. The framework introduced in this work can be expanded in several ways. Future work could use this approach to explore what factors make DNNs similar or different from one another. A comprehensive analysis of various DNN architectures, objectives or datasets25,26,28 could uncover the factors underlying representational alignment, and extension to other stimuli, tasks and domains, including brain recordings. Together, this framework promises a more comprehensive understanding of the relationship between human and AI representations, providing the potential to identify better candidate models of human cognition and behaviour and more human-aligned artificial cognitive systems. In the triplet odd-one-out task, participants are presented with three objects and must choose the one that is least similar to the others, that is, the odd one out. We define a dataset \({\mathcal{D}}:={\left\{\left(\{{i}_{s},{j}_{s},{k}_{s}\},\{{a}_{s},{b}_{s}\}\right)\right\}}_{s = 1}^{n}\), where n is the total number of triplets and {is, js, ks} is a set of three unique objects, with {as, bs} being the pair among them determined as the most similar. We used a dataset of human responses36 to learn an embedding of human object concepts. In addition, we simulated the triplet choices from a DNN. For the DNN, we simulated these choices by computing the dot product of the penultimate layer activation \({{\bf{z}}}_{i}\in {{\mathbb{R}}}_{+}\) after applying the rectified linear unit function, where \({S}_{ij}={{\bf{z}}}_{i}^{\top }{{\bf{z}}}_{j}\). The most similar pair {as, bs} was then identified by the largest dot product: Using this approach, we sampled the triplet odd-one-out choices for a total of 20 million triplets for the DNN. Let \({W}\in {{\mathbb{R}}}^{m\times p}\) denote a randomly initialized embedding matrix, where p = 150 is the initial embedding dimensionality. To learn interpretable concept embeddings, we used variational interpretable concept embeddings (VICE), an approximate Bayesian inference approach37. VICE performs mean-field variational inference to approximate the posterior distribution \(p({W}| {\mathcal{D}})\) with a variational distribution, qθ(W), where \({q}_{\theta }\in {\mathcal{Q}}\). VICE imposes sparsity on the embeddings using a spike-and-slab Gaussian mixture before updating the variational parameters θ. This prior encourages shrinkage towards zero, with the spike approximating a Dirac delta function at zero (responsible for sparsity) and the slab modelled as a wide Gaussian distribution (determining non-zero values). Therefore, it is a sparsity-inducing prior and can be interpreted as a Bayesian version of the elastic net74. Since the variational parameters are composed of two matrices, one for the mean and one for the variance, that is, θ = {μ, σ}, we can use the mean representation μi as the final embedding for an object i. Imposing sparsity and positivity constraints improves the interpretability of our embeddings, ensuring that each dimension meaningfully represents distinct object properties. Although sparsity is guaranteed via the spike-and-slab prior, we enforced non-negativity by applying a rectified linear unit function to our final embedding matrix, thereby guaranteeing that \({W}\in {{\mathbb{R}}}_{+}^{m\times p}\). Note that this is done both during optimization and at inference time. We used the same procedure as in ref. 37 for determining the optimal number of dimensions. Specifically, we initialized our model with p = 150 dimensions and reduced the dimensionality iteratively by pruning dimensions based on their probability of exceeding a threshold set for sparsity: where wij is the weight associated with object i and dimension j. Training stopped either when the number of dimensions remained unchanged for 500 epochs or when the embedding was optimized for a maximum of 1,000 epochs. We assessed reproducibility across 32 model runs with different seeds using a split-half reliability test. We chose the split-half reliability test for its effectiveness in evaluating the consistency of our model's performance across different subsets of data, ensuring robustness. We partitioned the objects into two disjoint sets using odd and even masks. For each model run and every dimension in an embedding, we identified the dimension that is the most highly correlated among all the other models by using an odd mask. Using the even mask, we correlated this highest match with the corresponding dimension. This process generated a sampling distribution of Pearson's r coefficients for all the model seeds. We subsequently Fisher z transformed the Pearson's r sampling distribution. The average z-transformed reliability score for each model run was obtained by taking the mean of these z scores. Inverting this average provides an average Pearson's r reliability score (Supplementary Section G). For our final model and all subsequent analyses, we selected the embedding with the highest average reproducibility across all dimensions. We assigned labels to the human embedding by pairing each dimension with its highest correlating counterpart from ref. These dimensions were derived from the same behavioural data, but using a non-Bayesian variant of our method. We then used the human-generated labels that were previously collected for these dimensions, without allowing for repeats. For the DNN, we labelled dimensions using human judgements. This allowed us to capture a broad and nuanced understanding of each dimension's characteristics. To collect human judgements, we asked 12 laboratory participants (6 male, 6 female; mean age, 29.08 years; s.d., 3.09 years; range, 25–35 years) to label each DNN dimension. Participants were presented with a 5 × 6 grid of images, with each row representing a decreasing percentile of importance for that specific dimension. Participants were asked to provide up to five labels that they thought best described each dimension. Word clouds showing the provided object labels were weighted by the frequencies of occurrence, and the top six labels were visualized. Due to computer crashes during data acquisition, three participants had incomplete data (32%, 80% and 93%). Study participation was voluntary, and participants were not remunerated for their participation. This study was conducted in accordance with the Declaration of Helsinki and was approved by the local ethics committee of the Medical Faculty of the University Medical Center Leipzig (157/20-ek). The first question asked whether the dimensions were primarily visual perceptual, semantic conceptual, a mix of both or whether their nature was unclear. For the second question, they rated the dimensions according to whether they reflected a single concept, several concepts or were not interpretable. Overall, both raters agreed agreed 81.86% of the time for question 1 and 90.00% of the time for question 2. Response ambiguity was resolved by a third rater (Supplementary Sections A–C). All raters were part of the laboratory but were blind to whether the dimensions were model or human generated. We additionally learned embeddings from early (convolutional block 1), middle (convolutional block 3) and late (convolution block 5) convolutional layers of VGG-16. For this, we applied global average pooling to the spatial dimensions of the feature maps and then sampled triplets from the averaged one-dimensional representations. To visualize the learned object dimensions, we used an activation maximization technique with a pretrained StyleGAN-XL generator \({\mathcal{G}}\) (ref. Our approach combines sampling with gradient-based optimization to generate images that maximize specific dimension values in our embedding space. We started by sampling a set of N = 100,000 concatenated noise vectors \({{\bf{v}}}_{i}\in {{\mathbb{R}}}^{d}\), where d is the dimensionality of the StyleGAN-XL latent space. For each noise vector, we generated an image \({{\bf{x}}}_{i}={\mathcal{G}}({{\bf{v}}}_{i})\) and predicted its embedding \({\hat{{\bf{y}}}}_{i}\in {{\mathbb{R}}}^{p}\) using our pipeline, where p is the number of dimensions in our embedding space. For a given dimension j, we selected the top k images that yielded the highest values for \({\hat{y}}_{ij}\), the jth component of \(\hat{{{\bf{y}}}_{i}}\). These images served as starting points for our optimization process. To refine these initial images, we performed gradient-based optimization in the latent space of StyleGAN-XL. Our objective function \({{\mathcal{L}}}_{{\rm{AM}}}\) balances two goals: increasing the absolute value of the embedding for dimension j and concentrating probability mass towards dimension j. Formally, we define \({{\mathcal{L}}}_{{\rm{AM}}}\) as The term on the right, referred to as the dimension specificity reward, concentrates probability mass towards a dimension without necessarily increasing its absolute value. The balance between these two objectives is controlled by the scalars α and β. The objective \({{\mathcal{L}}}_{{\rm{AM}}}\) was minimized using vanilla stochastic gradient descent. This optimization process was performed for each of the top k images selected in the initial sampling phase. The resulting optimized images provide visual representations that maximally activate specific dimensions in our learned embedding space, offering insights into the semantic content captured by each dimension. To highlight the image regions driving individual DNN dimensions, we used Grad-CAM. For each image, we performed a forward pass to obtain an image embedding and computed gradients using a backward pass. We next aggregated the gradients across all the feature maps in that layer to compute an average gradient, yielding a two-dimensional dimension importance map. We used RSA to compare the structure of our learned embeddings with human judgements and DNN features. This analysis was conducted in three stages: human RSA, DNN RSA, and a comparative analysis between human and DNN representations. We reconstructed a similarity matrix from our learned embedding. Given a set of objects \({\mathcal{O}}={o}_{1},\ldots ,{o}_{m}\), we computed the similarity Sij between each pair of objects (oi, oj) using the softmax function: where yi is the embedding of object oi, and the softmax function returns the probability of oi being more similar to ok than oj. To evaluate the explained variance, we used a subset of 48 objects for which a fully sampled similarity matrix and associated noise ceilings were available from previous work36. We then computed the Pearson correlation between our predicted RSM and the ground-truth RSM for these 48 objects. We followed a similar procedure, reconstructing the RSM from our learned embedding of the DNN features. We then correlated this reconstructed RSM with the ground-truth RSM derived from the original DNN features used to sample our behavioural judgements. To compare human and DNN representations, we conducted two analyses. First, we performed a pairwise comparison by matching each human dimension with its most correlated DNN dimension. This was done both with and without replacement, allowing us to assess the degree of alignment between human and DNN representational spaces. Second, we performed a cumulative RSA to determine the number of DNN dimensions needed to accurately reflect the patterns in the human similarity matrix. We then progressively added one DNN dimension at a time to a growing subset. After each addition, we reconstructed the RSM from this subset and correlated both the human RSM and the cumulative DNN RSM. This step-by-step process allowed us to observe how the inclusion of each additional DNN dimension contributed to explaining the variance in the human RSM. The images used in this study are obtained from the THINGS object concept and image database39, available via the OSF repository at https://osf.io/jum2f. All the result files pertaining to this study are made publicly available via a separate OSF repository at https://osf.io/nva43/. A Python implementation of all the experiments presented in this paper is publicly available via GitHub at https://github.com/florianmahner/object-dimensions/ and via Zenodo at https://doi.org/10.5281/zenodo.14731440 (ref. & Hinton, G. E. ImageNet classification with deep convolutional neural networks. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Vinyals, O. et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain it cortical representation. Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. Comparison of object recognition behavior in human and monkey. Kubilius, J., Bracci, S. & Op de Beeck, H. P. Deep neural networks as a computational model for human shape sensitivity. Cichy, R. M. & Kaiser, D. Deep neural networks as scientific models. Lindsay, G. W. Convolutional neural networks as a model of the visual system: past, present, and future. Kanwisher, N., Khosla, M. & Dobs, K. Using artificial neural networks to ask ‘why' questions of minds and brains. Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. Geirhos, R. et al. Generalisation in humans and deep neural networks. Beery, S., Van Horn, G. & Perona, P. Recognition in terra incognita. European Conference on Computer Vision 456–473 (Springer, 2018). Szegedy, C. et al. Intriguing properties of neural networks. Kriegeskorte, N., Mur, M. & Bandettini, P. A. Representational similarity analysis-connecting the branches of systems neuroscience. Attarian, M., Roads, B. D. & Mozer, M. C. Transforming neural network visual representations to predict human judgments of similarity. Roads, B. D. & Love, B. C. Learning as the unsupervised alignment of conceptual systems. Peterson, J. C., Abbott, J. T. & Griffiths, T. L. Evaluating (and improving) the correspondence between deep neural networks and human representations. Muttenthaler, L., Dippel, J., Linhardt, L., Vandermeulen, R. A. & Kornblith, S. Human alignment of neural network representations. International Conference on Learning Representions (ICLR, 2023). Conwell, C., Prince, J. S., Kay, K. N., Alvarez, G. A. & Konkle, T. A large-scale examination of inductive biases shaping high-level visual representation in brains and machines. Schrimpf, M. et al. Brain-Score: which artificial neural network for object recognition is most brain-like? Improving neural network representations using human similarity judgments. Wang, A. Y., Kay, K., Naselaris, T., Tarr, M. J. & Wehbe, L. Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset. Storrs, K. R., Kietzmann, T. C., Walther, A., Mehrer, J. & Kriegeskorte, N. Diverse deep neural networks all predict human inferior temporal cortex well, after training and fitting. & Vincent, P. Visualizing Higher-Layer Features of a Deep Network Report No. Zeiler, M. D. & Fergus, R. Visualizing and understanding convolutional networks. European Conference on Computer Vision 818–833 (Springer, 2014). Zhou, B., Sun, Y., Bau, D. & Torralba, A. Revisiting the importance of individual units in CNNs via ablation. Morcos, A. S., Barrett, David G. T., Rabinowitz, N. C. & Botvinick, M. On the importance of single directions for generalization. Bau, D. et al. Understanding the role of individual units in a deep neural network. Hebart, M. N., Zheng, C. Y., Pereira, F. & Baker, C. I. Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. Muttenthaler, L. et al. VICE: variational interpretable concept embeddings. & Hebart, M. N. Revealing interpretable object representations from human behavior. International Conferemce on Learning Representations (ICLR, 2019). Hebart, M. N. et al. THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior. THINGS: a database of 1,854 object concepts and more than 26,000 naturalistic object images. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR, 2015). Nonaka, S., Majima, K., Aoki, S. C. & Kamitani, Y. Brain hierarchy score: which deep neural networks are hierarchically brain-like? Jozwik, K. M., Kriegeskorte, N., Storrs, K. R. & Mur, M. Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments. & Baker, C. I. Similarity judgments and cortical visual responses reflect different properties of object and scene categories in naturalistic images. & Hebart, M. N. A high-throughput approach for the efficient prediction of perceived similarity of natural objects. Deng, J. et al. ImageNet: a large-scale hierarchical image database. Jain, L., Jamieson, K. G. & Nowak, R. D. Finite sample prediction and recovery bounds for ordinal embedding. Murphy, B., Talukdar, P. & Mitchell, T. Learning effective and interpretable semantic models using non-negative sparse embedding. International Conference on Computational Linguistics 1933–1950 (COLING, 2012). Fyshe, A., Wehbe, L., Talukdar, P., Murphy, B. & Mitchell, T. A compositional and interpretable semantic space. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 32–41 (ACL, 2015). Muttenthaler, L. & Hebart, M. N. THINGSvision: a Python toolbox for streamlining the extraction of activations from deep neural networks. Hermann, K., Chen, T. & Kornblith, S. The origins and prevalence of texture bias in convolutional neural networks. Singer, J. J. D., Seeliger, K., Kietzmann, T. C. & Hebart, M. N. From photos to sketches—how humans and deep neural networks process objects across different levels of visual abstraction. Geirhos, R. et al. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. International Conferene on Learning Representations (ICLR, 2019). Selvaraju, R. R. et al. Grad-CAM: visual explanations from deep networks via gradient-based localization. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. & Lipson, H. Understanding neural networks through deep visualization. Montavon, G., Samek, W. & Müller, K.-R. Methods for interpreting and understanding deep neural networks. Sauer, A., Schwarz, K. & Geiger, A. StyleGAN-XL: scaling StyleGAN to large diverse datasets. Sucholutsky, I. et al. Getting aligned on representational alignment. Kornblith, S., Norouzi, M., Lee, H. & Hinton, G. Similarity of neural network representations revisited. Understanding deep image representations by inverting them. Bau, D., Zhou, B., Khosla, A., Oliva, A. Network dissection: quantifying interpretability of deep visual representations. Understanding neural networks via feature visualization: a survey. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (eds Samek, W.et al.) 55–76 (Springer, 2019). Geirhos, R., Zimmermann, R. S., Bilodeau, B. L., Brendel, W. & Kim, B. Don't trust your eyes: on the (un)reliability of feature visualizations. International Conference on Machine Learning (ICML, 2024). Shortcut learning in deep neural networks. Hermann, K. L., Mobahi, H., Fel, T. & Mozer, M. C. On the foundations of shortcut learning. International Conference on Learning Representations (ICLR, 2024). DiCarlo, J. J., Zoccolan, D. & Rust, N. C. How does the brain solve visual object recognition? Jagadeesh, A. V. & Gardner, J. L. Texture-like representation of objects in human visual cortex. & Konkle, T. Contrastive learning explains the emergence and function of visual category-selective regions. Mur, M. et al. Human object-similarity judgments reflect and transcend the primate-IT object representation. When does perceptual alignment benefit vision representations? Dwivedi, K. & Roig, G. Representation similarity analysis for efficient task taxonomy and transfer learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition 12387–12396 (2019). Zou, H. & Hastie, T. Regularization and variable selection via the elastic net. & Hebart, M. N. THINGSplus: new norms and metadata for the THINGS database of 1854 object concepts and 26,107 natural object images. acknowledge support from a Max Planck Research Group grant of the Max Planck Society awarded to M.N.H. acknowledges support from the ERC Starting Grant COREDIM (ERC-StG-2021-101039712) and the Hessian Ministry of Higher Education, Science, Research and Art (LOEWE Start Professorship and Excellence Program ‘The Adaptive Mind'). acknowledges support from the project Dutch Brain Interface Initiative (DBI2) with project no. 024.005.022 of the research program Gravitation, which is (partly) financed by the Dutch Research Council (NWO). acknowledges support from the German Federal Ministry of Education and Research (BMBF) for the Berlin Institute for the Foundations of Learning and Data (BIFOLD) (01IS18037A) and for grants BIFOLD22B and BIFOLD23B. This study used the high-performance computing capabilities of the Raven and Cobra Linux clusters at the Max Planck Computing & Data Facility (MPCDF), Garching, Germany (https://www.mpcdf.mpg.de/services/supercomputing/). The funders had no role in the study design, data collection and analysis, decision to publish or preparation of the manuscript. was a Student Researcher at Google DeepMind while this work was done. Open access funding provided by Max Planck Society. These authors contributed equally: Florian P. Mahner, Lukas Muttenthaler. Vision and Computational Cognition Group, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany Florian P. Mahner, Lukas Muttenthaler & Martin N. Hebart Donders Institute for Brain, Cognition and Behaviour, Nijmegen, The Netherlands Berlin Institute for the Foundations of Learning and Data (BIFOLD), Berlin, Germany Department of Medicine, Justus Liebig University, Giessen, Germany Center for Mind, Brain and Behavior, Universities of Marburg, Giessen, and Darmstadt, Marburg, Germany Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar The authors declare no competing interests. Nature Machine Intelligence thanks Alex Murphy, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Mahner, F.P., Muttenthaler, L., Güçlü, U. et al. Dimensions underlying the representational alignment of deep neural networks with humans. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.nature.com/articles/s41593-025-01976-5'>Large-scale high-density brain-wide neural recording in nonhuman primates</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.nature.com', 'title': 'Nature'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 10:06:13
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. High-density silicon probes have transformed neuroscience by enabling large-scale neural recordings at single-cell resolution. However, existing technologies have provided limited functionality in nonhuman primates (NHPs) such as macaques. In the present report, we describe the design, fabrication and performance of Neuropixels 1.0 NHP, a high-channel electrode array designed to enable large-scale acute recording throughout large animal brains. The probe features 4,416 recording sites distributed along a 45-mm shank. Experimenters can programmably select 384 recording channels, enabling simultaneous multi-area recording from thousands of neurons with single or multiple probes. This technology substantially increases scalability and recording access relative to existing technologies and enables new classes of experiments that involve electrophysiological mapping of brain areas at single-neuron and single-spike resolution, measurement of spike–spike correlations between cells and simultaneous brain-wide recordings at scale. High-channel count electrophysiological recording devices such as Neuropixels probes1,2 are transforming neuroscience with rodent models by enabling recording from large populations of neurons anywhere in the rodent brain3,4,5,6,7,8,9. The capabilities provided by this approach have led to a myriad new discoveries4,7,9. The Neuropixels 1.0 probe has also been used to record neurons in both human10,11 and nonhuman primates (NHPs) such as macaques12,13,14,15, but its 10-mm length restricts access to superficial targets and the thin shank (24 µm) renders them difficult to insert through primate dura mater. Consequently, a large community of NHP researchers articulated the need for a probe that allows large-scale access to neurons throughout the primate brain. Current linear array technologies, such as V- or S-probes (Plexon, Inc.), provide access to the whole brain, but are limited to 64 channels and have relatively large diameters (for example, 380 µm) that increase as recording channels are added. Surface arrays such as the Utah array16 or floating microwire arrays17 allow for recording from up to 256 channels simultaneously, but are limited to recording at pre-specified depths in superficial cortex, require opening the dura for placement and cannot be moved after implantation. Alternative approaches using individually driven single electrodes have achieved recordings in deep brain regions from dozens or hundreds of neurons by a few dedicated labs18,19,20, although these approaches do not allow for dense sampling within a specific target region. Alternatively, two-photon (2P) imaging enables recording at single-cell resolution, but with limited temporal resolution and limited access to many brain areas. We developed the Neuropixels 1.0 NHP, a high-density integrated silicon electrode array, optimized for recording in NHPs and designed to enable flexible, configurable recording from large neuronal populations throughout the brain with single-neuron and single-spike resolution. Although the probe's design and electronic specifications are based on the Neuropixels 1.0 probe (http://Neuropixels.org), fabricating these probes with the desired combination of mechanical and electrical properties using a standard CMOS photolithography process required substantial engineering advances. Each probe is larger than a photolithographic reticle, requiring ‘stitching' of electrical traces across the boundaries between multiple reticles to achieve the required probe geometry21 and the introduction of stress compensation layers to prevent intrinsic material stresses from causing the shank to bend. Relative to Neuropixels 1.0, the two variants of Neuropixels 1.0 NHP feature a longer, wider and thicker shank (45 mm long, 125 µm wide and 90 µm thick; Fig. For each probe variant, the full length of the shank is populated with recording sites with a density of two sites every 20 µm (Fig. A switch under every site allows flexible selection of the 384 simultaneous recording channels across these 4,416 sites (11 banks of 384 channels, respectively, plus one half-sized bank at the shank–base junction; Fig. c, Some 4,416 recording sites covering the full length of the shank, grouped in 11.5 banks of 384 channels. d, Neuropixels 1.0 NHP probe silicon die photograph, indicating the four segments or sub-blocks used for the subfield stitching fabrication process. e, Details of the shank tip reference electrode layout (top) and CMOS circuit layout (bottom). Top left, cross-section taken outside the stitching overlap region. Top right, cross-section at the narrowest point. Inset scale bar units in nanometers. Inset, probe-tip geometry as fabricated, pre-grinding. h, Raw electrical recordings from 384 simultaneous channels in the motor cortex of a rhesus macaque monkey. i, Example waveforms from isolated single neurons (gray) and median waveform (colored). j, Auto- and CCGs for neurons shown in i. Here, we described this new probe and the engineering challenges surmounted by fabrication and illustrate the ability to address a range of new experimental use cases for recording in large animal models. We focused on recording large neuronal populations, recording both in deep structures and from multiple regions simultaneously. We illustrated these use cases with four example experiments from different labs, each pursuing research questions spanning sensory, motor and cognitive domains using macaques. The scale and access provided by Neuropixels 1.0 NHP enabled a wide range of new experimental paradigms, while streamlining neural data collection at a fraction of the cost of existing alternatives. The Neuropixels 1.0 NHP probe uses the same signal-conditioning circuits as the Neuropixels 1.0 probe1, integrating 384 low-noise readout channels with programmable gain and 10-bit resolution on a 130-nm Silicon-on-Insulator CMOS platform. Within the probe shank, an array of shift-register elements and a switch matrix enable programmable selection of recording sites. We addressed this limitation by developing a method called stitching21, which precisely aligns features from different exposures between adjacent reticles. The typical maximum size of a 130-nm CMOS chip is limited by the maximum reticle size (that is, 22 mm × 24 mm in this case) that can be exposed in a single lithography step. To build larger chips, the CMOS chip is divided into sub-blocks smaller than the reticle field, which are later used to re-compose (that is, stitch together) the complete chip by performing multiple reticle exposures per layer. The stitched boundary regions of the small design blocks are overlapped (that is, double exposed) to ensure uninterrupted metal traces from one reticle to the next, which is accounted for in the design. As multiple levels of metal wires are needed to realize and interconnect the large number of recording sites at high density, the conducting wires in the shank must be aligned between adjacent reticles for four of the six stacked aluminum metal layers. To enable stitching and to simplify photo-mask design and reuse, the probe is designed with three elements as shown in Fig. Four reticles are thus required to define the features on each probe. The probe base is 6 mm wide, allowing four probes to be written across these four reticles. Collectively, these approaches successfully enabled recording from the full 45 mm without signal degradation as a function of position (Extended Data Fig. One of the major design changes to this device, compared with the rodent probe1, was to strengthen and thicken the probe shank, which was necessary both to support the longer 45-mm shank length and to allow the probe to penetrate primate dura. To achieve this, we increased the thickness of the shank from 24 µm to 90 µm. For the data reported in the present study, we used the Neuropixels 1.0 NHP probe version with a 125 µm wide, 45 mm long shank, 4,416 selectable recording sites (pixels) and a 48 mm2 base. To facilitate insertion of the shank into the brain and to minimize dimpling and tissue damage1, the 20° top-plane, chisel-tapered shanks were mechanically ground to a 25° bevel angle on the side plane using a modified pipette microgrinder (Narishige, cat. This procedure resulted in a tip sharpened along both axes (Fig. Additional discussion of insertion mechanics, methods and hardware is provided in a wiki (https://github.com/cortex-lab/neuropixels/wiki). The programmable sites of Neuropixels 1.0 NHP provide a number of advantages over existing recording technologies appropriate for primates. First, the high-channel count represents a transformative capability for large animal research. Large-scale recordings permitted rapid surveys of brain regions, enabled analyses that infer the neural state on single trials, made it practical to identify ensembles of neurons with statistically significantly correlated spike trains and reduced the number of animals and time required to perform experiments. Second, the high density of recording sites enabled high-quality, automated spike sorting22 when recording from one or both columns within a 3.84-mm bank of recording sites (Fig. Third, users could choose to record with full density from one column in each of two banks, for 7.68 mm total length of high-quality, single-unit recordings, or specify alternative layouts (Extended Data Fig. Programmable site selection allowed experimenters to decouple the process of optimizing a recording location from probe positioning. This capability allowed experimenters to survey neural activity along the entire probe length to map the relative positions of electrophysiological features. We illustrated these collective advantages using example recordings in different macaque brain studies, including: (1) retinotopic organization of extrastriate visual cortex; (2) neural dynamics throughout the motor system; (3) face recognition in face patches of the inferotemporal (IT) cortex; and (4) neural signals underlying decision-making in the posterior parietal cortex. More than half of the macaque neocortex is visual in function23 and a multitude of visual areas containing neurons with distinct feature-selective properties (for example, motion, color) lie beyond the primary visual cortex (V1). However, many of these visual areas are located deep within the convolutions of the occipital, temporal and parietal lobes (Fig. This, combined with the limitations of prior recording technologies, led to most electrophysiological studies focusing on only a subset of visual areas. Fewer than half of the identified visual areas have been well studied (for example, areas V4, MT (middle temporal)), whereas most (for example, DP (dorsal prelunate), V3A, FST (fundus of the superior temporal sulcus), PO (parieto-occipital)) have been only sparsely investigated. This is practical only with technologies that enable large-scale surveys via simultaneous population recordings from both superficial and deep structures. Our initial tests with the Neuropixels 1.0 NHP probe demonstrate that it is well suited for that purpose. Inset, the estimated probe trajectory of one multi-bank recording. b, Spike waveforms of single neurons recorded across a single bank of (384) recording sites (3.84 mm) shown at their measured location on the probe surface. c, Population spike raster aligned to stimulus onset for units shown in b. d, Distribution of RFs of 202 visually responsive neurons across cortical depth in a single-bank recording. The arrows denote abrupt changes in RF progressions and putative visual area boundaries. e, Top view of c, illustrating the coverage of RFs across the contralateral visual field. Each probe was repeatedly used for up to 23 successive sessions. Units with firing rate >3 Hz are included. g, Spike waveforms of 3,029 single neurons recorded across 5 banks of recording sites (~19 mm) shown at their measured location on the probe surface. h, Heatmap of stimulus-evoked responses for all 2,729 visually responsive neurons. Each neuron is plotted at its corresponding cortical depth. i, RF heat maps for 1,500 of the most superficial neurons, indicating visual field locations where stimuli evoked responses for each single neuron. The white crosshair in each map denotes the estimated horizontal and vertical meridians, with each map covering 26 (H) × 32 (V) d.v.a. RFs are arranged in a 42 (rows) × 36 (columns) array. During individual experimental sessions, the activity of thousands of single neurons across multiple visual cortical areas could be recorded using a single NHP probe. Figure 2b,c shows the spike waveforms and rasters from one recording of 446 neurons simultaneously recorded from one bank of recording sites spanning 6–10 mm below the cortical surface. RFs shifted in an orderly manner for stretches of approximately 1 mm and then shifted abruptly, reflecting probable transitions between different retinotopic areas (for example, refs. When recording from up to 23 successive sessions in the same location with each probe, the number of neurons recorded varied but showed no clear decline with repeated penetrations (Fig. In other sessions, we recorded from up to five probe banks spanning 0–19 mm beneath the pial surface (Fig. Using this approach, it was possible to sample from different locations without moving the probe. In one example session, 3,029 single neurons were recorded, of which 2,729 neurons were visually responsive (Fig. As with the single-bank recordings (Fig. 2d,e), neuronal RFs shifted gradually for contiguous stretches, punctuated by abrupt changes. 2i), RFs at more superficial sites (0–3 mm) were located at more eccentric locations of the visual field and then abruptly shifted toward the center and closer to the lower vertical meridian (LVM; ~3 mm). At the same location, neurons became more selective to the direction of motion (Extended Data Fig. 3c,d), suggesting a transition from area V4 to areas MT or MST. After that, RFs were located more centrally at the lower contralateral visual field and were observed across several millimeters. At deeper sites (~6–7 mm), smaller RFs clustered near the horizontal meridian (HM) for more than 1 mm, then quickly shifted toward the upper vertical meridian (UVM; ~8 mm). Finally, at the deepest sites (>10 mm), RFs generally became larger and much less well defined. These representations were stable throughout the duration of a session (Extended Data Fig. These data illustrate how the Neuropixels 1.0 NHP's dense sampling and single-unit resolution facilitate large-scale and unbiased mapping of the response properties of neurons across multiple visual areas. Next, we demonstrated the utility of this technology for studying multiple brain areas involved in movement control. Constraints of existing technology have led to two broad limitations in studies of the motor system. First, motor electrophysiologists have been forced to choose between simultaneous recording from populations of superficial neurons in gyral motor cortex (PMd and rostral M1) using Utah arrays31 and, alternatively, recording fewer neurons in sulcal M1 using single-wire electrodes or passive arrays of 16–32 contacts (for example, Plexon S-probes or Microprobes Floating Microwire Arrays17). Recording from large populations of neurons in sulcal M1 has not been feasible. Second, the motor cortex is only one part of an extensive network of cortical and subcortical structures involved in generating movement29,32. Many investigations of the motor system focus on M1 and comparatively fewer experiments have investigated neural responses from the numerous additional structures involved in planning and controlling movements, the result in part of the challenge of obtaining large-scale datasets in subcortical structures in primates. Areas such as the supplementary motor area (SMA) and the basal ganglia (BG) are understood to be important for planning and controlling movements, but investigation of the functional roles and interactions between these regions is hampered by the challenge of simultaneously recording from multiple areas. We developed a system capable of simultaneous recordings from multiple Neuropixels 1.0 NHP probes in superficial and deep structures of rhesus macaques. We tested this approach using a task in which a monkey generated isometric forces to track the height of a scrolling path of dots (Fig. 3b) while the monkey tracked a variety of force profiles (one profile per condition). Each condition was repeated across multiple trials (Fig. Single neurons exhibited a diversity of temporal patterns throughout the motor behavior (Fig. When ordered using Rastermap34, neurons illustrated a diversity of phase relationships with respect to the behavior (Fig. Predictions of endpoint force from neural activity (via linear regression) improved steadily as more neurons were included in the analysis and the performance did not saturate even when including all recorded neurons (360) for an example session (Fig. Thus, despite the apparent simplicity of a one-dimensional force-tracking task, neural responses are sufficiently diverse that it is necessary to sample from many hundreds of neurons to capture a complete portrait of population-level activity. b, Recording targets in motor cortex (left) and schematic of recording target in sulcal M1, sagittal section (right). c, Trial-averaged arm force during pacman task. d, Trial-averaged firing rate for example neurons. e, Single-trial spike raster for four example neurons. f, Trial-averaged, normalized responses of all 360 neurons for the same session as data in c–e, ordered using Rastermap34. g, Linear model force prediction accuracy as a function of the number of neurons included in the analysis. h, Spike waveforms on ten channels of four example neurons, averaged across nonoverlapping time bins that represent one-fifth of the recording duration. i, Probe drift estimating output by Kilosort 2.5 over the duration of a single motor behavioral session. Each line represents the estimate for a subset of channels. With careful preparation, on most sessions we did not observe rapid probe motion due to pulse, respiration or mechanical forces from the motor task. Typically, we observed stable waveforms (Fig. In some isolated cases, when the primate generated a large unexpected movement, we did observe a fast translation of the probe (for example, Extended Data Fig. However, in practice, we found that low-drift recordings could be obtained across a variety of experimental preparations in disparate brain regions (Fig. a, Top, drift map visualization of position versus time for one representative recording from the motor cortex. The darker spots indicate larger amplitude spikes. Bottom, estimate of drift calculated using Kilosort for the same recording shown above. b–d, Same visualization as shown in a for visual cortex (b), area AF (c) and area LIP (d), respectively. Programmable site selection in the Neuropixels 1.0 NHP enables simultaneous recording from superficial and deep structures using a single probe. To illustrate this capability, we recorded from superficial motor cortex and internal globus pallidus (GPi) in the BG using 192 channels in each target location (Fig. Alternatively, the small form factor of the Neuropixels 1.0 NHP probes and headstages allows for dense packing of multiple probes by inserting probes along nonparallel trajectories to record from a large number of neurons in a single area. Figure 5c,d illustrates recordings from three probes in gyral PMd, yielding 673 neurons. This approach was tested on two sessions, in which we recorded from 6 and 7 probes, yielding 1,012 and 783 neurons, respectively. These yields were lower than average as a result of imperfect recording conditions unrelated to the insertion hardware. Sampling with multiple probes is also well suited for simultaneous recording from different brain areas. Figure 5e illustrates a system designed to record from M1, the GPi of the BG and the SMA, using multiple probes inserted into a single recording chamber, with representative neural responses illustrated in Fig. a, Multi-area recording using a single probe in the motor cortex and BG (GPi), allocating 192 channels to each region via programmable site selection. b, Example raw waveforms and site selection for the recording described in a. c, Recording from many neurons within a single small target region using multiple probes inserted along convergent trajectories. d, Example waveforms of neurons recorded on probes using the apparatus shown in c. e, Recording from disparate brain regions (SMA, M1 and GPI) using three probes, all inserted along parallel trajectories. f, Example neurons recorded using the apparatus shown in e. The IT cortex is a critical brain region supporting high-level object recognition and has been shown to harbor several discrete networks35, each specialized for a specific class of objects. The network that was discovered first and has been most well studied in nonhuman primates is the face-patch system. This system consists of six discrete patches in each hemisphere36, which are anatomically and functionally connected. Studying the face-patch system has yielded many insights that have transferred to other networks in the IT cortex, including increasing view invariance going from posterior to anterior patches and a simple, linear encoding scheme35. As such, this system represents an approachable model for studying high-level object recognition37. The code for facial identity in these patches is understood well enough that images of presented faces can be accurately reconstructed from neural activity of just a few hundred neurons38. A major remaining puzzle is how different nodes of the face-patch hierarchy interact to generate object percepts. To answer this question, it is imperative to record from large populations of neurons in multiple face patches simultaneously to observe the varying dynamics of face-patch interactions on a single-trial basis. This is essential because the same image can often invoke different object percepts on different trials39. In the present study, we recorded with one probe in each of two face patches, middle lateral (ML) and anterior fundus (AF), simultaneously (Fig. The Neuropixels 1.0 NHP probes recorded responses of 1,127 units (622 single units, 505 multi-units) across both face patches during a single session (Fig. A continuous segment of approximately 220 channels (2.2 mm) in ML and 190 channels (1.9 mm) in AF contained face-selective units, indicating that these extents of the probes were in the IT cortex. In ML 261 units and in AF 297 units were face selective (two-sided, two-sample Student's t-test, threshold P < 0.05). a, Simultaneous targeting of two face patches. Coronal slices from MRI show inserted tungsten electrodes used to verify targeting accuracy for subsequent recordings using Neuropixels 1.0 NHP (top, face-patch ML; bottom, face-patch AF). Yellow overlays illustrate functional MRI contrast in response to faces versus objects. b, Response rasters for a single stimulus presentation of simultaneously recorded neurons in ML and AF to a monkey face, presented at t = 0. Each line in the raster corresponds to a spike from a single neuron or multi-unit cluster, including both well-isolated single units and multi-unit clusters. c, Neuropixels 1.0 NHP enabling recordings from many face cells simultaneously. These plots show the average responses (baseline subtracted and normalized) of visually responsive cells (rows) to 96 stimuli (columns) from 6 categories, including faces and other objects. Bottom, exemplar stimuli from each category. The plots included 438 cells or multi-unit clusters in ML (left) and 689 in AF (right), out of which a large proportion responded selectively to faces. Units were sorted by channel, revealing that face cells are spatially clustered across the probe. e, Same as c, for face-patch AM, recorded in a different session. Changing the visual stimulus to a monkey face yielded a clear visual response across both face-patch populations. We measured responses of visually responsive cells to 96 different stimuli containing faces and nonface objects (Fig. A majority of cells in the two patches showed clear face selectivity. Using single-wire tungsten electrodes, this dataset would have taken about 2 years to collect, but is now possible in a single 2-h experimental session38. In addition, to the gain in efficiency created by this technology, simultaneous recordings of multiple cells and multiple areas allowed for investigation of how populations encode object identity in cases of uncertain or ambiguous stimuli, where the interpretation of the stimulus may vary from trial to trial but is nevertheless highly coherent on each trial. The anatomical depth of face patches puts them far out of reach for shorter high-density probes. For example, face-patch AM (anteromedial) sits about 42 mm from the craniotomy (Fig. 6d) along a conventional insertion trajectory, but face patches in this region are still accessible using the Neuropixels 1.0 NHP probe and exhibit face-selective responses within the boundaries of AM (Fig. For many cognitive functions, the processes that give rise to behavior vary across repetitions of a task. As such, technologies that enable the analysis of neural activity at single-trial resolution will be especially critical for research on cognition. Achieving this resolution is challenging because it requires recordings from many neurons that share similar functional properties. This challenge is exemplified in perceptual decision-making, in which decisions are thought to arise through the accumulation of noisy evidence to a stopping criterion, such that their evolution is unique on each trial in a dynamic motion discrimination task (Fig. This process is widely observed and known as drift diffusion41,42. Neural correlates of drift diffusion have been inferred from activity in the lateral intraparietal area (LIP) averaged over many decisions. Analysis of LIP activity during single decisions has proven particularly difficult because RFs have little to no anatomical organization (Fig. 7c, top), preventing simultaneous recordings from many neurons with the same receptive field. The monkey must decide the net direction of dynamic RDM and indicate its decision by making a saccadic eye movement, whenever ready, from the central fixation point (red) to a left-choice or right-choice target (black). The choice and response times are explained by the accumulation of noisy evidence to criterion level54. b, Simultaneous recordings in LIP and SC. Populations of neurons were recorded in LIP with a Neuropixels 1.0 NHP probe and in SC (deeper layers) with a multi-channel V-probe (Plexon). c, RFs are identified, post-hoc, from control blocks in which the monkey performed an oculomotor delayed response task55. A small fraction of the LIP neurons (top) has RFs that overlap the left-choice target (outlined in black). The SC (bottom) has a topographic map, so many neurons have overlapping RFs. The large sample size in LIP facilitates identification of neurons that respond to the same choice target in LIP and SC. Rates are offset by the mean firing rate of 0.18–0.2 s after motion onset to force traces to begin at 0. A few representative traces are highlighted for clarity. e, The same trials as in d aligned to saccade initiation, without baseline offset. Single-trial firing rates approximate drift diffusion in LIP—the accumulation of noisy evidence—whereas single-trial firing rates in SC exhibit a large saccadic burst at the time of the saccade, preceded by occasional nonsaccadic bursts. Neuropixels recording in the LIP overcomes this challenge. These recordings yield 50–250 simultaneously recorded neurons, of which 10–35 share an RF that overlaps one of the contralateral choice targets used by the monkey to report its decision. The average activity of these target-in-RF (Tin) neurons during a single decision tracks the monkey's accumulated evidence as it contemplates its options. Neuropixels technology also enables multi-area recordings from ensembles of neurons that share common features (Fig. For example, neurons in the deeper layers of the superior colliculus (SC) receive input from LIP and, like LIP neurons, also have spatial RFs and decision-related activity (Fig. An ideal experiment to understand how the two areas interact is to record simultaneously from populations of neurons in LIP and SC that share the same RF. This experiment is almost impossible with previous recording technology because of the lack of anatomical organization in the LIP. This second challenge is also overcome by Neuropixels recording in the LIP, allowing for post-hoc identification of neurons in the LIP and SC with overlapping RFs (Fig. Unlike LIP, single-trial analysis of the SC population revealed dynamics that are not consistent with drift diffusion (Fig. Instead, the SC exhibits bursting dynamics, which were found to be related to the implementation of a threshold computation44. Understanding how the anatomical structure of specific neural circuits implements neural computations remains an important but elusive goal of systems neuroscience. One step toward connecting disparate levels of experimental inquiry is mapping correlative measures of relative spike timing between pairs of neurons, which is indicative of either synaptic connection between two neurons or shared input drive6,13,45. This is often impractical or extremely challenging when recording from only a small number of neurons, because the likelihood of recording from a pair of neurons with a statistically significant peak in the spike cross-correlogram (CCG) can be quite low. The Neuropixels 1.0 NHP probe typically yields 200–450 (and sometimes more) neurons when recording with 384 channels in cortical tissue. Applying the same methodology established in ref. 6 to 13 sessions from rhesus PMd and M1 yielded 111 ± 89 putative connected pairs per session and a connection probability of 0.73 ± 0.61%. Figure 8a shows three example jitter-corrected CCG plots between pairs of neurons with significant peaks in the CCG. In many examples the CCG peak lagged between one neuron relative to the other, consistent with a 1- to 2-ms synaptic delay. For other neuron pairs, the CCG peak is synchronous between the two neurons, suggesting that they may receive common input (Fig. 6 illustrates the distribution of spike–timing delays for 479 neurons from an example recording session. Nearby neurons are more likely to exhibit significant CCG peaks than neurons located further apart. Using this approach, we can map the full set of putative connections for a given recording across the cortical lamina (Fig. a, Jitter-corrected CCGs of four example pairs of neurons exhibiting significant correlations in spike timing. 7x STD, 7 × standard deviation of CCG flank. b, Diagram of functionally connected neuron pairs from one example session, with neurons ordered by depth along the probe. c, Distribution of signal correlation for pairs of neurons with different CCG types in the visual cortex. d, Relationship between signal correlation and peak value of CCG. e,f, Example putative connected cell pair identified using two probes in area AF (e) and ML (f) with putative feedforward connection. g,h, Example putative connected cell pair identified using two probes in area ML (g) and AF (h) with putative feedback connection. i, The population of functionally connected cells between AL and MF regions dominated by cells that respond to faces. For example, neuron populations with diverse RFs obtained from multiple visual areas (shown in Fig. 2) allow us to quantitatively determine the association between CCG peaks and RF overlap. Using signal correlation (rsig) as a measure of such tuning similarity in visual fields46, we showed that functionally connected neuron pairs exhibit higher rsig compared with nonsignificant pairs (Fig. Specifically, synchronous pairs (putatively receiving common inputs) tend to share highly overlapping RFs (rsig mean = 0.60, P < 10−4 compared with nonsignificant pairs), whereas asynchronous pairs are more likely to share moderately overlapping RFs (rsig mean = 0.43, P < 10−4 compared with nonsignificant pairs and P < 10−4 compared with synchronous pairs). This same methodology can be applied to assess CCG peaks between multiple simultaneously recorded regions recorded using separate probes to assess feedforward and feedback signaling between two regions. Figure 8e shows the jitter-corrected CCG for a pair of neurons where one neuron is located in face-patch ML and the other in face-patch AF in the IT cortex. Figure 8f shows a single-trial spike raster of the downstream neuron in area AF, triggered on spikes of the neuron in ML, illustrating a possible feedforward signaling, whereas Fig. 8h illustrates a putative feedback connection with opposite timing response from the cell pair shown in Fig. Remarkably, pairs of face cells were >10× as likely to have significant peaks in CCG (2.3%, 125 of 5,368 face cell pairs) as pairs where one or both cells were not face cells (0.15%, 18 of 11,912 other pairs) between the ML and AF (Fig. We have presented a new recording technology and suite of techniques to enable electrophysiological recordings using high-density integrated silicon probes in rhesus and other nonhuman primates. This technology enables large-scale recordings from populations of hundreds of neurons from deep structures in brain areas that are inaccessible using alternative technologies. Creating a long and thin probe shank also required developing approaches for reducing the bending that resulted from internal stresses within the shank. This technology combines the advantages of multiple approaches—recording with single-neuron spatial resolution and single-spike temporal resolution—whereas the long shank length provides recording access to most of the macaque brain. Programmable site selection enabled recording from multiple brain structures using a single probe, as well as surveying multiple recording sites along the shank without moving the probe. The combination of the compact form factor, commercially available and turn-key recording hardware and modest cost per channel relative to technologies like the Utah array enabled straightforward scaling in the size of simultaneously recorded neural populations. These capabilities could be essential for achieving accurate estimates of neural dynamics on single trials or for estimating the value of small-variance neural signals embedded in the neural population response. The high spatial resolution offered a number of advantages over sparser sampling, including high-quality single-unit isolation, ability to perform automated drift correction22,49 (Extended Data Figs. 4 and 5) and localization of the position and depth of the recording sites within a brain structure (for example, inferring probe depth with respect to cortical lamina), using current source density or other features of the recording. The high density also offered additional potential advances such as identifying putative neuron subclasses using extracellular waveforms5,10,50. Scaling up recordings using the Neuropixels 1.0 NHP probe was straightforward by recording from multiple probes simultaneously. In the present study, we demonstrated recording from up to seven probes and scaling this approach further is realistic. In practice, if many probes were used within a single brain area, the recording yield could be lower than a simple linear scaling, because it can be challenging to optimize the probe placement and insertion geometry for each probe independently. In addition, it was comparatively more complex to prevent any single probe from encountering resistance when transiting the dura. Care should be taken when inserting many probes within a small region, particularly in superficial structures, and the subsequent recordings could exhibit lower neuron yield than anticipated, based on estimates from single-probe recording sessions. Even given this potential for reduced yield, this is still an excellent approach for recording very large populations of neurons. Deep brain regions have historically received less attention as a result of the relative challenge of recording large numbers of neurons in these areas. The ability to conveniently record from many neurons is transformative, by enabling experiments that would be triaged otherwise, such as high-risk, high-reward projects, or simply additional control experiments. In addition to convenience, however, simultaneous large-scale population recordings from multiple structures make it possible to address questions that would not be possible otherwise, such as identifying communication between areas, for example, via communication subspaces51,52, or accurately estimating neural state on single trials53. The recordings presented in this report have exhibited lower probe drift (often <10 µm) than recent Neuropixels recordings in humans, which can exhibit hundreds of micrometers of drift10,11. These demonstrations were performed in an operating room during neurosurgeries, which involved creating large craniotomies (sometimes ~30–50 mm in diameter) and reflecting the dura to expose the brain. This rapid cyclical drift introduces a challenge for drift correction and spike-sorting algorithms, often reducing the neuron yield relative to a stable preparation. This challenge is largely absent in many rhesus macaque recordings, which are most often performed with the dura intact, and often using a smaller craniotomy (often as small as 3.5 mm, although this size is not essential for enabling stable recordings). For superficial recordings, such as those in the motor cortex, we placed a blunt guide tube on the surface of the dura with gentle compression (~0.5 mm), which eliminated most residual pulsation. Furthermore, animal experiments have the luxury of additional setup time relative to human recordings in the operating room, which must take place within a 15- to 20-min window. The commercially released version of Neuropixels 1.0 features two linear columns of recording sites, which is expected to further simplify algorithmic drift correction relative to the probe variant demonstrated here, which featured two zig-zag columns of recording sites. The Neuropixels 1.0 NHP probe is now commercially available and integrates seamlessly with the existing set of community-supported hardware and software tools for Neuropixels probes. The Neuropixels recording system is straightforward to set up and integrate with other experimental hardware, such as behavior or stimulus control computers. Combining the low total system cost (roughly US$7,000–15,000) and large-scale recordings enables a dramatic reduction in the recording cost per neuron acquired relative to existing technologies. Although highly capable, these probes are limited in several ways. First, this technology has not been optimized for simultaneous, dense sampling across a wide swath of cortex. For applications requiring horizontal sampling, planar recording technologies like the Utah arrays or 2P imaging may be more appropriate. Second, in contrast with many passive electrodes, it is not currently possible to use the Neuropixels probe to deliver intracortical microstimulation (ICMS), although future versions of the probe may add this functionality. The Neuropixels technology is, however, capable of recording while stimulating through external electrodes, as recently demonstrated by O'Shea et al.15. Third, the Neuropixels 1.0 NHP design is not explicitly optimized for chronic implantation. Although it is probably possible to leave the probe in place over multiple days or sessions, this possible capability remains untested and requires new implant designs. The probe base contains active electronics and is not designed for implantation under dura. As such, this probe is most appropriate for acute recordings, although it could conceivably be implanted for subchronic (multiple-week) recordings with appropriate insertion methods and hardware. Last, although it is theoretically possible to insert the entire 45-mm-long shank into the brain, inserting a probe this deep introduces additional practical challenges to overcome—primarily a requirement for precise alignment of the probe's insertion axis with the insertion location. Taken together, these methodological advances enable new classes of neuroscientific experiments in large animal models and provide a viable scaling path toward recording throughout the whole brain. All surgical and animal care procedures were performed in accordance with National Institutes of Health (NIH) guidelines and approved by the institutional animal care and use committees of each institution involved in the study, including Stanford University, University of California, Berkeley and Columbia University. The Neuropixels 1.0 NHP probe consists of an integrated base and ‘shank' fabricated as a monolithic piece of silicon using a 130-nm CMOS lithography process. The 6 mm × 9-mm base is mounted to a 7.2 × 23-mm2 printed circuit board (PCB), which is attached to a 7.2 × 40-mm2-long flexible PCB. The base electronics, headstage, cable, PXIe system and software are identical to the Neuropixels 1.0 probe. Data collection was performed using SpikeGLX software (https://billkarsh.github.io/SpikeGLX/) and the system is fully compatible with OpenEphys software. The commercial release of these probes features recording sites distributed in two aligned vertical columns, in contrast to the ‘zig-zag' columns described in the present report, to optimize data collection for automated drift correction and enhance automated spike sorting. A third variant, identical to Neuropixels 1.0 in length, but with a thicker shank (122 µm versus 25 µm), is also now commercially available on special request. Recording sites are 12 µm × 12 µm, made of titanium nitride and have an impedance of ±150 kΩ at 1 kHz. During recordings, electrical measurements were referenced to: (1) the large reference electrode on the tip of the probe; (2) an external electrical reference wire placed within the recording chamber; or (3) a stainless steel guide-tube cannula. Electrical signals are digitized and recorded separately for the action potential band (10 bits, 30 kHz, 5.7 µV mean input-referred noise) and local field potential (LFP) band (10 bits, 2.5 kHz). Recording sites are programmatically selectable with some constraints on site selection (see Extended Data Fig. 1 for a description of site-selection rules and common configurations). Spike sorting was performed using Kilosort 2.5 and Kilosort 3.0, and results were curated using Phy. Analysis was performed using customized scripts written in Matlab and Python, leveraging the open-source software package neuropixels-utils (https://github.com/djoshea/neuropixel-utils). Several distinct methods were used to mount and insert probes, guided by the unique constraints of inserting probes to different depths and depending on the recording chambers and mechanical access available for different primates used in these studies, as well as the existing hardware used by each of four distinct research groups. For single-probe insertions, probes were mounted using customized adapters to a commercially available probe drive (for example, Narishige, Corp.) and inserted through a blunt guide tube for superficial recordings and a sharp penetrating guide tube for deeper recordings. For this application, we developed several approaches to maintain precise alignment of the probe and drive axis. First, we employed a linear rail bearing (IKO International) and customized three-dimensional (3D) printed fixture to maintain precise alignment of the insertion trajectory. 4, we developed a dovetail rail system that maintains precise alignment between a penetrating guide tube and the Neuropixels probe. The choice of appropriate insertion method depends on the mechanical constraints introduced by the recording chamber design, the depth of recording targets, number of simultaneous probes required and choice of penetrating or nonpenetrating guide tube. The interaction of these constraints and a more thorough discussion of insertion approaches are provided on the Neuropixels users wiki (https://github.com/cortex-lab/neuropixels/wiki). Open-source designs for mechanical mounting components for Neuropixels 1.0 NHP to drives from Narishige, NAN and other systems are available in a public repository: https://github.com/etrautmann/Neuropixels-NHP-hardware. To minimize the impact of slow tissue drift during recordings, the Neuropixels probes were often inserted 150–300 µm past the desired target depth, then withdrawn by that amount and allowed to ‘settle' for 30–60 min before beginning an experiment. In addition, for superficial recordings in motor cortex, the probe was inserted through a blunt guide tube that was placed in contact with the dural surface and lowered by a small amount (~500 µm), gently compressing the dura to reduce the tissue motion resulting from pulse and respiratory rhythms. In vitro noise measurements are performed in the standard, self-referenced configuration as described in the Neuropixels manual, with the reference and ground connected together and to a platinum wire electrode in a saline bath. The noise on each channel is measured by averaging Fourier power spectra from 5× 3-s-long sections of data and estimating the root mean square (r.m.s.) Gain measurements were performed by connecting the probe ground and reference to the Faraday cage ground and a 1-mV, 3-kHz sine wave was applied to the saline bath through a Pt wire electrode. 2d shows the mean amplitude over channels versus bank; error bars = 1 s.d. Two male adult rhesus monkeys (Macaca mulatta, 11 and 16 kg), monkey T and monkey H, served as experimental subjects. Each animal was surgically implanted with a titanium head post and a cylindrical titanium recording chamber (30-mm diameter). In each animal, the placement of the recording chamber was centered at ~17 mm from the midline and ~7 mm behind ear-bar-zero, and a craniotomy was performed, allowing access to multiple visual areas in the superior temporal sulcus (STS). All surgeries were conducted using aseptic techniques under general anesthesia and analgesics were provided during post-surgical recovery. We measured visual RFs by randomly presenting a single-probe stimulus out of either a 7 (H) × 11 (V) stimulus grid extending 18 (H) × 30 (V) degrees of visual angles (d.v.a.) The stimulus consisted of a drifting Gabor gratings (2° in diameter, 0.5 cycle per ° in spatial frequency, 4 ° s−1 in speed, 100% Michelson contrast) and was presented for a duration of 0.1 s. Monkeys were rewarded with a drop of juice if they maintained fixating at the fixation spot throughout the trial. Neuropixels data were collected using SpikeGLX. Spike sorting was performed using either Kilosort 2.0 (single-bank recording, monkey T) or Kilosort 3.0 (multi-bank recording, monkey H) and manually curated with Phy. For a given stimulus location, we obtained the neuronal activity by counting all the spikes during the stimulus presentation period, accounted for by a time delay of 50 ms. Neuronal RFs were defined as the stimulus locations that elicited >90% of the peak visual responses. Details of the pacman behavioral task and experimental hardware presented in Fig. Three monkeys (M. mulatta) served as experimental subjects. Placement of the chambers was guided using structural magnetic resonance imaging (MRI). The recordings in monkey C were performed using a standard 19-mm plastic recording chamber (Christ, Inc.), whereas monkeys I and J were implanted with customized, low-profile, footed titanium chambers (Rogue Research). On a subset of 15 sessions in monkey C, we also targeted GPi in the basal ganglia. In monkey J, we reported data from one session while simultaneously recording in GPi, SMA and M1. For monkey C, the Neuropixels 1.0 NHP probe was held using a standard 0.25-inch dovetail mount rod with a customized adapter to mount it to a hydraulic drive (Narishige, Inc.). A 21G blunt guide tube, 25 mm in length, was held using a customized fixture and placed over the desired recording location. The dura was then penetrated with a tungsten electrode (FHC, size E), which was bent at 27 mm to prevent the tip from inserting further than 2 mm past the end of the guide tube. This electrode was inserted manually via forceps, once or several times, as necessary, which also provided feedback on the depth and difficulty in penetrating the dura. This procedure sometimes took several attempts to find the correct insertion point, but was generally successful in less than a few minutes. For monkeys I and J, the Neuropixels 1.0 NHP probe was held using a customized fixture mounted to a linear rail bearing (IKO, Inc.). This apparatus was designed to enable close packing of many probes and to solve the challenge of precisely targeting structures deep in the brain without trial and error. The linear rail is mounted in a customized, 3D printed base, which mounts directly to the recording chamber. This base also provides support for either sharp or blunt guide tubes, as required. In general, blunt guide tubes were preferred, but if necessary sharp guide tubes were sometimes used when the dura had become thicker and difficult to penetrate. This apparatus greatly simplifies the procedure of using many probes in a small space, while not relying on commercial drives to provide the mechanical rigidity required to safely insert a delicate probe. Additional details on the customized hardware are provided in the Neuropixels 1.0 NHP user wiki: https://github.com/etrautmann/Neuropixels-NHP-hardware. Principal component analysis trajectories were calculated after smoothing spikes with a 25-ms Gaussian kernel and averaging across successful trials. Rastermap was run with default parameters after normalizing trial-averaged neural activity. An offline force model prediction performance was computed using a 50-ms time lag between arm force and neural activity. Neurons were randomly subselected and 80% of trials from six target conditions were used to train a linear regression model in Python, using scikit-learn, whereas the remaining 20% of trials were used to calculate model performance. Ten iterations were performed for each level of neurons retained. Probe drift calculations were performed using KS 2.5. Two monkeys (M. mulatta, 8–11 kg) served as experimental subjects. In each, a head post and two recording chambers were implanted using aseptic surgical procedures and general anesthesia. Placement of the LIP chamber was guided by structural MRI. A schematic of the task is displayed in Fig. 7a and its description can be found in the corresponding figure legend. A second (control) task—an oculomotor delayed response task—was used to measure the response fields of neurons in LIP and SC, described in the legend for Fig. We conducted eight recording sessions in which activity in the LIP and SC was recorded simultaneously. In the LIP we used a single Neuropixels 1.0 NHP probe, yielding 54–203 single units per session. In the SC, we used 16-, 24- and 32-channel V-probes (Plexon) with 50- to 100-µm electrode spacing, yielding 13–36 single units per session. In each session, we first lowered the SC probe and approximated the RFs of SC neurons using a few dozen trials of a delayed saccade task. If the RF locations in the SC were suitable, we then lowered the Neuropixels probe into the LIP through a dura-penetrating, stainless steel guide tube (23G) at 5 µm s−1 using a MEM microdrive (Thomas Recording) that was attached to a chamber-mounted, three-axis micromanipulator. Custom-designed adapters were used for mounting the Neuropixels probe on to the drive (wiki: https://github.com/etrautmann/Neuropixels-NHP-hardware). Once the target depth was reached (~10 mm below the dura), we allowed 15–30 min of settling time to facilitate recording stability. To precisely measure RF locations in both areas, the monkeys performed 100–500 trials of the delayed saccade task and LIP neurons with RFs that overlapped those of the SC neurons were identified post-hoc. Finally, the monkeys performed a reaction-time RDM discrimination task until satiated (typically 1,500–3,000 trials). We restricted our analysis of the LIP data to neurons with RFs that overlapped those of the simultaneously recorded SC neurons (164 of 1,084 total LIP neurons). Spike trains were discretized into 1-ms bins and convolved with a Gaussian kernel (σ = 25 ms) to produce the single-trial activity traces depicted in Fig. Two monkeys (M. mulatta) served as experimental subjects. Each animal was surgically implanted with an MRI-compatible Ultem head post and a large rectangular recording chamber (61 at 46-mm diameter, 65 at 50-mm diameter and 58 at 61-mm diameter, respectively), covering most of the animal's acrylic implant. Monkeys were trained to passively fixate on a spot for juice reward while visual stimuli of 5° size, such as images of faces or objects, were presented on a liquid crystal display (LCD) screen (Acer). We targeted face patches ML and AF (monkey 1) and face-patch AM (monkey 2) in the IT cortex for electrophysiological recordings. Face patches were identified using functional (f)MRI. Monkeys were scanned in a 3T scanner (Siemens), as described previously56. MION contrast agent was injected to increase the signal:noise ratio. During fMRI, monkeys passively viewed blocks of faces and blocks of other objects to identify face-selective patches in the brain. During electrophysiology, monkeys viewed two stimulus sets: one consisting of the same stimuli shown during fMRI, consisting of faces and nonface objects, with 150 ms of ON time and 150 ms of OFF time (Fig. While inserting tungsten electrodes, we performed structural MRI to confirm correct targeting (Fig. Subsequently, we performed a total of 72 Neuropixels insertions. To perform very deep recordings (for example, 42 mm from the craniotomy; Fig. 4d), we lowered a cannula holder to touch or gently push the dura. A probe holder, which held the probe, was slid through the cannula holder via matching dovetails. Neuropixels data were recorded using SpikeGLX and OpenEphys and spikes were sorted using Kilosort 3.0. Functional interactions between pairs of neurons were measured with an established crosscorrelation method. CCGs were calculated using spike trains from pairs of simultaneously recorded neurons, during either the whole stimulus presentation period or the intertrial intervals. \(\theta (\tau )\) is a triangular function calculated as \(\theta (\tau )=N-\left|\tau \right|\) which corrects for the overlapping time bins at different time lags. A jitter-corrected method was used to remove correlations caused by stimulus locking or slow fluctuations: The correction term (CCGjittered) captured slow correlation longer than the jitter window (caused by common stimulation or slow fluctuation in the population response), so, once it's subtracted, only the fine temporal correlation is preserved. A 25-ms jitter window was chosen based on previous studies6. Only well-isolated single units with a firing rate of at least 1 Hz were included for CCG. As with previous studies6,13, in the present study a CCG is classified as significant if the peak of jitter-corrected CCG occurred within 10 ms of zero time lag and if this peak is >7 s.d. This calculation is performed using the neuropixel-utils library available online (https://github.com/djoshea/neuropixel-utils). For many of the analyses presented here, standard statistical tests were used. For all experiments, animals were trained on the task before any recording. Eight adult male M. mulatta animals were used in the results presented here, with data presented for two animals for each of the four example use cases (multiple visual areas, M1, LIP and IT cortex). No statistical method was used to predetermine sample size for each experiment. The Investigators were not blinded to allocation during experiments and outcome assessment, but the analysis was largely automated, reducing the influence of subjective judgments. In most cases, blinding was not relevant to the technical proofs of concept that we present in this report. Spike sorting was conducted primarily using automated spike-sorting algorithms (Kilosort 2.5 and 3.0), followed by some minimal manual curation and inspection of cluster isolation quality. This manual step was performed by different individuals in different labs, but none of the results presented here are highly sensitive to the individual choices made during spike sorting. For most experiments, datasets from specific sessions were excluded from further analysis if large-scale probe movement precluded automated spike sorting with Kilosort, but this happened only occasionally and was not a frequent occurrence. The stability recording results presented in Figs. The fraction of datasets with poor recording quality or excessive drift is probably not generalizable to other preparations. Instead, our purpose was to illustrate that highly stable recordings are achievable in multiple brain areas with relative ease. Similarly, we make no statistical claims about the expected yield of neurons recorded on a probe in a given session; instead we present several observed values from these datasets. We feel that this is important because accurate neuron counts remain subjective if manual curation is performed. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data to replicate key analyses shown in Figs. 1–6 and 8 are available via Zenodo at https://doi.org/10.5281/zenodo.14744139 (ref. 7 are available via Zenodo at https://zenodo.org/records/7946011 (ref. Code to replicate the analysis shown in Fig. 7 is available via Zenodo at https://zenodo.org/records/7946011 (ref. Fully integrated silicon probes for high-density recording of neural activity. Steinmetz, N. A. et al. Neuropixels 2.0: a miniaturized high-density probe for stable, long-term brain recordings. Steinmetz, N. A., Zatka-Haas, P., Carandini, M. & Harris, K. D. Distributed coding of choice, action and engagement across the mouse brain. High-density extracellular probes reveal dendritic backpropagation and facilitate neuron classification. Siegle, J. H. et al. Survey of spiking in the mouse visual system reveals functional hierarchy. Allen, W. E. et al. Thirst regulates motivated behavior through modulation of brainwide neural population dynamics. Deep posteromedial cortical rhythm in dissociation. Gardner, R. J. et al. Toroidal topology of population activity in grid cells. Large-scale neural recordings with single neuron resolution using Neuropixels probes in human cortex. High-density single-unit human cortical recordings using the Neuropixels probe. Accurate estimation of neural population dynamics without spike sorting. & Moore, T. Functional interactions among neurons within single columns of macaque V1. Sun, X. et al. Cortical preparatory activity indexes learned motor memories. Direct neural perturbations reveal a dynamical mechanism for robust computation. The Utah intracortical electrode array: a recording structure for potential brain-computer interfaces. A floating metal microelectrode array for chronic implantation. Chronic, wireless recordings of large-scale brain activity in freely moving rhesus monkeys. & Gray, C. M. A large-scale semi-chronic microdrive recording system for non-human primates. Mao, D. et al. Spatial modulation of hippocampal activity in freely moving macaques. Rominger, J. P. & Lin, B. J. Seamless stitching for large area integrated circuit manufacturing. & Van Essen, D. C. Distributed hierarchical processing in the primate cerebral cortex. & Tootell, R. B. H. Faces and objects in macaque cerebral cortex. Orban, G. A., Van Essen, D. & Vanduffel, W. Comparative mapping of higher visual areas in monkeys and humans. Gattass, R. & Gross, C. G. Visual topography of striate projection zone (MT) in posterior superior temporal sulcus of the macaque. Maguire, W. M. & Baizer, J. S. Visuotopic organization of the prelunate gyrus in rhesus monkey. Baker, S. N. The primate reticulospinal tract, hand function and functional recovery. Bhandari, R., Negi, S. & Solzbacher, F. Wafer-scale fabrication of penetrating neural microelectrode arrays. Flexible neural control of motor units. Stringer, C. et al. Rastermap: a discovery method for neural population recordings. A map of object space in primate inferotemporal cortex. Comparing face patch systems in macaques and humans. A new no-report paradigm reveals that face cells encode both consciously perceived and suppressed stimuli. Roitman, J. D. & Shadlen, M. N. Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task. Shadlen, M. N. & Kiani, R. Decision making as a window on cognition. Direct observation of the neural computations underlying a single decision. Stine, G. M., Trautmann, E. M., Jeurissen, D. & Shadlen, M. N. A neural mechanism for terminating decisions. Jia, X. et al. Multi-regional module-based signal transmission in mouse visual cortex. Ts'o, D. Y., Gilbert, C. D. & Wiesel, T. N. Relationships between horizontal interactions and functional architecture in cat striate cortex as revealed by cross-correlation analysis. Windolf, C. et al. DREDge: robust motion correction for high-density extracellular recordings across species. Lee, E. K. et al. Non-linear dimensionality reduction on extracellular waveforms reveals cell type diversity in premotor cortex. Semedo, J. D., Zandvakili, A., Machens, C. K., Yu, B. M. & Kohn, A. Cortical areas interact through a communication subspace. Vyas, S., Golub, M. D., Sussillo, D. & Shenoy, K. V. Computation through neural population dynamics. Direct observation of the neural computations underlying a single decision. & Shadlen, M. N. The neural basis of decision making. & Wurtz, R. H. Visual and oculomotor functions of monkey substantia nigra pars reticulata. Relation of visual and auditory responses to saccades. Tsao, D. Y., Freiwald, W. A., Tootell, R. B. H. & Livingstone, M. S. A cortical region consisting entirely of face-selective cells. Large-scale high-density brain-wide neural recording in nonhuman primates—code and data. We thank B. Schneeveis and T. Tabachnik for engineering assistance. In addition, we thank Columbia University's ICM for the quality of care that they provided for our animals, especially during the pandemic and lockdown. We thank W.-l. Sun, for probe testing and software development (HHMI Janelia). was supported by the NIH National Research Service Award, National Institute of Neurological Disorders and Stroke (F32). were supported by the NIH Brain Initiative (grant no. was supported by the NIH/National Institute of Mental Health (grant no. ws supported by the National Eye Institute (grant nos. was supported by the American Parkinson Disease Post-Doctoral Fellowship. were supported by the NIH (grant nos. The funders had no role in study design, data collection and analysis or decision to publish the manuscript. These authors contributed equally: Janis K. Hesse, Gabriel M. Stine, Ruobing Xia, Shude Zhu. These authors jointly supervised this work: Tirin Moore, Michael Shadlen, Krishna Shenoy, Doris Tsao, Barundeb Dutta, Timothy Harris. Department of Neuroscience, Columbia University Medical Center, New York, NY, USA Eric M. Trautmann, Andrew Zimnik, Elom Amematsro, Natalie A. Steinemann, Mark Churchland & Michael Shadlen Zuckerman Institute, Columbia University, New York, NY, USA Eric M. Trautmann, Saurabh Vyas, Andrew Zimnik, Elom Amematsro, Natalie A. Steinemann, Mark Churchland & Michael Shadlen Grossman Center for the Statistics of Mind, Columbia University, New York, NY, USA Eric M. Trautmann, Elom Amematsro & Mark Churchland Meta Reality Labs, Seattle, WA, USA Janis K. Hesse, Daniel A. Wagenaar & Doris Tsao Ruobing Xia, Shude Zhu & Tirin Moore Ruobing Xia, Shude Zhu, Tirin Moore & Krishna Shenoy Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA, USA Bill Karsh, Jennifer Colonell, Marius Pachitariu & Timothy Harris Computation & Neural Systems, California Institute of Technology, Pasadena, CA, USA Alexandru Andrei, Carolina Mora Lopez, John O'Callaghan, Jan Putzeys, Bogdan C. Raducanu, Marleen Welkenhuysen & Barundeb Dutta Kavli Institute for Brain Sciences, Columbia University, New York, NY, USA Howard Hughes Medical Institute, Columbia University, New York, NY, USA Howard Hughes Medical Institute, Berkeley, CA, USA Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, USA Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Correspondence to Eric M. Trautmann, Tirin Moore or Barundeb Dutta. works for Meta Reality Labs, although the work presented in the present article was performed via his roles at Columbia, University of California, Davis and Stanford. The other authors declare no competing interests. Nature Neuroscience thanks Ueli Rutishauser and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. a, Site selection rule - any electrode selected on any bank is unavailable for selection in other banks. Multiple electrodes can be connected to a single readout channel, as explored by ref. b, Dense recording from Bank 0, places 384 channels spanning 3.84 mm. c, half density configuration covering Banks 0 and 1, spanning 7.68 mm. d, Quarter density, covering banks 0–3, spanning 15.36 mm. a, Distribution of measured noise (mean +/− SD) for 384 recording channels across probe banks 0-11. b, Correlation of noise measured on the same readout channel for all recording channels recording from bank 3 (abscissa) and bank 7 (ordinate). Correlation between all pairs of banks is similar. c, Distribution of difference of noise recorded from one readout site and other sites that can be connected to the same readout channel. For the probe tested, a total of three sites had measured noise > channel mean + 2 uV. d, Measured peak-to-peak amplitude of a 1 mV, 3 kHz sine wave (mean +/− SD) as a function of position along the probe. Additional methodological details provided in the Materials and Methods Summary. a, Distribution of receptive fields (RFs) of 2729 visually responsive neurons across cortical depth from the 5-banks recording across multiple visual areas. RFs from the superficial and deeper part of the brain are demonstrated separately for clarity. c, Polar angle (theta) and Eccentricity (rho) of each RF's geometric center across cortical depth. d, Left, heat map of evoked responses across drift directions of grating (vertical thickness is greater for less dense neuronal population). Color scale represents the magnitude of evoked responses. In c and d, each neuron is plotted at its corresponding cortical depth. Horizontal lines denote the section of cortex where the center of RFs falls on Lower vertical meridians (LVM), horizontal meridian (HM), upper vertical meridian (UVM), and horizontal meridian (HM), respectively, superficial to deep. Putative visual areas are identified and labeled. LVF: lower visual field; UVF: upper visual field; FST: fundus of the superior temporal (FST) area. e, Polar angle (theta) and Eccentricity (rho) of each RF's geometric center across cortical depth calculated using the first and second half of trials within a session, demonstrating stability of function properties of neurons recorded throughout a session. a, Probe drift estimates computed using Kilosort 2.5 for six example stable sessions in motor cortex (top) and IT cortex (bottom). b, Drift estimates for two less-stable sessions, including both slow drift of the probe within neural tissue and rapid shifts. Unstable sessions were comparably uncommon, and typically resulted from not placing gentle pressure on the surface of dura with a guide tube. Importantly, data from sessions with probe drift can often still be used, as demonstrated by refs. a, spike raster plot with spikes identified via Kilosort 3 and plotted at estimated depth along the probe. a, Latency distribution between identified peaks in cross-correlograms of neurons with statistically significant spike timing relationships. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Large-scale high-density brain-wide neural recording in nonhuman primates. Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/podcast/episode/how-h5n1-went-from-an-illness-in-wild-birds-to-a-global-pandemic-threat/'>There Is a Beach That Contains Clues of How a Bird Flu Pandemic Could Take Off</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 10:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>How Bird Flu Became a Human Pandemic Threat The first hints that a new strain of avian illness is emerging could be found on this beach on Delaware Bay, where migrating birds flock. Here's what virus detectives who return there every year know right now. Young, Jeffery DelViscio, Fonda Mwangi, Alex Sugiura & Rachel Feltman This week Science Quickly is doing a three-part deep dive to bring you the latest research on bird flu. From visiting dairy farms to touring cutting-edge virology labs we'll explore what scientists have learned about bird flu—and why it poses such a potential risk to humans. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. Today's episode brings us back to the start: the wild flocks where new strains of bird flu evolve and spread. Our host is Lauren Young, associate editor for health and medicine at Scientific American. Lauren Young: Out on Norbury's Landing, a small strip of sandy beach at the southern tip of New Jersey on the Delaware Bay, Pamela McKenzie peers through her binoculars at a massive flock of shorebirds. McKenzie: It's just, like, a sea of red bellies. Young: A flurry of different migratory birds, including red knots, ruddy turnstones and sanderlings, are making a pit stop on their long migration up to the Arctic Circle. The birds are just in sight, and Pam desperately wants to get closer without disturbing them. McKenzie: Of course, right where we need to go. But Pam is out here collecting bird poop. Pamela McKenzie of St. Jude Children's Research Hospital collects avian fecal samples at a beach in Delaware Bay in New Jersey. McKenzie is a virus hunter who has returned to the beaches in the area for years looking for new strains of avian influenza, including H5N1. Every year in mid-May she hops between the various beaches of Delaware Bay, scooping poop that just might contain avian influenza viruses. By the second day of this year's collection her team had already found samples that came back positive for different bird flu viruses, but not the headline-making H5N1—at least not yet. McKenzie: What's unique about Delaware Bay is that it's a hotspot for influenza. As director of surveillance for the St. Jude Center of Excellence for Influenza Research and Response she and her fellow research scientists take an annual visit to Delaware Bay. They do this to stay on top of the avian influenza viruses actively circulating in the flocks of migrating shorebirds. Webster: Vast quantities of virus [were] in the feces. And so going back to Delaware Bay we didn't have to catch the birds; we simply followed them and took fecal samples from the beach when they pooped. [CLIP: Waves crash on the beach, and birds caw.] Young (tape): So that one is attached and mating. McKenzie: Yeah, so if she was over here laying eggs, it would be trying to fertilize the eggs, and—like right here: see how its claws are attached to her? Young: The heavily armored crabs look a bit alien, and sometimes a bit silly, as they draw odd tracks in the sand like uncoordinated Roombas. Those eggs provide the perfect buffet for the migrating birds that need to bulk up before their next leg in their journey. As time passes and the tide retreats, first the birds swoop in to feast, and then Pam comes in, hot on their trail of droppings. After 15 years of doing this work Pam has developed a special eye for bird poop. She can make a pretty good first guess of what poop belongs to the migratory bird she's most interested in. Pamela McKenzie inspects a sample on the beach in southern New Jersey. She is collecting bird poop to test for avian influenza. Young: Fresh poop is best—damp but not drenched from the tide. Influenza subtypes are generally classified based on two specific surface proteins: hemagglutinin and neuraminidase. There are 144 H and N possible combinations of avian influenza. Webster: Amongst those was H5N1, indeed, but not from the European or Chinese ones. Young: The particular shorebirds stopping by Delaware Bay might not be carrying the kind of bird flu that could be dangerous to domestic animals or humans. But with the right genetic mixing we could potentially see outbreaks of a new “killer” strain like the one currently ripping through U.S. farms. But to really understand the high-pathogenic H5N1 in our cows and chickens—and where it might go from here—we have to go back in time and look at wild birds. A highly pathogenic, or “high-path,” virus, as many influenza researchers like to call it, can wipe out an entire poultry flock in just a few days. Sporadic spillovers from wild to domestic birds have continued ever since. Keiji Fukuda: In the influenza field it was clear that there was a very large group of influenza viruses, which infected birds and sometimes infected animals, and then there was a much smaller group of human influenza viruses, which infected people. Young: That's Keiji Fukuda, a retired physician and influenza epidemiologist who worked for various institutions, including the University of Hong Kong, World Health Organization and the U.S. Centers for Disease Control and Prevention. Young: That changed in 1997, when a previously healthy three-year-old boy in Hong Kong was hospitalized and developed a severe pneumonia. Webster: It couldn't be identified at CDC. It couldn't be identified in London or in Holland, where they sent it, and they applied to me for the whole range of influenza virus reference serum, and they identified this virus as an H5, an H5N1. And no one would quite believe that this virus had killed the child. Young: That shocked scientists and public health leaders, including molecular virologist Nancy Cox. Nancy Cox: We didn't expect to see high-path avian influenza viruses infecting humans. It was really quite out of left field. Fukuda: How could this boy have become infected? Fukuda: Was this boy associated with any kind of unusual exposures? Cox: Were there other cases that had yet to be identified in Hong Kong? Young: Everyone hoped the child was a tragic one-off case. [CLIP: Fukuda responds to the reporter: “Well, by stronger, you mean it could become more adapted to humans and sort of pass through? Young: That was younger Keiji back in 1997, talking to a reporter at a press conference in Hong Kong as the outbreak was unfolding. Fukuda: We're dealing with a virus which has remained persistent for at least some period of time, and we have no idea: “Is this the beginning of another pandemic?” And the investigations took on a whole different flavor. Young: Keiji says the team eventually determined that the virus seemingly spread through traditional live bird markets, often referred to as wet markets. Guided by public health advisers, government officials ordered that the markets suspend sales and get cleaned—and that farms and markets cull all poultry. Fukuda: At that time it was a very kind of disquieting decision and implementation. Young: Although it was a brutal decision for farmers and sellers the tactic worked, effectively squashing an outbreak that seemed on the verge of taking off. Thankfully there was no evidence of human-to-human transmission, which is key to kick-starting a pandemic. Cox: What we saw at the very beginning of the H5N1 outbreak back in 1997 is that the viruses that we identified from poultry and from people were really very, very similar. Young: But she says a lot has happened since the 1997 Hong Kong outbreak. Cox: Now we've had this virus circulating globally, and what we're seeing is a huge amount of diversity, and what does that mean? Young: As H5N1 has fanned across the globe over the years its activity has been a bit like a simmering volcano: occasionally waking up in dramatic spurts, only to go quiet again. And each time it flares up the virus gets a new opportunity to tweak itself—ever so slightly. Wendy Puryear: During that whole 30-year time period there continued to be ongoing evolution and shifts and changes in the virus. Young: Wendy Puryear is a scientist studying influenza evolution and adaptation at the Cummings School of Veterinary Medicine at Tufts University. She's been watching with increasing unease how changes, or mutations, are creating a vast diversity of H5N1 viruses—including ones that might be better at infecting different animals. So this is one that we've been worried about for a long time. Young: Wendy says H5N1 keeps hitting mutation milestones that are getting too close for comfort. Puryear: We keep going further down that road of “at least it hasn't.” “At least it hadn't gone into a lot of wild animals and was disseminating around the globe.” And [starting] a few years ago the number of bird species carrying H5N1 has ballooned. More than 500 different avian species, ranging from seabirds to songbirds, have tested positive for H5N1, according to the Food and Agriculture Organization of the United Nations. Well, at least it wasn't going into mammals.” Young: Then around 2020 and 2021 highly pathogenic H5N1 started to infect different mammals, to date affecting more than 90 different species in total, including coyotes, minks, opossums, skunks and rodents. The virus had previously been found in the occasional fox or tiger, typically predators that might've eaten an infected wild bird. But the list of newly infected mammal species is growing in a way that hasn't been seen before. Pruyear: “At least there wasn't evidence of mammal-to-mammal transmission.” Well, then we had that in marine mammals in South America. Young: In 2022 and 2023 the virus spread among various marine animals along the coast of Peru and Chile, killing more than 30,000 sea lions. Groups of dolphins, porpoises and otters were also infected. Young: No one expected the virus to hit U.S. dairy cows. How it got onto farms in the first place is still a bit of a mystery; you'll hear a lot more about that in the next episode of this three-part series. But it's important to say that scientists do have a strong hunch about how the virus made that jump—and you probably guessed it: wild birds. Louise Moncla: There's this whole diversity of low-path viruses that don't really cause as many problems that circulate endemically in these wild birds in North America. Moncla: Through this process called reassortment this incurring kind of new virus that entered started mixing with those viruses, and so we now have this diverse mixture of viruses sort of circulating in wild birds, resulting in the emergence of these new genotypes ... Young: New genotypes, or unique genetic profiles, like the high-path H5N1 that scientists think started infecting dairy cows. This genetic mixing, or reassortment of different influenza viruses, occurs when they co-infect one host: a bird, an animal or, worse, a human. That opens up the window for genetic information to be exchanged. Here's Wendy again to unpack a bit of what Louise said. Puryear: Not only do you have this regular evolution that happens with the virus being sloppy in how it replicates, but the fact that it has its genome on separate pieces, its genetic information is actually—those genes are on separate chunks of, of RNA, and that means that it can take a whole gene and swap it out with a different form of influenza, so that gives a whole new kinda Frankenstein version of the virus that can then move forward. Young: And this process of virus info swapping can potentially spiral into something much bigger—and deadlier. Moncla: Reassortment is a really important process for influenza evolution because it has led to every past pandemic that we know about. So we usually get influenza pandemics when viruses from two different species mix via reassortment and [that] results in a virus for which a host population like humans doesn't have any prior immunity. Young: But those viral swap meets leave footprints—clues that help researchers like Wendy and Louise track influenza evolution through time and space. So these genomes provide this nice little map of how this virus has been moving between different host species or populations or geographic areas. Young: And wild birds help paint a picture of where the virus might be now and where it might go next. These feathered virus carriers have effectively moved influenza around the world and into our domesticated animals. But Louise, Wendy, Nancy, Keiji and folks at St. Jude are all quick to say that migrating birds and wildlife shouldn't be blamed for H5N1's current stronghold—it's the way that humans monitor and respond to the situation. Moncla: Now that these viruses are really being driven by transmission of wild birds we need to understand how these viruses evolve in wild birds a lot better. And so something I'm really hoping continues to happen is surveillance in wild birds. You know, so without this kind of continuous surveillance effort in wild birds we wouldn't have been able to understand the outbreak in dairy cattle or these human spillovers and where they're coming from. Young: Wild birds can't be stopped, but they can be watched—just like how the St. Jude group is surveying the shorebirds at Delaware Bay, year after year. Young: Back in Delaware Bay, armed with vials of bird poop and a compact scientific camper van, another virus hunter is doing exactly that. I am the director of laboratory operations for the Webby Lab group at St. Jude Children's Research Hospital, which is a—our lab group is a large influenza research laboratory. Young (tape): Great, and tell us where we are right now. Lisa Kercher, the director of laboratory operations for the Webby Lab group at St. Jude Children's Research Hospital, stands in the doorway of her converted camper that she has retrofitted as a mobile avian influenza testing laboratory. Young: Lisa lives part time in her truck and camper, living and sleeping alongside carefully stored poop samples preserved in cold liquid nitrogen. Young: Her working area is stocked with protective gear, reagents, pipettes, well plates and a variety of miniature equipment, including a PCR machine that can quickly amplify DNA from samples Pam collected the day before. Kercher: It can then immediately run a PCR for flu and for H5, and I know on my little laptop here if that's positive within about an hour. Young: At that point in site collecting she had processed 250 fecal samples. Before she started doing this real-time surveillance work two years ago, the team wouldn't know what avian influenza subtypes they had on their hands until about six months after the sample collections. Kercher: It's very hard to do epidemiology of how the virus is moving and tracking when you spend six months waiting for the sequence to come out of a national lab or any big lab. It's just hard logistically to then backtrack and figure that out. And then you are usually faced with a whole different virus by that time. Young: But if Lisa wants to be really fast, she needs more data. Kercher: So when you get a flu virus from a fecal sample you have to give it a name, you have to same say the species that it came from. That's why Lisa is teaming up with local wildlife ecologists. Young (tape): So what do we have here today? Larry Niles: Most of those noises are from the sanderling. Niles: We caught a handful of [red] knots and ruddy turnstones. Larry Niles holds a sanderling that is about to be weighed and measured and sampled at a beach in Delaware Bay in New Jersey. He co-leads the Delaware Bay Shorebird Project with Wildlife Restoration Partnerships. Larry has been catching shorebirds here as part of his conservation work for the last 29 years. And yes, his group uses something called a cannon net to nab these birds because ... Young: The birds are temporarily corralled in cloth-covered boxes, waiting for Larry and the other researchers and volunteers to gently pull them out to collect various data points. And though Larry has his own research to do on the ecology of the birds—their health, their population numbers and what might threaten those things—he says it's a real bonus to have virus detectives like Pam and Lisa to work with, side by side. Young: For Lisa, getting access to the shorebirds directly unlocks all sorts of crucial information. Young: There are four main flyways, [also] called avian superhighways, that run through North America. But these avian superhighways have also become increasingly concerning for H5N1. Lisa Kercher, the director of laboratory operations for the Webby Lab group at St. Jude Children's Research Hospital, works in her converted camper on avian fecal samples. Kercher: Those birds are carrying this virus in greater numbers and in lots more areas where there's potential for spillover into domestic poultry farms. And of course, this happened in the dairy farms—it spilled over into the cattle, so this virus is now very prevalent all over North America. But the flyways are important because the birds that carry it are moving quickly down the flyway in a very short period of time, and you have a lot of opportunity for spillover there. Young: Lisa says that speed matters for a rapidly changing virus like H5N1. She could imagine her mobile lab getting scaled up into a large biosurveillance network: multiple satellite labs dotted up and down all the flyways, relaying genetic sequences to other influenza trackers like Louise and Wendy but also to farmers on the ground trying to keep their chickens and cows healthy. Kercher: Wouldn't it be great if the farmer had a way to go on his computer and look at a dashboard and say, “Wow, I wonder where the flu is?” They need to know where it is circulating in the wild birds. And if they knew where it was ahead of time—or at least where it was coming from—they would have an opportunity, if they chose, to up their biosecurity a little bit. Young: Until then virus detectors like Pam and Lisa continue to keep a watchful eye on the surprising twists and turns of H5N1, looking to the birds and the clues they leave behind. Kercher: We'll never catch up with Mother Nature. We're never gonna catch up with the virus and how it mutates. But if we can get closer and approach it more, you can then look for mutations, much quicker things that make the virus resistant to antivirals or things that make it more mammalian adaptable. You would wanna know that sooner rather than later. Feltman: That's all for today's episode, but there's lots more to come. Tune in on Wednesday for part two of our special series on bird flu, which explores how avian influenza made its unprecedented leap into cattle. Science Quickly is produced by me, Rachel Feltman, along with Fonda Mwangi, Kelso Harper, Naeem Amarsy and Jeff DelViscio. Shayna Posses and Aaron Shattuck fact-check our show. Our theme music was composed by Dominic Smith. Subscribe to Scientific American for more up-to-date and in-depth science news. For Scientific American's Science Quickly, I'm Rachel Feltman. Young is an associate editor for health and medicine at Scientific American. She has edited and written stories that tackle a wide range of subjects, including the COVID pandemic, emerging diseases, evolutionary biology and health inequities. Young has nearly a decade of newsroom and science journalism experience. Before joining Scientific American in 2023, she was an associate editor at Popular Science and a digital producer at public radio's Science Friday. She has appeared as a guest on radio shows, podcasts and stage events. Young has also spoken on panels for the Asian American Journalists Association, American Library Association, NOVA Science Studio and the New York Botanical Garden. Her work has appeared in Scholastic MATH, School Library Journal, IEEE Spectrum, Atlas Obscura and Smithsonian Magazine. Young studied biology at California Polytechnic State University, San Luis Obispo, before pursuing a master's at New York University's Science, Health & Environmental Reporting Program. Jeffrey DelViscio is currently chief multimedia editor/executive producer at Scientific American. Before that he spent more than eight years at the New York Times, where he worked on five different desks across the paper. He was a Knight Science Journalism Fellow at the Massachusetts Institute of Technology in 2018. Fonda Mwangi is a multimedia editor at Scientific American. She previously worked as an audio producer at Axios, The Recount and WTOP News. He has worked on projects for Bloomberg, Axios, Crooked Media and Spotify, among others. Rachel Feltman is former executive editor of Popular Science and forever host of the podcast The Weirdest Thing I Learned This Week.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/06/250622225921.htm'>Affordances in the brain: The human superpower AI hasn't mastered</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 09:34:30
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Researchers from the University of Amsterdam have discovered unique brain activations that reflect how we can move our bodies through an environment. The study not only sheds new light on how the human brain works, but also shows where artificial intelligence is lagging behind. That sounds simple, but how does your brain actually determine these action opportunities? PhD student Clemens Bartnik and a team of co-authors show how we make estimates of possible actions thanks to unique brain patterns. The team, led by computational neuroscientist Iris Groen, also compared this human ability with a large number of AI models, including ChatGPT. "AI models turned out to be less good at this and still have a lot to learn from the efficient human brain," Groen concludes. Using an MRI scanner, the team investigated what happens in the brain when people look at various photos of indoor and outdoor environments. The participants used a button to indicate whether the image invited them to walk, cycle, drive, swim, boat or climb. At the same time, their brain activity was measured. "Psychologists call the latter "affordances" -- opportunities for action; imagine a staircase that you can climb, or an open field that you can run through." The brain did this even when participants were not given an explicit action instruction. The research thus demonstrates for the first time that affordances are not only a psychological concept, but also a measurable property of our brains. The team also compared how well AI algorithms -- such as image recognition models or GPT-4 -- can estimate what you can do in a given environment. "When trained specifically for action recognition, they could somewhat approximate human judgments, but the human brain patterns didn't match the models' internal calculations," Groen explains. "Even the best AI models don't give exactly the same answers as humans, even though it's such a simple task for us," Groen says. AI models can't do that because they only exist in a computer." "As more sectors -- from healthcare to robotics -- use AI, it is becoming important that machines not only recognize what something is, but also understand what it can do," Groen explains. Groen also points out the sustainable aspect of AI. "Current AI training methods use a huge amount of energy and are often only accessible to large tech companies. Note: Content may be edited for style and length. Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.sciencedaily.com/releases/2025/06/250622225927.htm'>Quantum dice: Scientists harness true randomness from entangled photons</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.sciencedaily.com', 'title': 'ScienceDaily'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 08:29:07
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>People often draw straws, throw dice or flip coins to make fair choices. Random numbers can enable auditors to make completely unbiased selections. Randomness is also key in security; if a password or code is an unguessable string of numbers, it's harder to crack. Many of our cryptographic systems today use random number generators to produce secure keys. Even the most careful coin flips can have bias; with enough study, their outcomes could be predicted. "True randomness is something that nothing in the universe can predict in advance," said Krister Shalm, a physicist at the National Institute of Standards and Technology (NIST). Einstein believed that nature isn't random, famously saying, "God does not play dice with the universe." Scientists have since proved that Einstein was wrong. Unlike dice or computer algorithms, quantum mechanics is inherently random. "We really wanted to take that experiment out of the lab and turn it into a useful public service." To make that happen, NIST researchers and their colleagues at the University of Colorado Boulder created the Colorado University Randomness Beacon (CURBy). CURBy produces random numbers automatically and broadcasts them daily through a website for anyone to use. The Bell test measures pairs of "entangled" photons whose properties are correlated even when separated by vast distances. Einstein called this quantum nonlocality "spooky action at a distance." This is the first random number generator service to use quantum nonlocality as a source of its numbers, and the most transparent source of random numbers to date. That's because the results are certifiable and traceable to a greater extent than ever before. "CURBy is one of the first publicly available services that operates with a provable quantum advantage. "The quality and origin of these random bits can be directly certified in a way that conventional random number generators are unable to." NIST performed one of the first complete experimental Bell tests in 2015, which firmly established that quantum mechanics is truly random. However, turning these quantum correlations into random numbers is hard work. Special processing steps and strict protocols are used to turn the outcomes of the quantum measurements on entangled photons into 512 random bits of binary code (0s and 1s). The result is a set of random bits that no one, not even Einstein, could have predicted. In some sense, this system acts as the universe's best coin flip. Hashes are used in blockchain technology to mark sets of data with a digital fingerprint, allowing each block of data to be identified and scrutinized. The protocol can expand to let other random number beacons join the hash graph, creating a network of randomness that everyone contributes to but no individual controls. Intertwining these hash chains acts as a timestamp, linking the data for the beacon together into a traceable data structure. It also provides security, allowing Twine protocol participants to immediately spot manipulation of the data. "The Twine protocol lets us weave together all these other beacons into a tapestry of trust," Palfree added. The whole process is open source and available to the public, allowing anyone to not only check their work, but even build on the beacon to create their own random number generator. CURBy can be used anywhere an independent, public source of random numbers would be useful, such as selecting jury candidates, making a random selection for an audit, or assigning resources through a public lottery. "I wanted to build something that is useful. It's this cool thing that is the cutting edge of fundamental science," Kavuri added. "NIST is a place where you have that freedom to pursue projects that are ambitious but also will give you something useful." Materials provided by National Institute of Standards and Technology (NIST). Note: Content may be edited for style and length. Stay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.scientificamerican.com/article/rubin-observatorys-first-images-just-unveiled-the-universe-as-weve-never/'>Rubin Observatory's First Images Just Unveiled the Universe as We've Never Seen It Before</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.scientificamerican.com', 'title': 'Scientific American'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-23 04:01:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Majestic First Images from Rubin Observatory Show Universe in More Detail Than Ever Before A small section of the Vera C. Rubin Observatory's total view of the Virgo Cluster shows two prominent spiral galaxies (lower right), three merging galaxies (upper right), several groups of distant galaxies, many stars in the Milky Way galaxy, and more. Editor's Note (6/23/25): This story will be updated with additional images and details shortly after 11 A.M. EDT. Welcome to a mind-blowing new era of astronomy. The long-awaited Vera C. Rubin Observatory, a cutting-edge new telescope perched atop a mountain in Chile, is releasing its first images of the universe on June 23—and its views are just as jaw-dropping as scientists hoped. (The observatory is holding a celebratory event today at 11 A.M. EDT to reveal additional images that you can watch a livestream of on YouTube. If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. Those 10 million galaxies are roughly 0.05 percent of the approximately 20 billion galaxies that Rubin will capture during its 10-year Legacy Survey of Space and Time. “In a lot of ways, it almost doesn't matter where we look,” said Aaron Roodman, a physicist at Stanford University and program lead for the Rubin Observatory's LSST Camera, in a preview press conference held on June 9. “In some sense, we could have looked anywhere and gotten fantastic images.” In the end, the team decided to share several mosaics of images from the observatory that highlight its extremely wide field of view, which can capture multiple alluring targets in a single snapshot. This image combines 678 separate images taken by the Rubin Observatory in just more than seven hours of observing time. Combining many images in this way clearly reveals otherwise faint or invisible details, such as the clouds of gas and dust that comprise the Trifid Nebula (top right) and the Lagoon Nebula, which are several thousands of light-years away from Earth. Individual detail images (at top and below) show a mix of bright Milky Way stars against a backdrop of myriad more distant galaxies. In addition, the team has released a teaser video of a stunning zoomable view of some 10 million galaxies that was created by combining some 1,100 images taken by the new observatory. These first glimpses from Rubin showcase the observatory's unprecedented discovery power. “We've been working on this for so many years now,” says Yusra AlSayyad, an astronomer at Princeton University and the Rubin Observatory's deputy associate director for data management. “I can't believe this moment has finally come.” Meghan Bartels is a science journalist based in New York City. She joined Scientific American in 2023 and is now a senior news reporter there. Her writing has also appeared in Audubon, Nautilus, Astronomy and Smithsonian, among other publications. She attended Georgetown University and earned a master's degree in journalism at New York University's Science, Health and Environmental Reporting Program.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            