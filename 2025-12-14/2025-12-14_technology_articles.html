
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - TECHNOLOGY Article Summaries - 2025-12-14</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            TECHNOLOGY
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/12/14/rivians-survival-plan-involves-more-than-cars/'>TechCrunch Mobility: Rivian's survival plan involves more than cars</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 17:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. To get this in your inbox, sign up here for free — just click TechCrunch Mobility! Senior reporter Sean O'Kane popped over to Palo Alto to check out Rivian's Autonomy & AI Day, which some insiders told us would be the company's most important event. Via Sean (and a few of my thoughts sprinkled in) after the event … It was easy to get lost in the buzz words at times during Rivian's “Autonomy & AI Day” this week. But there was a clear underlying message being shared: Rivian is trying to build a company that is about more than just selling cars. For instance, there were no humanoid robots wandering around the company's Palo Alto campus. Rivian's hands-free version of its driver-assistance software — which today can be used on about 135,000 miles of road — will expand to 3.5 million miles and include surface streets. This expanded capability, which will launch in early 2026 and eventually include point-to-point hands-free (but eyes on) automated driving comes with a cost of $2,500 or $49.99 per month. Rivian revealed it has developed its own custom 5nm processor, which it says will be built in collaboration with both Arm and TSMC. That chip will power Rivian's “autonomy computer” — the backbone of an upgraded automated-driving system —that will debut in the R2 SUV in late 2026. But there's another scenario we should also consider: licensing its tech to others. After all, Rivian already has a joint venture with Volkswagen Group to share its electrical architecture and base-level software. Barclays' Dan Levy wrote Friday that “subsequent discussions reiterated hopes/potential” for Rivian to license its whole AV platform, or just components like the customs processor. And when I asked CEO RJ Scaringe if Rivian will sell the processor to Mind Robotics, he responded wryly: “It doesn't take a lot of imagination.” Rivian is building its own AI assistant (deeper dive into the tech). And it is coming to its EVs in early 2026. Rivian goes big on autonomy with custom silicon, lidar, and a hint at robotaxis In the meantime, here's a tiny tidbit to hold you over. As you read above, senior reporter Sean O'Kane was at Rivian's AI & Autonomy Day and one of the whispers he heard was about the company's public demo of its AI assistant and concerns it might not work. The risks are high for public demos, which is why many companies avoid them. At the start of 2025, I didn't think TechCrunch would publish an aviation-startup-meets-data-center story. Its first customer will be data center startup Crusoe. Boom has raised $300 million to help commercialize this new business. The round was led by Darsana Capital Partners with participation from Altimeter Capital, Ark Invest, Bessemer Venture Partners, Robinhood Ventures, and Y Combinator. The plan is to use money from its Superpower stationary turbine business to fund the development of its supersonic aircraft. Self-driving trucks company Aurora Innovation made a commercial agreement with Detmar Logistics to autonomously transport frac sand in the Permian Basin. Some deals don't always work out, or they change. Four years ago, Ford and South Korean battery maker SK On struck a deal to form a joint venture and spend $11.4 billion to build factories in Tennessee and Kentucky that would produce batteries for the next generation of electric F-Series trucks. 700Credit, a company that runs credit checks and identity verification services for auto dealerships across the United States, said a data breach affected at least 5.6 million people who had their names, addresses, dates of birth, and Social Security numbers stolen. Apparently, that wasn't a convincing argument; NASA and USPS have stopped using them. Lucid is being sued by its former chief engineer Eric Bach, who alleges wrongful termination, discrimination, and retaliation. Bach, who is of German heritage, also claims one of the automaker's top HR executives referred to him as a “German Nazi.” Subaru unveiled its Uncharted EV and the specs might attract buyers. And nope, this is not the first baby born in a Waymo. Meanwhile on the Waymo news front, a leaked letter from Tiger Global Management to its investors disclosed that Waymo is now providing 450,000 robotaxi rides per week — nearly double the amount it disclosed this spring. As a reminder, I asked: The pace of autonomous vehicle development has quickened, prompting more scrutiny and questions around safety and accountability. With iOS 26.2, Apple lets you roll back Liquid Glass again — this time on the Lock Screen Google launched its deepest AI research agent yet — on the same day OpenAI dropped GPT-5.2 Disney hits Google with cease-and-desist claiming ‘massive' copyright infringement OpenAI fires back at Google with GPT-5.2 after ‘code red' memo Google debuts ‘Disco,' a Gemini-powered tool for making web apps from browser tabs Marco Rubio bans Calibri font at State Department for being too DEI Claude Code is coming to Slack, and that's a bigger deal than it sounds</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/pc-components/motherboards/embargo-12-14-0600-pst-msi-x870e-godlike-x-motherboard-review'>MSI X870E Godlike X Motherboard Review: 10th anniversary edition brings more exclusivity, numbered placard, and a Lucky plushy</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 14:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>When you purchase through links on our site, we may earn an affiliate commission. The Godlike X is an excellent flagship motherboard. If you're a big MSI fan, it can be worth the premium for those extras. Otherwise, get the original for hundreds less. 15x USB ports (7x Type-C) on rear IO Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test. 2025 marks the 10th anniversary of MSI's “Godlike” motherboards first hitting store shelves. From there, MSI added unique technologies, including a dynamic display, a wireless extender, magnetic RGB modules, and EZ Link, to improve the user experience. Today's Godlike and other flagship-class motherboards are the pinnacle of consumer board technology, offering users the best of what's available, cost be damned. The Godlike X we have in on the test bench celebrates the 10-year milestone with a special collector's edition for AMD (sorry, no Intel), dubbed Godlike X. It's collectable, as the board is a limited run of 1,000 units, each identified by a numbered golden nameplate on the M.2 Shield Frozr heatsink. MSI also includes a premium collectors' stand to show off the RGB heatsink (USB-C-powered) and a cute Lucky plush with branded keychain ‘charms' celebrating the anniversary. To give you a quick refresher on the impressive specifications, the Godlike X bestows upon owners a premium audio codec and DAC/HPA, fast networking including Wi-Fi 7, 10GbE, and 5GbE ports, five native (seven total) M.2 sockets, robust power delivery with 27 total phases, an incredible seven USB Type-C ports on the rear IO, an informative (and customizable) LED screen, and more. On top of that, it looks great with three tastefully implemented RGB zones, and it has the EZ Link design and EZ Bridge features for easier building and upgrades. The Godlike X's performance was just like its paternal twin and among the best across most of our test suite. All this was done with the default settings and PBO disabled. But before we share test results and discuss details, here are the specifications from MSI's website. Inside the premium box are a slew of accessories. It even comes with the cool powered collector's stand to show off the numbered Shield Frozr M.2 heatsink. There are also several extension cables, thermistors, stickers, and more. The long list is below, and we've included an image of Lucky and the collector's stand all lit up. It's covered in shrouds, heatsinks, an LCD screen, and RGB lighting areas that grace the front of the board, while a backplate covers the rear and doubles as an additional heatsink for multiple components. The EZ Bridge houses the Dynamic Dashboard III TFT LCD panel that displays system status, temperatures, voltages, BIOS Flash status, and error messages. You can even customize it using your own image file (.gif, .bmp, .png, .jpg). There's no doubt it's a good-looking motherboard, and will be cooler for some because it's a limited edition. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. MSI lists support up to DDR-9000(OC), which is plenty fast for the platform. We could run our DDR5-7200 kit, but our Klevv DDR5-8000 kit booted to Windows and did not complete the stress test. I didn't see our kit on the QVL list, so we weren't disappointed. Perhaps with some additional tweaking, it could get there. As always, stick to the QVL list for your best opportunity at plug-and-play, especially with high-speed kits. This unique item contains the Dynamic Dashboard III (3.99-inch 800x480 TFT full-color LCD) and several connections (think front panel, USB, some fans, and more) that magnetically attach to the motherboard. The Dynamic Dashboard displays system status, temperatures, voltages, BIOS Flash status, error messages, accepts custom images, and is truly the showpiece of the board. From top to bottom, you have a 4-pin ARGB header, CPU and PUMP fan headers, 24-pin ATX power, supplemental 8-pin PCIe power (required for 60W charging via USB-C), Front panel USB 3.2 Gen 2x2 (20 Gbps) and Gen 2 (10 Gbps) Type-C ports, and another 4-pin fan header. The hub offers seven more fan headers (for a total of 10), two ARGB headers, one 3-pin RGB header, and a water-flow connection. Power for the hub comes from a SATA power connector, while control over most devices is handled through the MSI Center and its applications, or, for some devices (like fans), through the BIOS as well. Power is supplied via the 8-pin EPS connector(s) to a Renesas RAA229628 controller. From there, it moves to the Renesas R2209004 110A SPS MOSFETs. The VRMs are among the most robust on the platform and will not hold you back, even if you decide to overclock with sub-ambient cooling. On the bottom left side, hidden under the heatsink, is a flagship-class audio solution based on the 7.1-channel Realtek ALC4082 codec/chip and supported by an ESS 9219Q Combo DAC/HPA, with its own audio capacitors and the audio separation line to minimize EMI. You won't find much better audio hardware on a modern motherboard. You can switch it to x4, but this disables the M2_4 slot.Onboard, two M.2 sockets (M.2_1/2) are PCIe 5.0 x4-capable and handle up to 80mm modules. If you need additional PCIe 5.0 M.2 storage, MSI includes the M.2 Xpander-Z Slider Gen 5 add-in card with two PCIe 5.0 x4 slots with an EZ Slide design for quick installation/removal. The Godlike can hold a wild seven M.2 drives — the most I recall seeing on any consumer motherboard. Under a magnetic strip, you'll find the other two SATA Ports, three switches (Dashboard, LED, BIOS Select), LN2, battery (CMOS), and OC jumpers.Before getting to the rear IO, we snapped a few pictures of the onboard ICs that control some of the board's features. The rear IO on the X870E Godlike X has a lot going on, as you'd expect. The black background with white writing allows for easy reading and properly labels each port so you know exactly what it is. Starting with USB ports, there are a total of 15, with seven of them being USB Type-C! In the middle are three buttons: one to Flash the BIOS, a second to clear the CMOS, and the third is a flexible Smart Button (Reset, Mystic Light on/off, Safe Boot, or Turbo Fan). Above those are the two Ethernet ports (10 Gbps and 5 Gbps), standard Wi-Fi 7 antenna connections, and the two-plug (mic-in and line-out) plus SPDIF audio stack. Joe Shields is a staff writer at Tom's Hardware. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York,</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/review/samsung-galaxy-xr/'>Samsung's Galaxy XR Is a More Reasonably Priced Apple Vision Pro, but It Lacks Polish</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 11:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>I have had the new M5-powered Apple Vision Pro and Samsung Galaxy XR headsets sitting on my desk for several weeks. Samsung's headset is roughly half that—still a pretty penny at $1,800. Mustering the energy to don them has been a task. You can watch movies, play immersive games, and get some work done with multiple virtual screens. None of these experiences has been so compelling that I want to wear a headset on my face for more than an hour. Still, I gave it the ol' college try, and my takeaway is that Google and Samsung have more work ahead of them to improve the Android XR experience; I've also had a strange and newfound appreciation for the quality of Apple's mixed reality headset. I purchased the Galaxy XR from Samsung's website, and the company offers a handy link to EyeBuyDirect, where you can buy prescription Kodak magnetic inserts for $100 so you can use the headset without glasses. While I appreciate how much more lightweight the Galaxy XR is compared to the Vision Pro, I constantly struggled to find a comfortable fit. You put the headset on and turn a knob at the back to tighten the band around your head, but there's often a good amount of pressure resting on your forehead, which also gets warm when the audible fans kick in; it's near impossible not to have a sweaty brow after a bout in XR. Two magnetic light shields in the box do a decent job of blocking ambient light from filtering into your virtual reality, but they're not perfect, as I still encountered some slight light bleed. The Galaxy XR routinely told me to shift the headset up or down whenever I popped it on my head, and while I occasionally could find a fit that worked for a spell, I wouldn't call it comfortable. The included forehead cushion helps a bit, but I wouldn't have minded softer padding. Surprisingly, I found Apple's Vision Pro with the new Dual Knit Band immensely more supportive. Overall, the gesture-based navigation and eye tracking are fine, though not quite as precise as Apple's system in the Vision Pro. (And yes, I did try recalibrating the eye tracking.) Where the Vision Pro can pick up on minute changes I make with my fingers, I found myself needing to be more pronounced with my gestures on the Galaxy XR. On the Galaxy XR, there were far more occasions where my virtual screens just moved around because it thought I was making a pinch gesture. Even looking at my hand and pinching—how you evoke the home button—sometimes took multiple pinches to activate. When I tried to work in Chrome and opened more than six tabs, Chrome stopped working and required a restart. My Telegram app did the same here and there, and so did Google Play Services when I tried to run Grimvalor once. Initially, I couldn't pair the Galaxy XR to my PC because this feature requires a Samsung Galaxy Book laptop, but Google recently issued an update that introduced a beta app called PC Connect for Android XR. This was excellent and a much nicer way to work, rather than trying to recreate my workspace with Android apps, but I still had the issue with the mouse cursor periodically disappearing, so I was never able to really get much work done. I did try Game Link, which requires you to download Samsung's Game Link app on the Microsoft Store on your PC, along with Steam and SteamVR. There could be various things causing the issue, so it's hard to say if this is a Galaxy XR problem or something else. (It's worth noting that there are other ways to connect the Galaxy XR to your PC via third-party apps, but I didn't try them.) What's nice about Android XR is that Samsung and Google aren't starting from scratch in terms of the app ecosystem. You can pretty much access any Android app and bring it into your virtual space, though some apps are better designed for this experience than others. (The Slack app still sucks on anything larger than a phone screen.) A key feature of Android XR is the ability to chat with Google's Gemini chatbot willy-nilly. I did this a few times when I wanted to figure out how to connect the PS5 controller to the headset, and it pulled up YouTube video recommendations so I could watch a how-to. When I looked at the chat transcript, it became clear that Gemini sometimes responded to my queries with its instructions on how to parse a command. I need to check if Telegram is installed on the device first. This happened multiple times when I invoked Gemini to ask a question. I still don't know how often I would realistically chat with Gemini, even if the experience was more polished. Don't get me started on my avatar. I customized a cartoonish Galaxy avatar to replicate my likeness; they look like Bitmoji (aka not great), and the avatar does a poor job of re-creating facial expressions. Minutes into my Zoom meeting with a coworker, he complained that it was hard to figure out what my face was trying to do and that he also couldn't hear me very well because the headset's mic kept cutting off. Thankfully, Google came to the rescue with a recent update that added “Likeness,” designed to be a more realistic representation of yourself, like Apple's Personas. It immediately looked more like me, but my eyes were weirdly wide, and anytime I made a mouth movement, it would show my full set of teeth. I video-called my wife via Google Meet, and she just said no, screenshotted my face, and posted it on her Instagram Story for all our friends to laugh at. I really like playing around with the latest and greatest tech, and I enjoy working spatially with the right hardware. But headsets like the Galaxy XR and Vision Pro are simply too bulky, heavy, and annoying to wear for more than an hour. Putting them on feels like a chore, and I don't think they offer that much greater an experience than I can have in my non-virtual world. Heck, I don't even mind the tethered battery. If these can get drastically lighter, comfier, and smaller, I could see a world where I'd pop 'em on for a few hours and enjoy a virtual reality to get away from life. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/playstation-portals-latest-update-proves-sony-needs-a-real-handheld-console-again/'>PlayStation Portal's Latest Update Proves Sony Needs a Real Handheld Console Again</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 11:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Launched November 2023, Portal was intended as a mere accessory for PlayStation 5. It had no native processing abilities, simply using Sony's Remote Play technology to stream whatever happened on players' personal PS5 to the portable's screen. Although it could technically be used anywhere with a strong Wi-Fi signal, difficulties connecting to public networks and high speed requirements to even launch a stream meant the Portal was effectively only suitable for in-home use, to free up the main TV or play in another room. Even so, another year on and the Portal's success seems unstoppable. It takes Cloud Streaming out of its beta phase, expanding the streamable library from only those titles included in the PS Plus selection to many games digitally owned by players. Until now, if you didn't have a game installed locally on your PS5 or it wasn't included in that cloud catalog, too bad, no Portal play for you. The assortment available is already vast—more than 3,000 games at time of writing. It potentially extends availability of titles you own to wherever you want to play them, and could even help alleviate data storage woes. Although the PS5's internal drives can be expanded, SSDs can be pricey at higher capacities, and players with large digital collections often can't install everything they own. Previously, these features were only enabled for Remote Play gaming on Portal, since they were effectively being done through the PS5 and mirrored on the Portal's screen. Accessibility features have also been improved, adding a screen reader tool and adjustable text sizes. I was cautiously excited at the overhaul's potential, thinking it might help me tackle my ever-growing backlog of unfinished games. Lately, I've recently been trying to make my way through Koei Tecmo's Atelier Ryza trilogy, cozy but absolutely massive Japanese RPGs. I say "trying" because these are time sinks by any measure—around 80 hours each, with the recently released trilogy pack featuring the "DX" versions that add even more content, and there are only so many hours in the day that I can spend rotting in front of my PS5. However, given all three are included in the new cloud streaming roster, being able to play them in shorter chunks on the go might be a great way to get through these gargantuan adventures. Plus, with Cloud Streaming out of beta, surely it's overcome its previous shoddy performance, right? For all the features Sony has added to Portal and the overall catalog it has access to, it is still fatally stymied by being a streaming-only piece of hardware. Sony officially states that “PlayStation Portal Remote Player requires broadband internet Wi-Fi with at least 5 Mbps for use. Trying to connect to Sony's servers and stream a game on anything but a lightning-fast connection remains an exercise in frustration, and makes out-of-home play nigh-impossible. Another speed test told me I might have been onto something, as I was now getting 37.14 Mbps—surely more than doubling the speed will help me play a game? Nope: “Can't start because your internet connection quality might not be sufficient to play streaming games.” Points for the variety of ways to say nothing works, I guess. There are numerous reasons why connection speeds in any given network environment can fluctuate, but I was surprised that the experience now proved worse than the last time I tested the Portal in a coffee shop, where an even slower 11 Mbps connection sufficed to at least launch Spider-Man: Miles Morales and Gris, albeit not in great quality. This go around, I could log in and browse both the PS Plus catalog and my own library of digitally owned games, but absolutely nothing would play. It's particularly annoying given that Sony's pitch for the new Cloud Streaming offering is that it “makes it easier to enjoy PS5 games on the go—at a hotel, café, friend's place, or anywhere else with a high-speed internet Wi-Fi connection.” In fact, at a friend's house was the only place I could get it working, thanks to an 802 Mbps connection speed—but it shouldn't take nearly 54x the recommended 15 Mbps for the Portal to perform its sole function. Despite what Sony says about minimum speed requirements, in reality streaming games on the Portal needs a thumping great Wi-Fi connection. So, if multiple attempts to morph the Portal into being a vaguely portable console have, charitably, underdelivered, there's only one path left for Sony: fully commit to a true, dedicated gaming handheld again. It's easy to understand why Sony might be cautious on this, since its last effort—the brilliant but under-appreciated PlayStation Vita—failed to match the success of its predecessor, the PlayStation Portable (an estimated 14 to 16 million units sold, versus the PSP's 82.5 million). However, the market is vastly different now, and so are player expectations. The Nintendo Switch changed everything: rather than handhelds being bespoke systems with their own games, often cut-down versions of the titles enjoyed at home, there was now a system that offered the exact same experience wherever you went. Players loved this, and seemingly everyone was copying the Switch's approach. By the time Portal launched in 2023, the Steam Deck was already riding high and had inspired a slate of rivals, all of which allowed PC gamers to take their libraries with them wherever they went. Even Xbox is trying to get in on the action, with its ROG Xbox Ally handhelds in partnership with Asus, and its "Play Anywhere" initiative. The good news is Sony may be working on something of a course correction. There's potentially very good incentive for that last approach, since a combo of the upcoming Steam Machine and Steam Deck could steal everyone's lunch. While there are no official announcements from Sony on its future hardware plans—bar some very technical nuggets shared by PS5 lead architect Mark Cerny back in October 2025—there have been some purported leaks claiming that Sony is pushing developers to adopt the PS5's new "Power Saver" setting, which reduces performance of demanding games to cut their energy usage. Beyond environmental benefits, slashing power drain for big games would also help optimize battery life, a necessity for a handheld processing games locally. With its latest Cloud Streaming update for Portal, Sony recognizes that people want to play the same games on the go that they do at home, continuing their progress and having a continuity of experience between platforms. Relying on cloud gaming as a delivery mechanism is a fool's errand—the network infrastructure just isn't there to make it viable, especially in public. No matter what features are added, PlayStation Portal is incapable of making that dream a reality—and until it has a handheld that can, Sony's portable play plans are in the clouds. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/this-star-is-being-eaten-alive-and-its-explosive-death-will-be-visible-in-broad-daylight-2000698104'>This Star Is Being Eaten Alive—and Its Explosive Death Will Be Visible in Broad Daylight</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 11:00:51
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>A case of astronomical fratricide is doomed to end in a fiery supernova bright enough to be spotted from Earth during the day. A study published this August in the journal Monthly Notices of the Royal Astronomical Society investigated a binary star system about 10,000 light-years from Earth called V Sagittae. Researchers finally solved the century-long mystery behind what makes it so freaking bright. They found that the system is strangely luminous because one of the pair, a super-dense white dwarf, is absolutely scarfing down on its larger sibling at unprecedented speed. Eventually, the two stars will collide, producing a supernova explosion of unusual brightness. “V Sagittae is no ordinary star system—it's the brightest of its kind and has baffled experts since it was first discovered in 1902. “It's a process so intense that it's going thermonuclear on the white dwarf's surface, shining like a beacon in the night sky.” This unexpected finding provides insight that could reshape our knowledge about the birth and death of stars, explained Pasi Hakala, a researcher at the University of Turku and co-lead author of the study. “The white dwarf cannot consume all the mass being transferred from its hot star twin, so it creates this bright cosmic ring,” he continued. “The speed at which this doomed stellar system is lurching wildly, likely due to the extreme brightness, is a frantic sign of its imminent, violent end.” A nova is an explosion in a binary star system, and this one would make V Sagittae visible to people on Earth without the help of any instruments. “But when the two stars finally smash into each other and explode, this would be a supernova explosion so bright it'll be visible from Earth even in the daytime,” he concluded. Astronomers have captured a supernova in its earliest stages—and the aftermath is unexpectedly olive-shaped. A team of researchers is now pumping the brakes on that idea. The unusual interaction triggered a strange new type of supernova that appeared to explode twice.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://news.ycombinator.com/item?id=46261998'>Willison on Merchant's "Copywriters reveal how AI has decimated their industry"</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://news.ycombinator.com', 'title': 'Hacker News'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 11:00:45
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The way I'd read this sentiment is that the arrangement of society is ultimately arbitrary and if we could only choose a different system we could by truly free. The core traits seem to be group dynamics, hierarchical competition, status-attainment -- all where resources are not infinite nor are opportunities for status.We've already had sufficient technological advances such that people would not need to do much labor, but functionally speaking I just don't think people can organize themselves into _any_ possible arrangement. I think the potential arrangements that could exist are limited by nature. We've already had sufficient technological advances such that people would not need to do much labor, but functionally speaking I just don't think people can organize themselves into _any_ possible arrangement. I think the potential arrangements that could exist are limited by nature. I posit that until this point in history there has never been a time where technology would allow us to grow and distribute food for free (in terms of both financial cost and labour of time). With the rise and convergence of AI, robotics, low-cost renewable energy, advances in optimal light-biomass conversion, diminishing costs on vertical farms, and self-driving vehicles, we have within our reach a way to produce food at essentially no cost.Think through what would happen to society and our economy if food was free for anyone, anywhere. Think about the meaning of work.If these ideas intrigue you, beta are readers wanted, see my profile for contact. Think through what would happen to society and our economy if food was free for anyone, anywhere. Think about the meaning of work.If these ideas intrigue you, beta are readers wanted, see my profile for contact. If these ideas intrigue you, beta are readers wanted, see my profile for contact. I promise you as an anarchist agitator that is unbelievably new just even in the last couple years and precisely what usually happens prior to actual direct action.My fellow anarchists hate the fact that Donald Trump did more for anarchist-socialist praxis than every other socialist writer in history. My fellow anarchists hate the fact that Donald Trump did more for anarchist-socialist praxis than every other socialist writer in history. https://www.bloodinthemachine.com/p/i-was-forced-to-use-ai-u...I bookmarked the series which looks exactly like what everyone in tech is saying ISN'T happening:https://www.bloodinthemachine.com/s/ai-killed-my-jobBut I'm sure somebody will blow this off as “it's only three examples and is not really representative”But if it is representative…“then it's not as bad as other automation waves”or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. I bookmarked the series which looks exactly like what everyone in tech is saying ISN'T happening:https://www.bloodinthemachine.com/s/ai-killed-my-jobBut I'm sure somebody will blow this off as “it's only three examples and is not really representative”But if it is representative…“then it's not as bad as other automation waves”or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. https://www.bloodinthemachine.com/s/ai-killed-my-jobBut I'm sure somebody will blow this off as “it's only three examples and is not really representative”But if it is representative…“then it's not as bad as other automation waves”or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. But I'm sure somebody will blow this off as “it's only three examples and is not really representative”But if it is representative…“then it's not as bad as other automation waves”or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. But if it is representative…“then it's not as bad as other automation waves”or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. or if it is as bad as other automation waves…“well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. “well there's nothing you can do about it”Anecdotally I was in an Uber yesterday on the way to a major Metropolitan airport and we passed a Waymo. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. I asked the Uber driver how they felt about Waymo and Uber collaborating and if he felt like it was a threat to his job.His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. His answer was basically “yes it is but there's nothing anybody can do about it you can't stop technology it's just part of life.”If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. If that's how people who are being replaced feel about it, while still continuing to do the things necessary to train the systems, then there will be assuredly no human future (at least not one that isn't either subsistence or fully machine integrated) because the people being replaced don't feel like they have the capacity to stand up to it. This happens for many reasons, of which AI is just one. In turn, I think this means that the way to address the problem of job loss should not be AI soecific.If it turns out that AI does not create more jobs than are lost; that will be a new thing. I think that can happen, but on a longer timeframe.When most jobs can be done by AI, we will need a societal change to deal with that. I have read pieces nearly a hundred years old saying this, there are almost certainly much earlier writings that identify this needs to be addressed.There will undoubtedly be a few individuals that will seek to accumulate wealth and power who aim to just not employ humans. I don't think that can happen on a systemic scale because it would be too unstable.Two of the things that supports wealth inequality is 1) people do not want to risk what they currently have, and 2) they are too busy surviving to do anything about it.A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. I think that can happen, but on a longer timeframe.When most jobs can be done by AI, we will need a societal change to deal with that. I have read pieces nearly a hundred years old saying this, there are almost certainly much earlier writings that identify this needs to be addressed.There will undoubtedly be a few individuals that will seek to accumulate wealth and power who aim to just not employ humans. I don't think that can happen on a systemic scale because it would be too unstable.Two of the things that supports wealth inequality is 1) people do not want to risk what they currently have, and 2) they are too busy surviving to do anything about it.A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. When most jobs can be done by AI, we will need a societal change to deal with that. I have read pieces nearly a hundred years old saying this, there are almost certainly much earlier writings that identify this needs to be addressed.There will undoubtedly be a few individuals that will seek to accumulate wealth and power who aim to just not employ humans. I don't think that can happen on a systemic scale because it would be too unstable.Two of the things that supports wealth inequality is 1) people do not want to risk what they currently have, and 2) they are too busy surviving to do anything about it.A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. There will undoubtedly be a few individuals that will seek to accumulate wealth and power who aim to just not employ humans. I don't think that can happen on a systemic scale because it would be too unstable.Two of the things that supports wealth inequality is 1) people do not want to risk what they currently have, and 2) they are too busy surviving to do anything about it.A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. Two of the things that supports wealth inequality is 1) people do not want to risk what they currently have, and 2) they are too busy surviving to do anything about it.A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. A world where people lose their jobs and have no support results in a populace with nothing to lose and time to act. Who will we be in our communities and societies?> I have read pieces nearly a hundred years old saying thisYou can read pieces 100 years old talking about famine, polio, Communist and fascist dictatorships, the subordination of women, etc. We changed the world, not by crying about inevitability but with vision, confidence, and getting to work. > I have read pieces nearly a hundred years old saying thisYou can read pieces 100 years old talking about famine, polio, Communist and fascist dictatorships, the subordination of women, etc. We changed the world, not by crying about inevitability but with vision, confidence, and getting to work. You can read pieces 100 years old talking about famine, polio, Communist and fascist dictatorships, the subordination of women, etc. We changed the world, not by crying about inevitability but with vision, confidence, and getting to work. Also, inevitability is a common argument of people doing bad things. A problem I have with Brian Merchant's reporting on this is that he put out a call for stories from people who have lost their jobs to AI and so that's what he got.What's missing is a clear indication of the size of this problems. Are there a small number of copywriters who have been affected in this way or is it endemic to the industry as a whole?I'd love to see larger scale data on this. As far as I can tell (from a quick ChatGPT search session) freelance copywriting jobs are difficult to track because there isn't a single US labor statistic that covers that category. Are there a small number of copywriters who have been affected in this way or is it endemic to the industry as a whole?I'd love to see larger scale data on this. As far as I can tell (from a quick ChatGPT search session) freelance copywriting jobs are difficult to track because there isn't a single US labor statistic that covers that category. I'd love to see larger scale data on this. As far as I can tell (from a quick ChatGPT search session) freelance copywriting jobs are difficult to track because there isn't a single US labor statistic that covers that category. Not only are you unlikely to know if you didn't find work because an AI successfully replaced you, but it's likely to attract the most bitter people in the industry looking for someone to blame.And, btw, I hate how steeply online content has obviously crashed in quality. It's very obvious that AI has destroyed most of what passed as "reporting" or even just "listicles". But there are better ways to measure this than approaching this from a labor perspective, especially as these jobs likely aren't coming back from private equity slash-and-burning the industry. And, btw, I hate how steeply online content has obviously crashed in quality. It's very obvious that AI has destroyed most of what passed as "reporting" or even just "listicles". But there are better ways to measure this than approaching this from a labor perspective, especially as these jobs likely aren't coming back from private equity slash-and-burning the industry. That's why I always look for multiple angles and sources on an issue like this (here that's the impact of AI on freelance copywriting.) But that also makes for a story that doesn't represent the reality as most people will experience it, rather the worst case. Assuming you understand what I meant: As for being naive, that's hardly true; my opinion comes from experience. This pushed salaries down, even for senior and advanced developers. In this instance, and probably most instances of art/craft, copywriters need to figure out what is creative again, because what is considered "creative" has changed.I could also see this being the journey that AI customer support took, where all staff were laid off and customers were punted to an AI agent, but then the shortcomings of AI were realized and the humans were reintroduced (albeit to a lesser degree). The sad part is that the managers deciding on using AI are the ones who rarely understand what is good public communication - that's why they were hiring someone to help them with it.With AI they get some text that seems legit but the whole process of figuring out why&how is simply skipped. With AI they get some text that seems legit but the whole process of figuring out why&how is simply skipped. Once AI can write proper compelling converting copy then I'll change my mind. Meanwhile people spend hundreds of dollars on pro LLM access and learn to use tool calling, deep research, agents and context engineering. https://news.ycombinator.com/item?id=46264119Thanks, and thanks for bringing the article to wider attention. I believe that good skillful writing, drawing, or coding, by a human who actually understands and believes in what they're doing can really elevate the merely "good" to excellent.However, when I think about the reality of most corporate output, we're not talking about "good" as a baseline level that we are trying to elevate. The human touch was previously being used in barely successful attempts to put a coat of paint over some obvious turds. They were barely succeeding anyways, the crap stunk through. May as well let AI pretend to try, and we'll keep trying until the wheels finally fall off. However, when I think about the reality of most corporate output, we're not talking about "good" as a baseline level that we are trying to elevate. The human touch was previously being used in barely successful attempts to put a coat of paint over some obvious turds. They were barely succeeding anyways, the crap stunk through. May as well let AI pretend to try, and we'll keep trying until the wheels finally fall off. The human touch was previously being used in barely successful attempts to put a coat of paint over some obvious turds. They were barely succeeding anyways, the crap stunk through. May as well let AI pretend to try, and we'll keep trying until the wheels finally fall off. CPS samples 60k households per month to represent ~150+ million workers. If you are not actively looking for work for more than a month, you are also not technically unemployed.Unemployment data is a lagging indicator for detecting recessions not early technological displacement in white-collar niches. If you are not actively looking for work for more than a month, you are also not technically unemployed.Unemployment data is a lagging indicator for detecting recessions not early technological displacement in white-collar niches. Unemployment data is a lagging indicator for detecting recessions not early technological displacement in white-collar niches. They can't even attract that much business now because the lowest end of the market completely disappeared and is doing it at home by themselves.Prepress/typesetting work went from a highly-specialized job that you spent years mastering and could raise a family with to a still moderately difficult job that paid just above minimum wage within a single generation, just due to Photoshop, Illustrator, and InDesign. Those tools don't even generate anything resembling a final product, they just make the job digital instead of physical. In the case of copywriting, AI instantly generates something that a lazy person could ship with. Those tools don't even generate anything resembling a final product, they just make the job digital instead of physical. In the case of copywriting, AI instantly generates something that a lazy person could ship with. EDIT: To put it another way, I remember a talk where you dismissed people's concerns about their data being used for training after AI was integrated into a product, citing the company's (a big AI player) denial--as if that should just be taken at face value, because they says so--a perspective that many of us view as naive or disingenuous. I think most of my regular readers understand that.I was responsible for one of the first widely read reports on the ethics of model training back in 2022 when I collaborated with Andy Baio to cover Stable Diffusion's unlicensed training data: https://waxy.org/2022/08/exploring-12-million-of-the-images-...Calling me "their most enthusiastic shill" is not justified. Have you seen what's out there on LinkedIn/Twitter etc?The reason I show up on Hacker News so often is that I'm clearly not their most enthusiastic shill. I was responsible for one of the first widely read reports on the ethics of model training back in 2022 when I collaborated with Andy Baio to cover Stable Diffusion's unlicensed training data: https://waxy.org/2022/08/exploring-12-million-of-the-images-...Calling me "their most enthusiastic shill" is not justified. Have you seen what's out there on LinkedIn/Twitter etc?The reason I show up on Hacker News so often is that I'm clearly not their most enthusiastic shill. Calling me "their most enthusiastic shill" is not justified. Have you seen what's out there on LinkedIn/Twitter etc?The reason I show up on Hacker News so often is that I'm clearly not their most enthusiastic shill. Certainly a better effort at transparency than what most writers are willing to do. I think the level of disclosure you do is fine. Certainly a better effort at transparency than what most writers are willing to do. Saying "we don't train on data submitted is our API" isn't exactly transparent, I want to know what they ARE training on.That lack of transparency is why they have a trust crisis in the first place! Saying "we don't train on data submitted is our API" isn't exactly transparent, I want to know what they ARE training on.That lack of transparency is why they have a trust crisis in the first place! That lack of transparency is why they have a trust crisis in the first place! It's time to get your bag before the AI bubble pops. (Odd to see a complaint about me being an "AI cheerleader" attached to a post about the negative impact of AI on copywriting and how I think that sucks.) The extent of your analysis is> whelp that suckswith a tone similar to what one might take when describing the impact of flatscreen TVs on the once-flourishing TV repair business, without mentioning all of the legitimate ethical (and legal) objections people have to how AI companies train their models and how the models are used.> Anything I could do to be less insufferable?Sure, go do a series on how they use residential IPs to hide their scraping, or on how they're probably violating copyright in a multitude of ways, including software FOSS licenses by disregarding attribution clauses and derivative work licensing obligations, especially for copyleft licenses like the GPL. Write about people using these systems to effectively "rewrite" GPL'd code so they can (theoretically) get around the terms completely. > whelp that suckswith a tone similar to what one might take when describing the impact of flatscreen TVs on the once-flourishing TV repair business, without mentioning all of the legitimate ethical (and legal) objections people have to how AI companies train their models and how the models are used.> Anything I could do to be less insufferable?Sure, go do a series on how they use residential IPs to hide their scraping, or on how they're probably violating copyright in a multitude of ways, including software FOSS licenses by disregarding attribution clauses and derivative work licensing obligations, especially for copyleft licenses like the GPL. Write about people using these systems to effectively "rewrite" GPL'd code so they can (theoretically) get around the terms completely. with a tone similar to what one might take when describing the impact of flatscreen TVs on the once-flourishing TV repair business, without mentioning all of the legitimate ethical (and legal) objections people have to how AI companies train their models and how the models are used.> Anything I could do to be less insufferable?Sure, go do a series on how they use residential IPs to hide their scraping, or on how they're probably violating copyright in a multitude of ways, including software FOSS licenses by disregarding attribution clauses and derivative work licensing obligations, especially for copyleft licenses like the GPL. Write about people using these systems to effectively "rewrite" GPL'd code so they can (theoretically) get around the terms completely. > Anything I could do to be less insufferable?Sure, go do a series on how they use residential IPs to hide their scraping, or on how they're probably violating copyright in a multitude of ways, including software FOSS licenses by disregarding attribution clauses and derivative work licensing obligations, especially for copyleft licenses like the GPL. Write about people using these systems to effectively "rewrite" GPL'd code so they can (theoretically) get around the terms completely. Sure, go do a series on how they use residential IPs to hide their scraping, or on how they're probably violating copyright in a multitude of ways, including software FOSS licenses by disregarding attribution clauses and derivative work licensing obligations, especially for copyleft licenses like the GPL. Write about people using these systems to effectively "rewrite" GPL'd code so they can (theoretically) get around the terms completely. I know there are a ton of fly-by-night startups abusing residential IP scraping and I hate it, but if it's Anthropic or OpenAI or Google Gemini that's a story worth telling.I have written a lot about training data - https://simonwillison.net/tags/training-data/ - including highlighting instances where models attempted to train on ethical sources.I've also pointed out when a model claims to use ethical data but still uses a scrape of the web that's full of unlicensed content, eg https://simonwillison.net/2025/Jun/7/comma/ and https://simonwillison.net/2024/Dec/5/pleias-llms/ I have written a lot about training data - https://simonwillison.net/tags/training-data/ - including highlighting instances where models attempted to train on ethical sources.I've also pointed out when a model claims to use ethical data but still uses a scrape of the web that's full of unlicensed content, eg https://simonwillison.net/2025/Jun/7/comma/ and https://simonwillison.net/2024/Dec/5/pleias-llms/ I've also pointed out when a model claims to use ethical data but still uses a scrape of the web that's full of unlicensed content, eg https://simonwillison.net/2025/Jun/7/comma/ and https://simonwillison.net/2024/Dec/5/pleias-llms/ I can see some limited scenarios in up and coming industries or strategically important industries where government job programs could be at least argued for.The copywriting industry is clearly not either of those. The copywriting industry is clearly not either of those. Look at how things went for the "Learn to code" workforce. They were told that software would be a valuable skill to have, and sunk a lot of time and money into fronted coding bootcamps. The job market in 2025 looks very different with Sonnet 4.5, which is particularly good at frontend code. What skills would you tell all those copywriters to go re-train in? How confident are you that won't be useless in 10, 15 years? Maybe you can say that they should have trained in other fields of software, but hindsight is 20/20.I am not saying automation is bad, or that the jobs we have today should be set in stone and nothing change. But, there will be society level ramifications if we take some significant fraction of the workforce and tell them they're out of a job. I am not saying automation is bad, or that the jobs we have today should be set in stone and nothing change. But, there will be society level ramifications if we take some significant fraction of the workforce and tell them they're out of a job. No, it's not, and the steep decline in quality of writing has reflected this.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/the-netflix-warner-deal-faces-increasingly-plausible-sounding-government-opposition-2000699498'>The Netflix-Warner Deal Faces Increasingly Plausible-Sounding Government Opposition</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 10:00:36
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>At this phase, conventional wisdom will tell you that the Trump Administration is probably not going to use the force of the government to shoot down Netflix's effort to acquire the core studio and streaming assets of Warner Bros. However, with South Carolina Republican Senator Tim Scott now signaling antitrust concerns in a letter to Trump's regulators, according to Semafor, it's now plausible to say there is real pushback. Semafor quotes the letter as saying there are “significant antitrust problems” with the deal, and that it harms “moviegoers, on-camera talent, writers, producers, and everyone who loves the entertainment industry.”Scott also reportedly expressed concerns about Netflix's ease around price hikes and the deal's potential negative impact on TV showrunners and movie theaters. And for what it's worth, Scott's opposition sounds pretty full-throated: “The transaction warrants rigorous antitrust review under all applicable antitrust merger and monopolization laws and, to the extent appropriate, a lawsuit to block it,” Semafor quotes the letter as saying.Shortly after the terms of the deal were announced, Utah Republican Senator Mike Lee, the head of the antitrust committee, said it “should send alarm to antitrust enforcers around the world.”Trump himself has said the deal “could be a problem.”Interestingly, Polymarket gives Paramount's hostile bid for Warner Bros. Discovery a slightly better chance of succeeding by the end of 2027 than Netflix's slightly smaller deal, which has been approved by the boards of both companies. Then again, antitrust enforcement can mean a lot of things. There could be a lawsuit from the Department of Justice or Federal Trade Commission, or some kind of foot-dragging effort by regulators asking for divestitures or making other demands. Scrutiny from legislators like Scott is just one of many potential arrows in the quiver of this deal's opponents. Subscribe and interact with our community, get up to date with our customised Newsletters and much more. The hit Netflix series returns on December 25 with three more episodes, leading into the big series finale on December 31. Domee Shi and Maggie Kang look back on the inverse reactions to 'Elio' and 'KPop Demon Hunters' and their shared desires to center original stories.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/in-a-first-ai-models-analyze-language-as-well-as-a-human-expert/'>For the First Time, AI Analyzes Language as Well as a Human Expert</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 07:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The original version of this story appeared in Quanta Magazine. Among the myriad abilities that humans possess, which ones are uniquely human? Language has been a top candidate at least since Aristotle, who wrote that humanity was “the animal that has language.” Even as large language models such as ChatGPT superficially replicate ordinary speech, researchers want to know if there are specific aspects of human language that simply have no parallels in the communication systems of other animals or artificially intelligent devices. This view was summed up by Noam Chomsky, a prominent linguist, and two coauthors in 2023, when they wrote in The New York Times that “the correct explanations of language are complicated and cannot be learned just by marinating in big data.” AI models may be adept at using language, these researchers argued, but they're not capable of analyzing language in a sophisticated way. Gašper Beguš, a linguist at the University of California, Berkeley. That view was challenged in a recent paper by Gašper Beguš, a linguist at the University of California, Berkeley; Maksymilian Dąbkowski, who recently received his doctorate in linguistics at Berkeley; and Ryan Rhodes of Rutgers University. While most of the LLMs failed to parse linguistic rules in the way that humans are able to, one had impressive abilities that greatly exceeded expectations. It was able to analyze language in much the same way a graduate student in linguistics would—diagramming sentences, resolving multiple ambiguous meanings, and making use of complicated linguistic features such as recursion. This finding, Beguš said, “challenges our understanding of what AI can do.” This new work is both timely and “very important,” said Tom McCoy, a computational linguist at Yale University who was not involved with the research. “As society becomes more dependent on this technology, it's increasingly important to understand where it can succeed and where it can fail.” Linguistic analysis, he added, is the ideal test bed for evaluating the degree to which these language models can reason like humans. One challenge of giving language models a rigorous linguistic test is making sure they don't already know the answers. To avoid this, Beguš and his colleagues created a linguistic test in four parts. Three of the four parts involved asking the model to analyze specially crafted sentences using tree diagrams, which were first introduced in Chomsky's landmark 1957 book, Syntactic Structures. So far, there's no convincing evidence that other animals can use recursion in a sophisticated way. Beguš' test fed the language models 30 original sentences that featured tricky examples of recursion. Beguš, among others, didn't anticipate that this study would come across an AI model with a higher-level “metalinguistic” capacity–“the ability not just to use a language but to think about language,” as he put it. That is one of the “attention-getting” aspects of their paper, said David Mortensen, a computational linguist at Carnegie Mellon University who was not involved with the work. There has been debate about whether language models are just predicting the next word (or linguistic token) in a sentence, which is qualitatively different from the deep understanding of language that humans have. McCoy was surprised by o1's performance in general, particularly by its ability to recognize ambiguity, which is “famously a difficult thing for computational models of language to capture,” he said. Humans “have a lot of commonsense knowledge that enables us to rule out the ambiguity. But it's difficult for computers to have that level of commonsense knowledge.” A sentence such as “Rowan fed his pet chicken” could be describing the chicken that Rowan keeps as a pet, or it could be describing the meal of chicken meat that he gave to his (presumably more traditional) animal companion. To speak fluently, like a native speaker, people follow phonological rules that they might have picked up through practice without ever having been explicitly taught. Here are some example words from one of the languages: For this language, o1 correctly wrote that “a vowel becomes a breathy vowel when it is immediately preceded by a consonant that is both voiced and an obstruent”—a sound formed by restricting airflow, like the “t” in “top.” The languages were newly invented, so there's no way that o1 could have been exposed to them during its training. The recent results show that these models can, in principle, do sophisticated linguistic analysis. But no model has yet come up with anything original, nor has it taught us something about language we didn't know before. If improvement is just a matter of increasing both computational power and the training data, then Beguš thinks that language models will eventually surpass us in language skills. But in view of recent progress, Mortensen said he doesn't see why language models won't eventually demonstrate an understanding of our language that's better than our own. “It's only a matter of time before we are able to build models that generalize better from less data in a way that is more creative.” The new results show a steady “chipping away” at properties that had been regarded as the exclusive domain of human language, Beguš said. Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/12/13/indias-spinny-lines-up-160m-funding-to-acquire-gomechanic-sources-say/'>India's Spinny lines up $160M funding to acquire GoMechanic, sources say</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-14 04:30:07
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Spinny, an Indian online marketplace for used cars, is raising around $160 million as it moves to acquire car services startup GoMechanic, TechCrunch has learned. Nearly $90 million of the round is primary, people said; Existing investor Accel has already wired about $44 million of that amount, with some details of the investment appearing in regulatory filings in India this week, which Indian outlet Entrackr first reported. Accel, Fundamentum, and Blume Ventures did not respond to requests for comments. Those funds were earmarked to scale Spinny's core used-car business. Earlier reports suggested Spinny could buy GoMechanic for around ₹4.5 billion (approximately $49.70 million) in a cash-and-stock deal. The startup had previously been backed by high-profile investors, including Sequoia Capital, Tiger Global, and SoftBank. For Spinny, acquiring GoMechanic would deepen its control across the used-car value chain. Spinny operates its own large reconditioning centers to refurbish vehicles before sale and relies on third-party service shops for after-sales servicing of customer cars — a gap GoMechanic could bring in-house. The platform would service vehicles bought or sold through Spinny, and help attract car owners who may not yet be customers. That could help expand Spinny's vehicle supply without significantly increasing customer acquisition costs. The acquisition comes as India's used-car market is projected to grow at a compound annual growth rate of about 10% to roughly 9.5 million units by 2030, from nearly 6 million units today, per a recent report by Mahindra First Choice and Volkswagen Pre-owned Certified. The GoMechanic deal would mark Spinny's latest move to broaden its footprint in India's automotive market. Jagmeet covers startups, tech policy-related updates, and all other major tech-centric developments from India for TechCrunch. You can contact or verify outreach from Jagmeet by emailing mail@journalistjagmeet.com. With iOS 26.2, Apple lets you roll back Liquid Glass again — this time on the Lock Screen Google launched its deepest AI research agent yet — on the same day OpenAI dropped GPT-5.2 OpenAI fires back at Google with GPT-5.2 after ‘code red' memo Google debuts ‘Disco,' a Gemini-powered tool for making web apps from browser tabs Marco Rubio bans Calibri font at State Department for being too DEI Claude Code is coming to Slack, and that's a bigger deal than it sounds</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            