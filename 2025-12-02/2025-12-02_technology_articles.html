
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - TECHNOLOGY Article Summaries - 2025-12-02</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            TECHNOLOGY
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://news.ycombinator.com/item?id=46124205'>100000 TPS over a billion rows: the unreasonable effectiveness of SQLite</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://news.ycombinator.com', 'title': 'Hacker News'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 18:01:07
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>I'm working on a container-based agent project and also trialling using agents heavily to write the individual features. It's working pretty well but the agents have been very zealous at documenting things lol).This is my first real project using sqlite and we've hit some similarly cool benchmarks:* 5-15ms downtime to backup a live sqlite db with a realistic amount of data for a crud db* Capable of properly queueing hundreds of read/write operations when temporarily unavailable due to a backup* e2e latency of basically 1ms for CRUD operations, including proto SerDe* WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. This is my first real project using sqlite and we've hit some similarly cool benchmarks:* 5-15ms downtime to backup a live sqlite db with a realistic amount of data for a crud db* Capable of properly queueing hundreds of read/write operations when temporarily unavailable due to a backup* e2e latency of basically 1ms for CRUD operations, including proto SerDe* WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. * 5-15ms downtime to backup a live sqlite db with a realistic amount of data for a crud db* Capable of properly queueing hundreds of read/write operations when temporarily unavailable due to a backup* e2e latency of basically 1ms for CRUD operations, including proto SerDe* WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. * Capable of properly queueing hundreds of read/write operations when temporarily unavailable due to a backup* e2e latency of basically 1ms for CRUD operations, including proto SerDe* WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. * e2e latency of basically 1ms for CRUD operations, including proto SerDe* WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. * WAL lets us do continuous, streaming, chunked backups!Previously I'd only worked with Postgres and Spanner. This article first says that normally you would run an application and the database on separate servers and then starts measuring the performance of a locally embedded database. If you have to keep the initial requirement for your software, then SQLite is completely out of equation. If you can change the requirement, then you can achieve similar performance by tuning the local PGSQL instance -- and then it also becomes a valuation of features and not just raw throughput. It'd be a very short article if so, don't you think? Full article would be something like: "Normally you'd have a remote connection to the database, and since we're supposed to test SQLite's performance, and SQLite is embedded, it doesn't compare. You can throw a super cluster at the problem and still fundamentally do around 1000 TPS. [1] Various HN posts regarding Hetzner vs AWS in terms of costs and perf. 3x EX44 running Patroni + PostgreSQL would give you 64GB of working memory, at least 512 GB NVMe of dataset (configurable with more for a one-time fee) at HA + 1 maintenance node. Practically speaking, that would have carried the first 5 - 10 years of production at the company I work at with ease, for 120 Euros hardware cost/month + a decent sysadmin.I also know quite a few companies who toss 3-4x 20k - 30k at DELL every few years to get a database cluster on-prem so that database performance ceases to be a problem (unless the application has bad queries). > This particular upper bound is untested since the developers do not have access to hardware capable of reaching this limit.> However, tests do verify that SQLite behaves correctly and sanely when a database reaches the maximum file size of the underlying filesystem (which is usually much less than the maximum theoretical database size) and when a database is unable to grow due to disk space exhaustion. > However, tests do verify that SQLite behaves correctly and sanely when a database reaches the maximum file size of the underlying filesystem (which is usually much less than the maximum theoretical database size) and when a database is unable to grow due to disk space exhaustion. Scale out is typically more elastic, at least for reads. https://yourdatafitsinram.net/Not sure using EC2/AWS/Amazon is a good example here, if you're squeezing for large single-node performance you most certainly go for dedicated servers, or at least avoid vCPUs like a plague. Not sure using EC2/AWS/Amazon is a good example here, if you're squeezing for large single-node performance you most certainly go for dedicated servers, or at least avoid vCPUs like a plague. Since then, the website has been running seamlessly for about 3 weeks now. > "PSA: SQLite WAL checksums fail silently and may lose data" https://news.ycombinator.com/item?id=44672902> sqlite-parquet-vtable, [...] And a working ECC or non-ECC RAM bus, and [...].How bad is recovery from WAL checksum / journal corruption [in SQLite] [with batching at 100k TPS]?And should WAL checksums be used for distributed replication "bolted  onto" SQLite?>> (How) Should merkle hashes be added to sqlite for consistency? How bad is recovery from WAL checksum / journal corruption [in SQLite] [with batching at 100k TPS]?And should WAL checksums be used for distributed replication "bolted  onto" SQLite?>> (How) Should merkle hashes be added to sqlite for consistency? >> (How) Should merkle hashes be added to sqlite for consistency? SQLite would probably still be faster over the network with proper Merkleization</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.geekwire.com/2025/seattle-biotech-startup-curi-bio-lands-10m-to-expands-its-rd-support-for-drug-discovery/'>Seattle biotech startup Curi Bio lands $10M to expands its R&D support for drug discovery</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.geekwire.com', 'title': 'GeekWire'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:54:58
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Its Series B round was led by Seoul-based DreamCIS, which supports biopharma R&D through extensive research services. “We are thrilled to partner with DreamCIS, who shares our conviction that drug discovery urgently needs more human-relevant data at the preclinical stage,” said Michael Cho, Curi Bio's chief strategy officer, in a statement. “The vast majority of new drugs fail in human clinical trials because preclinical animal and 2D cell models have failed to be good predictors of human outcomes.” Curi Bio's platform integrates bioengineered tissues created from induced pluripotent stem cells (iPSCs) with data collection and analysis. The Seattle area is a hub of life science and biotech companies, including numerous efforts focused on AI-assisted research. “We were incredibly impressed by the company's innovative platforms and their ability to generate functional data from 3D human tissues at scale.” Fred Hutch leaders raise $10M for new AI startup aiming to expedite drug discovery Between hype and hope: Seattle biotech leaders size up AI's real impact on drug development Protein design startup Outpace lands $144M to test treatments that target solid tumors Seattle startup Accipiter Bio emerges with $12.7M and big pharma deals for AI-designed proteins</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.geekwire.com/2025/ai-roleplay-startup-yoodli-raises-40m-reports-900-revenue-growth/'>AI roleplay startup Yoodli raises $40M, reports 900% revenue growth</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.geekwire.com', 'title': 'GeekWire'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:53:15
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The Seattle startup, which sells AI-powered software to help people practice real-world conversations such as sales calls and feedback sessions, announced a $40 million Series B round on Tuesday to fuel growth. WestBridge Capital, a $7 billion global investment firm, led the round. Yoodli's software lets users create personas to simulate conversations with another person or multiple people. Customers include SAP, Google, Snowflake, the University of Washington, Korn Ferry, and others. “At a moment when AI is replacing human jobs, we're doubling down on a different belief: that AI should help people become the best version of themselves in the conversations that matter most,” co-founder and CEO Varun Puri wrote on LinkedIn. Yoodli's revenue has grown around 900% in the past year. The idea is to replace passive formats such as slide decks and training videos with interactive practice that builds conversational muscle memory. “Experiential learning is the next step of conversation coaching — helping people learn, practice, and apply skills with roleplays at the center of their experience,” Puri wrote. The raise comes amid competition in the AI-powered workforce training market, as employers look for tools to upskill workers in communication, leadership, and customer engagement — areas where traditional learning management systems may have limitations. Neotribe and Madrona also participated in the latest funding round. Have a scoop that you'd like GeekWire to cover? AI roleplay startup Yoodli raises $13.7M to help sales teams practice their pitches Yoodli CEO Varun Puri on startup grit, filtering out noise, and building the ‘AI roleplay' category Tech Moves: Yoodli, A-Alpha Bio, MoxiWorks add execs, TiE Seattle names president</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.geekwire.com/2025/amazon-unveils-frontier-agents-new-chips-and-private-ai-factories-in-aws-reinvent-rollout/'>Amazon unveils ‘frontier agents,' new chips and private ‘AI factories' in AWS re:Invent rollout</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.geekwire.com', 'title': 'GeekWire'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:02:30
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>LAS VEGAS — Amazon is pitching a future where AI works while humans sleep, announcing a collection of what it calls “frontier agents” capable of handling complex, multi-day projects without needing a human to be constantly involved. The rollout features three specialized agents: A virtual developer for Amazon's Kiro coding platform that navigates multiple code repositories to fix bugs; a security agent that actively tests applications for vulnerabilities; and a DevOps agent that responds to system outages. Unlike standard AI chatbots that reset after each session, Amazon says the frontier agents have long-term memory and can work for hours or days to solve ambiguous problems. Amazon is starting with the agents focused on software development, but Singh made it clear that it's just the beginning of a larger long-term rollout of similar agents. To keep frontier agents from breaking critical systems, Amazon says humans remain the gatekeepers. The DevOps agent stops short of making fixes automatically, instead generating a detailed “mitigation plan” that an engineer approves. Microsoft, Google, OpenAI, Anthropic and others are all moving in a similar direction. Some of the other notable announcements at re:Invent today: AI Factories: AWS will ship racks of its servers directly to customer data centers to run as a private “AI Factory,” in its words. Custom Models: Amazon introduced Nova Forge, a tool that lets companies build their own high-end AI models from scratch by combining their private data with Amazon's own datasets. It's designed for businesses that find standard models too generic but lack the resources to build one entirely alone. Executives also previewed Trainium 4, promising to double energy efficiency again. Killing “Tech Debt”: AWS expanded its Transform service to rewrite and modernize code from basically any source, including proprietary languages. The tool uses AI agents to analyze and convert these custom legacy systems into modern languages, a process Amazon claims is up to five times faster than manual coding. Stay tuned to GeekWire for more coverage from the event this week. The chips powering your smart TV, voice assistant, tablet, and car all have something in common: MediaTek From AI experiences in your smart home, vehicle, office, and beyond — processing voice commands, visual recognition, and predictive responses are faster than ever. Click for more about underwritten and sponsored content on GeekWire. Click for more about underwritten and sponsored content on GeekWire. Amazon will test new rapid delivery concept at Seattle site, filings reveal Have a scoop that you'd like GeekWire to cover? Amazon links Nova Act, its AI agent creator, to VS Code, Cursor and Kiro</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/diseased-baby-ants-ask-their-nestmates-to-poison-them-with-acid-to-protect-the-colony-study-finds-raised_hands-1-2000694317'>Diseased Baby Ants Ask Their Nestmates to Poison Them With Acid to Protect the Colony, Study Finds</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:00:12
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>New research shows that terminally ill baby ants tell other ants to kill them, potentially protecting the rest of the colony from their infection. In a study published today in the journal Nature Communications, researchers revealed that ant pupae—what a larva grows into before becoming an adult—of the Lasius neglectus ant species actively produce a chemical signal that causes other colony members to destroy them. “Sick individuals often conceal their disease status to group members, thereby preventing social exclusion or aggression,” the researchers wrote in the paper. On the other hand, researchers have documented sick adult ants leaving their colony to avoid spreading their disease. Similarly, workers that have been exposed to fungal spores practice social distancing,” Sylvia Cremer, co-author of the study and group leader of the Cremer Group at the Institute of Science and Technology Austria (ISTA), said in an ISTA statement. “Yet, this is only possible for mobile individuals. Ant brood within the colony, like infected cells in tissue, are largely immobile and lack this option.” Once worker ants receive the signal, they take the pupae from the cocoon, punch holes into them, and inject them with formic acid—an antimicrobial poison that works like a self-produced disinfectant. While previous research had demonstrated that worker ants can recognize sick pupae and kill them to disinfect the nest, scientists didn't know if passive cues or intentional signaling by the sick pupae triggered this dynamic. Only sick ants near adult worker ants produced this signal, indicating that the cue isn't just an immune response or side effect of infection. Because workers destroy specific pupae within an entire brood (eggs, larvae, and pupae) pile, “the scent cannot simply diffuse through the nest chamber but must be directly associated with the diseased pupa,” explained Thomas Schmitt, co-author of the study and a chemical ecologist from the University of Würzburg. “Accordingly, the signal does not consist of volatile compounds but instead is made up of non-volatile compounds on the pupal body surface.” While colonies are technically communities made of many individual ants, they work as a single superorganism. Ants within a colony are like the cells in our body. Similarly, our germline cells are responsible for the production of offspring, and our somatic cells execute all the other important tasks. For ants, “what appears to be self-sacrifice at first glance is, in fact, also beneficial to the signaler: it safeguards its nestmates, with whom it shares many genes. If a terminally ill ant pretended to be healthy and died, it could become infectious and endanger the whole colony. They have stronger immune defenses than worker pupae and can restrict the infection independently. Sick pupae only emit the signal when the infection is uncontrollable, empowering fellow ants to intervene in situations of real threats while avoiding the destruction of pupae that can recover. “This precise coordination between the individual and colony level is what makes this altruistic disease signaling so effective,” Cremer concluded. Subscribe and interact with our community, get up to date with our customised Newsletters and much more. According to an old Bulgarian tradition, some ants are the perfect, all-natural yogurt-making machines. Chagas disease is likely endemic throughout the southern half of the country, researchers argue in a new paper. At least two different populations of a parasitic wasp native to Europe have been found on opposite coasts of North America. But no one knows how they got there.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/'>Amazon releases an impressive new AI chip and teases an Nvidia-friendly roadmap</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Amazon Web Services, which has been building its own AI training chips for years now, just introduced a new version known as Trainium3 that comes with some impressive specs. AWS used its annual tech conference to formally launch Trainium3 UltraServer, a system powered by the company's state-of-the art, 3 nanometer Trainium3 chip, as well as its homegrown networking tech. As you might expect, the third-generation chip and system offer big bumps in performance for AI training and inference over the second-generation chip, according to AWS. AWS says the system is more than 4x faster, with 4x more memory, not just for training, but for delivering AI apps at peak demand. While the world races to build bigger data centers powered by astronomical gigawatts of electricity, data center giant AWS is trying to make systems that drink less, not more. It is, obviously, in AWS's direct interests to do so. This means the AWS Trainium4-powered systems will be able to interoperate and extend their performance with Nvidia GPUs while still using Amazon's homegrown, lower-cost server rack technology. It's worth noting, too, that Nvidia's CUDA (Compute Unified Device Architecture) has become the de facto standard that all the major AI apps are built to support. If the company follows previous rollout timelines, we'll likely hear more about Trainium4 at next year's conference. Apple just named a new AI chief with Google and Microsoft expertise, as John Giannandrea steps down Anduril's autonomous weapons stumble in tests and combat, WSJ reports This Thanksgiving's real drama may be Michael Burry versus Nvidia The future will be explained to you in Palo Alto Here are the 49 US AI startups that have raised $100M or more in 2025</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/amazon-nova-forge-ai-models/'>Amazon Has New Frontier AI Models—and a Way for Customers to Build Their Own</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The models are nowhere near as popular as those offered by rivals like OpenAI and Google, but Amazon's plan to make them highly customizable could see them gain traction with its cloud users. The new models are being made available today to a limited number of customers. More significantly, given the importance of its cloud business, Amazon is releasing a tool called Nova Forge that will let customers create specialized frontier models by adding their own training data to unfinished versions of the Nova 2 Lite and Pro models. It is already possible to fine-tune off-the-shelf AI models like Google's Gemini and OpenAI's GPT. But Amazon's approach lets customers add data at various stages of model training, including the process of building the base model, a stage known as custom pretraining that is normally reserved for large AI labs. “Everyone is looking for a frontier model that's an expert in their domain,” Rohit Prasad, who leads Amazon's AI efforts, told WIRED ahead of today's announcements. Prasad says that Amazon developed the technologies behind Nova Forge to empower internal teams, including those developing Alexa and AI agents to build custom models. “This is essentially a new open training paradigm,” he says. One customer that has tested the approach is Reddit, which used Nova Forge to create a custom model to identify content that breaks the platform's rules. Fine-tuning a conventional model would not work, says Reddit chief technology officer Chris Slowe, because most models are designed to avoid offensive or violent content entirely, meaning they would refuse to analyze some materials. Slowe adds that Reddit's customized model could have a range of uses, and will most likely be put to work next to automate content moderation. Other companies testing Nova Forge include Booking.com, Sony, and Nimbus Therapeutics, a biotech firm. Those companies also, however, report a wide range of problems in using AI, among them a lack of expertise and resources needed to build custom models. Today most AI models are either closed, meaning they can only be accessed through API or app, or open, meaning they can be downloaded and run on one's own hardware. Nova Forge offers a new approach, albeit one that is locked into Amazon's cloud. Prasad says a frontier model built using Nova Forge should be significantly cheaper, without providing specifics. The company is, however, quietly building up a portfolio of advanced AI capabilities. It has also integrated generative AI into its shopping platform, for example through an ecommerce-focused chatbot helper called Rufus. Like other big tech companies, Amazon is investing billions to build new AI infrastructure, part of a colossal—and potentially risky—bet that demand for AI will continue to grow at an impressive clip. Amazon is competing with Google and Microsoft for cloud customers. OpenAI is also rapidly building its own infrastructure and could someday become a cloud player itself. Prasad notes that the model is especially good at agentic tasks like following complex instructions and using tools on a computer. The company says that its smaller model, Nova 2 Lite, is similar to Claude 4.5 Haiku, GPT-5 Mini, and Gemini Flash 2.5 on various benchmarks. Nova 2 Omni shows that Amazon is no slouch in AI research these days. Prasad says to his knowledge no other AI company has released a fully mutli-model of this kind. Reddit's Slowe says the customizable nature of Nova is probably its most important quality. “I do believe it has a lot of potential,” he says. “For a large set of situations, it will be substantially better than what we get off the shelf.”</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/amazon-aws-ceo-matt-garman-ai-agents/'>AWS CEO Matt Garman Wants to Reassert Amazon's Cloud Dominance in the AI Era</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 16:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>But AWS has also been building in-house foundation models, new chips, massive data centers, and agents meant to keep enterprise customers locked inside its ecosystem. The company believes these offerings will give it an edge as businesses of all shapes and sizes deploy AI in the real world. WIRED sat down with AWS CEO Matt Garman ahead of the company's annual re:Invent conference in Las Vegas to discuss his AI vision, and how he plans to extend Amazon's lead in the cloud market over its fast-rising competitors, Microsoft and Google. Through Bedrock, Amazon's platform for building AI apps, he says customers can access a variety of AI foundation models while keeping the familiar data controls, security layers, and reliability that AWS is known for. If that pitch holds up, it could help AWS dominate in the AI era. Now, people are building applications that have AI in them,” said Garman, arguing that AI is becoming a feature inside large products rather than a standalone experiment. Many of the announcements at this year's re:Invent fall along these lines. Amazon unveiled new, cost-efficient AI models in its Nova series; agents that can work autonomously on software development and cybersecurity tasks; as well as a fresh offering, Forge, that lets enterprises cheaply train AI models on their own data. The stakes are high for AWS to get this right. While Amazon's cloud unit dominated the smartphone era, smaller rivals like Google Cloud and Microsoft Azure have grown at higher rates since the arrival of ChatGPT. Microsoft and Google have surged by tightly integrating with frontier AI models—the technology underlying ChatGPT and Gemini, respectively—attracting enterprises eager to experiment with cutting-edge capabilities. This rise of AWS's rivals has raised questions about Amazon's broader AI strategy, and how the incumbent will fare in the years to come. Garman says he's been hearing these concerns for years, but less so in recent months. He argues that the tide is turning, pointing to AWS's stronger-than-expected results in the company's third quarter as evidence that his strategy is working. AI seems to be driving major organizational changes inside of Amazon. Those layoffs came just a few months after Amazon CEO Andy Jassy said Amazon would need fewer people in certain areas because of AI. Internally, Garman says that AI tools are significantly accelerating some engineering teams' work as employees shift from writing code themselves to directing a team of AI agents. He says one AWS team recently did a big rewrite of an internal codebase, expecting that it would take 30 people roughly 18 months to complete. Some Amazon employees have a less optimistic tone when it comes to the company's embrace of AI. In November, more than 1,000 anonymous Amazon employees signed a petition warning that the company's “aggressive” AI rollout could come at a cost to the environment. They are ways to make people more effective at their jobs.” Reddit said the AI model it made with Amazon developed a “social intuition,” and can now help with content moderation for millions of its online communities. This year, early stage labs like Mira Murati's Thinking Machines Lab and Ilya Sutskever's Safe Superintelligence nabbed multibillion-dollar investments, despite that they're not yet deploying widely used products. “When people talk about a bubble, I think those are the deals that are most at risk,” said Garman, referring to AI startups that have raised billion dollar seed rounds. Those are the ones where I think there's open questions.” “I see the value that so many companies are getting out of AI today, and I don't see that there's any pullback,” said Garman. And so for us, that is a good signal, and we're still in the early stages of what that value is going to be.” I'd love to hear from Model Behavior readers: What are you doing with AI today that wasn't possible 12 months ago? Respond in the comments below or email me—happy to keep things off record. This is an edition of the Model Behavior newsletter. If the US has to build data centers, here's where they should go Event: Join some of the most influential voices in tech and beyond WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/12/02/mistral-closes-in-on-big-ai-rivals-with-mistral-3-open-weight-frontier-and-small-models/'>Mistral closes in on Big AI rivals with new open-weight frontier and small models</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 15:37:07
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>French AI startup Mistral launched its new Mistral 3 family of open-weight models on Tuesday, a launch that aims to prove it can lead in making AI publicly available and serve business clients better than Big Tech rivals.The 10-model release includes a large frontier model with multimodal and multilingual capabilities and nine smaller offline-capable, fully customizable models. The launch comes as Mistral, which develops open-weight language models and Europe-focused AI chatbot Le Chat, has appeared to be playing catch up with some of Silicon Valley's closed-source frontier models. Open-weight models release their model weights publicly so anyone can download and run them. Meanwhile, closed-source models, like OpenAI's ChatGPT, keep their weights proprietary and only provide access through APIs or controlled interfaces. “In practice, the huge majority of enterprise use cases are things that can be tackled by small models, especially if you fine-tune them,” Lample continued. Initial benchmark comparisons, which place Mistral's smaller models well behind its closed-source competitors, can be misleading, Lample said. Large closed-source models may perform better out-of-the-box, but the real gains happen when you customize. “In many cases, you can actually match or even out-perform closed-source models,” he said. Mistral's large frontier model, dubbed Mistral Large 3, catches up to some of the important capabilities that larger closed-source AI models like OpenAI's GPT-4o and Google's Gemini 2 boast, while also trading blows with several open-weight competitors. Mistral positions Large 3 as suitable for document analysis, coding, content creation, AI assistants, and workflow automation. Mistral says this range gives developers and businesses the flexibility to match models to their exact performance, whether they're after raw performance, cost efficiency, or specialized capabilities. The company claims Ministral 3 scores on par or better than other open-weight leaders while being more efficient and generating fewer tokens for equivalent tasks. All variants support vision, handle 128,000-256,000 context windows, and work across languages. Lample emphasizes that Ministral 3 can run on a single GPU, making it deployable on affordable hardware — from on-premise servers to laptops, robots, and other edge devices that may have limited connectivity. “We don't want AI to be controlled by only a couple of big labs.” Some other companies are pursuing similar efficiency trade-offs: Cohere's latest enterprise model, Command A, also runs on just two GPUs, and its AI agent platform North can run on just one GPU. That sort of accessibility is driving Mistral's growing physical AI focus. Earlier this year, the company began working to integrate its smaller models into robots, drones, and vehicles. Mistral is collaborating with Singapore's Home Team Science and Technology Agency (HTX) on specialized models for robots, cybersecurity systems, and fire safety; with German defense tech startup Helsing on vision-language-action models for drones; and with automaker Stellantis on an in-car AI assistant. “Using an API from our competitors that will go down for half an hour every two weeks — if you're a big company, you cannot afford this,” Lample said. Rebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Apple just named a new AI chief with Google and Microsoft expertise, as John Giannandrea steps down Here are the 49 US AI startups that have raised $100M or more in 2025</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/tech-industry/semiconductors/u-s-government-awards-gelsinger-backed-euv-developer-xlight-with-usd150-million-in-federal-incentives-company-to-develop-new-electron-based-light-source-for-lithography-tools'>U.S government awards Gelsinger-backed EUV developer xLight with $150 million in federal incentives — company to develop new electron-based light source for lithography tools</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 14:08:14
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>To build a prototype FEL-based EUV light source, apparently. When you purchase through links on our site, we may earn an affiliate commission. xLight, a U.S.-based startup developing an EUV light source based on a particle accelerator, on Tuesday signed a Letter of Intent (LOI) with the U.S. Department of Commerce for $150 million in proposed federal incentives under the CHIPS and Science Act. xLight came out of the blue earlier this year when it hired Pat Gelsinger, former chief executive of Intel, as executive chairman. The money, if awarded, will be used to bring xLight's free-electron laser (FEL) based light source closer to reality once it is built in Albany and its viability is proven in practice. The Trump administration has been particularly sceptical about Joe Biden's CHIPS and Science Act, claiming that it is a waste of taxpayers' money. However, it looks like the U.S. government has changed its mind with xLight, which promises to develop a new FEL-based light source for EUV lithography tools that would replace traditional laser-produced plasma (LPP) sources based on CO2 lasers. If xLight succeeds in building its technology and wedding it to ASML's lithography scanners, then the U.S. will control an important part of the global supply chain for these tools, which is perhaps something that got the current American leadership. "Building an energy-efficient EUV laser with tenfold improvements over today's technology will drive the next era of Moore's Law, accelerating fab productivity, while developing a critical domestic capability," said Pat Gelsinger, Executive Chairman of the Board, xLight, and General Partner, Playground Global. xLight gets powerful FEL by first using a particle accelerator, which speeds up electrons to very high velocities with radiofrequency (RF) and magnetic fields; these fast particles are then fed into an FEL, where the electrons from the accelerator pass through undulators that create a periodic magnetic field and, as a result, generate coherent, high-intensity light beams featuring required wavelengths (13.5nm in case of EUV, though scalable to 2nm for soft X-ray). Once EUV light is generated, it is piped via a network of specialized grazing-incidence mirrors with turning stations to multiple ASML scanners, up to 20 per FEL unit. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. As there is no plasma conversion step, xLight claims that its device enables higher brightness, narrower spectral width, and femtosecond pulses for sharper patterning. However, the company has yet to prove two critical things: that its technology works in general and is viable for mass production of semiconductors. The latter will arguably be harder than the former, as xLight will have to find a company with multiple Low-NA (or better High-NA) EUV tools that is willing to conduct experiments with scanners that cost around $200 million (nearly $400 million for High-NA EUV) a unit. Furthermore, since xLight uses a DOE lab network, at least some of the elements of its technology may be classified, meaning that the company will have trouble exporting them to other countries in any form. Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, & reviews in your feeds. Anton Shilov is a contributing writer at Tom's Hardware. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/tech-industry/china-issues-first-batch-of-general-rare-earth-export-licenses-to-magnet-makers'>China issues first batch of ‘general' rare-earth export licences to magnet makers — country's stranglehold over industry continues, but tensions are easing</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 13:07:29
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Early approvals cover major suppliers after a year of tighter controls. When you purchase through links on our site, we may earn an affiliate commission. Reuters reports that JL Mag Rare Earth, Ningbo Yunsheng, and Beijing Zhong Ke San Huan High-Tech received approvals tied to individual customers on December 2. Beijing began designing the new system following the October 30 meeting between Donald Trump and Xi Jinping, which aimed to ease trade tensions after a year marked by successive layers of export restrictions. These licences run for a year and attach to specific downstream clients rather than entire product lines. Ningbo Yunsheng and Zhong Ke San Huan, meanwhile, obtained narrower permissions covering part of their client lists. The development comes after Beijing tightened its oversight over both rare-earth materials and finished magnet products in recent months. Those measures heightened scrutiny of technologies used in magnet fabrication and expanded categories requiring explicit approval. It also increased lead times as licensing evaluations grew more complex. Year-to-date export data released in October showed that Chinese exports of rare-earth magnets totaled 45,290 tons, a 5.2% decline compared to the previous year. Because the licences are assigned to specific buyers, relief will arrive unevenly both across sectors and within them. Some larger manufacturers may see faster turnaround immediately, while others remain subject to slower reviews. It's also important to remember that while licenses are being granted, the underlying export-control framework remains intact. The first approvals provide a new track through it rather than an exemption from it, and any wider easing will become visible only as additional firms are granted licenses. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, & reviews in your feeds. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/pc-components/gpus/latest-gpu-market-analysis-shows-nvidia-losing-ground-to-amd-and-intel-cracks-the-1-percent-share-milestone-for-the-first-time'>Latest GPU market analysis shows Nvidia losing ground to AMD — and Intel cracks the 1% share milestone for the first time</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 12:23:50
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>JPR says Nvidia's GPU market share decreased by 1.2% in Q3'25. When you purchase through links on our site, we may earn an affiliate commission. The latest quarterly GPU market share figures have been released by specialist analyst outfit Jon Peddie Research (JPR). However, probably more interesting are the figures showing that AMD and Intel are gaining market share – at the expense of Nvidia, of course (as it is a three-horse race). Make no mistake, Nvidia's market share still looks almost unassailable at 92% in Q3'25. It remains dominant and still makes some of the best graphics cards for gaming in 2025. But its position has slipped from 94% in the previous quarter. Nevertheless, more than nine out of 10 graphics cards sold are still Nvidia GPU-based, according to JPR's figures. More importantly, the analysts think this can be explained by a wave of “panic buying because of the pending tariff,” occurring in Q2. Naturally, U.S.-based buyers shifted their AIB buying decisions forward due to looming tariffs creating retail pricing uncertainty. While it is a Goliath of the PC CPU world, its AIB products are still minnows. At least now, it can hold its head high with a full 1% market share of the dGPU business. AMD is also shown to be on the up. This popularity manifests as a 7% market share, up by 0.8% from Q2'25. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. You will see this percentage is resolutely above 100% as PC systems usually get a GPU upgrade, or two, during their service life. Dr. Jon Peddie, president of JPR, openly worries about “an inflation-driven recession due to the socioeconomic turmoil created by the Trump administration.” Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, & reviews in your feeds. Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/your-iphone-already-has-iphone-fold-software-but-apple-wont-let-you-use-it/'>Your iPhone Already Has iPhone Fold Software, but Apple Won't Let You Use It</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 11:15:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Hackers poking around in iOS 26 recently uncovered something Apple definitely didn't intend anyone to see: every modern iPhone is running the operating system Apple's upcoming “iPhone Fold” will likely use. Which means these phones are—right now—already capable of running a full, fluid desktop experience. From a performance standpoint, that shouldn't be surprising. At Apple's September 2025 event, the company claimed the A19 Pro chip inside the iPhone Air and iPhone 17 Pro offers “MacBook Pro levels of compute.” And that iPhone chip is reportedly destined to power a cheaper MacBook in 2026. The line between Apple's hardware is being further blurred, then—but what's wild is that the software side of things has blurred completely too. For years, Apple has insisted that iOS and iPadOS are distinct, despite sharing code and habitually borrowing each other's features. But a self-proclaimed “tech geek” on Reddit who got iPad features running on an iPhone claimed they're not merely similar—they're essentially the same: “Turns out iOS has all the iPadOS code (and vice versa; you can for instance enable Dynamic Island on iPad).” The hack relies on an exploit that tricks the iPhone's operating system into thinking it's running on an iPad. That unlocks smallish tweaks such as a landscape Home Screen, an iPad-style app switcher, and more Dock items. But it also provides transformative changes such as running desktop-grade apps that aren't available for iPhone, full windowed multitasking, and optimal external display support. It's not like the “phone as PC” dream is new. Perhaps the concept is a niche nerd fantasy. In June, after 15 years, the iPad got key software features, including resizable and movable windows. Except, as WIRED demonstrated, an iPad can replace a computer for plenty of people—you just need the right accessories. But where will any momentum for this future come from? Android 16 is technically ready for another crack at desktop mode, with a new system that builds on DeX. But even now, having finally escaped beta, it's buried in developer settings. That might be down to the grim state of big-screen Android apps, or the desktop experience itself feeling, politely, “rocky.” Paradoxically, Apple appears to be further ahead despite never announcing any of this. It already has a deep ecosystem of desktop-grade iPad apps. And the iPad features running on iPhone already look polished. Apple likes each device to be its own thing, optimized for a specific form factor. However, with imagination, you can see the outlines of a new ecosystem of profitable accessories for a more capable iPhone. But Apple hasn't got where it has by selling accessories nor by making a market for others to do so. Most of its profits come from a long-running strategy to nudge people into buying more hardware that coexists. Strategically chosen—if sometimes artificial—limits and product segmentation have pride of place in Cupertino's rulebook. A convergence model could knock user experience and simplicity; but Apple would likely be more fearful of how it could negatively impact sales. Rumors suggest that Apple has solved the “screen crease” problem and will in 2026 ship a foldable with a 7.8-inch, 4:3 display that's similar to (but sharper than) the iPad mini's. A tablet-sized display that doesn't let you multitask like on an iPad would be absurd, especially on a device likely to cost two or three times more than an actual iPad mini. Doubly so if Apple puts last year's iPhone chip into a MacBook that will have a full desktop environment and support at least one external display. And for anyone fretting about being forced into a more desktop-style iPhone, Apple already solved that problem. It killed the Steve Jobs vision of the iPad that sat between two computing extremes by letting users switch modes. The iPhone could follow suit, defaulting to its original purist mode while allowing power users to tap into windowing and external device support. These hacks, then, have given us a window into the iPhone Fold operating system and other aspects of a possible Apple future. They show that iPad features on iPhone already look slick and make complete sense. And the crazy thing is they're in your iPhone's software right now. Next year, they'll almost certainly be unleashed on the most expensive iPhone Apple has ever made. The question is whether Apple will let regular iPhone users have them, too. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/the-helldivers-movie-has-a-director-and-the-choice-makes-perfect-sense-2000694277'>The ‘Helldivers' Movie Has a Director and the Choice Makes Perfect Sense</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 11:00:02
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>We knew a Helldivers movie was in the works—and that the game's developers were (jokingly, sorta) hoping to attract A-list actors to meet grisly fates as part of the story's space marines-fighting-aliens action. Well, now there's an A-list director attached who could help make those dreams come true, and his hiring lets us know just how serious Sony and PlayStation are about their Starship Troopers heir apparent: it's Justin Lin, who made five (almost six) Fast & Furious movies. Indeed, he also directed Star Trek Beyond. And if you want a director able to wrangle ensemble casts made up of big-name talent, Lin has quite a bit of experience doing that just from those two franchises. Lin's most recent film, this year's independent drama Last Days, was a departure from his Hollywood trajectory, so Helldivers signals he's moving back into the big-budget action-movie realm. The news of Lin joining Helldivers was reported first by the Hollywood Reporter, which noted he'll also produce. The script is by Gary Dauberman, a horror veteran (Annabelle, The Nun, It Chapter Two) who also penned this year's horror video game adaptation Until Dawn. According to THR, “Lin is not a gamer and leaned into that as a strength when pitching his take on the material, said insiders. There are plenty of details waiting to be drawn in, something that is compelling to the filmmaker.” There's no word yet on what order the busy filmmaker will make these projects, or when Helldiver might be expected in theaters. Check out when to expect the latest Marvel, Star Wars, and Star Trek releases, what's next for the DC Universe on film and TV, and everything you need to know about the future of Doctor Who. Subscribe and interact with our community, get up to date with our customised Newsletters and much more. Justin Lin famously left the 'Fast and Furious' sequel days into its shoot and was later replaced by Louis Leterrier. The BRZRKR movie may finally be coming, and Fast & Furious' Justin Lin will be the one to direct. You don't play Helldivers without dying one or three times, so why shouldn't the movie have some celebs perish in the name of freedom? Helldivers, Horizon: Zero Dawn, and Ghost of Tsushima are the latest in Sony's catalogue of Playstation series getting the film and TV treatment.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/anacondas-are-the-rare-prehistoric-giants-that-never-shrank-ssssstudy-reveals-2000694118'>Anacondas Are the Rare Prehistoric Giants That Never Shrank, Ssssstudy Reveals</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-12-02 00:01:15
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Many of their descendants today are significantly smaller, but anacondas (Eunectes) have proven to be unexpectedly stubborn. That aligns with the usual size of today's anacondas, meaning these tropical reptiles have remained humongous for millions of years. Specifically, the team measured 183 fossilized backbones from at least 32 anacondas. Anacondas can have over 300 vertebrae, but the size of just one allows researchers to reliably reconstruct how long the snake was. This analysis, along with fossil data from elsewhere in South America, revealed that anacondas reached their maximum size 12.4 million years ago. They continued being huge, even when many other giant species went extinct. “By measuring the fossils we found that anacondas evolved a large body size shortly after they appeared in tropical South America around 12.4 million years ago, and their size hasn't changed since.” Alfonso-Rojas and his colleagues also used an approach called the ancestral state reconstruction, which consisted of reconstructing the length of giant anacondas and related species of extant snakes with a snake family tree. This method confirmed that when anacondas first emerged during the Miocene, they would have been on average 13 to 16 feet (4 to 5 meters) long. “This is a surprising result because we expected to find the ancient anacondas were seven or eight metres long,” Alfonso-Rojas explained. “But we don't have any evidence of a larger snake from the Miocene when global temperatures were warmer.” Snakes are especially sensitive to temperature, so researchers thought snakes would have been larger back then. They had it great during the Miocene, when all of northern South America would have been similar to the current Amazonian region. Today there are significantly fewer anacondas than back then, but they still have enough habitats and food to sustain their gigantism. Good thing other species didn't stick around at their original size. Could you imagine if we still had megalodons swimming around? For decades, researchers believed they were looking at fish remains. New research shows that woolly mammoths roamed farther east than scientists previously thought. Paleontologists have long debated whether small tyrannosaurs were their own species or simply teenage T. rexes. Two duck-billed dinosaur carcasses were preserved in a thin layer of clay for 66 million years. Now, they've helped researchers recreate their living appearance.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            