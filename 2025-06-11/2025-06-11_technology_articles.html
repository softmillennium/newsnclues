
<html>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
        <title id="title">News'n'Clues - TECHNOLOGY Article Summaries - 2025-06-11</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            .menu { color: #444444; }
            .copyright { margin: 0 auto; }
            p { font-family: serif; }
            body {  background-color: #2c2c2c; font-family: Arial, sans-serif; font-size: 20px; color: #f4f4f4;  }
            a { text-decoration: none; color: #f4f4f4; }
            .section { margin-bottom: 20px; }
            .heading {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(90deg, #fc4535, #1a6198);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.2);
                line-height: 1.3; /* Prevents letters from being cut off */
                padding-bottom: 5px; /* Ensures space below the text */
                margin: 30px;
            }
            .hidden {
                display: none;
            }            
            
        </style>
    </head>
    <body>
        <div id="banner" class="w-full">
            <img src="../images/banner.jpg" alt="News Banner">
        </div>

        <div id="title" class="w-full flex items-center" style="background-color: #fc4535;">
          <a href="../index.html"><img src="../images/logo.jpg" alt="News Logo" class="h-auto"></a>
          <div class="flex-grow text-center">
              <div id="title_heading" class="text-4xl font-bold mb-4" style="color:#1a6198;">Article Summaries</div>
          </div>
        </div>
        <div id='category_heading' class="section text-center heading">
            TECHNOLOGY
        </div>
        <div id="articles">
            
                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/waymo-data-privacy-protests-los-angeles/'>How Waymo Handles Footage From Events Like the LA Immigration Protests</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 18:39:25
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>One of the most widely shared images from the city, where federal authorities have sent almost 5,000 active-duty Marines and National Guard members, is of five Waymo robotaxis that were vandalized and set on fire. The charred Waymo cars have also raised fresh questions about what kinds of technology authorities can use to surveil protesters and potentially collect evidence to make arrests. According to Waymo's website, its latest driverless cars have 29 external cameras, providing “a simultaneous 360° view around the vehicle,” as well as an unknown number of internal ones. Over the past several years, Waymo has repeatedly shared video footage with police after receiving formal legal requests. But it's not clear how often the self-driving company, which is owned by Google's parent organization Alphabet, complies with these demands. When police do obtain footage, they could also combine it with other technology—like face recognition or nonbiometric tools for finding and tracking people in videoclips—to identify possible criminal suspects. Waymo spokesperson Sandy Karp tells WIRED that the company's general policy is to challenge data requests that are overly broad or lack sound legal basis, but it doesn't disclose or comment on specific cases. A separate support page for Waymo One, the dedicated app for its robotaxi service, notes that the company “may share certain data with law enforcement as needed to comply with legal requirements, enforce agreements, and protect the safety of you and others.” Waymo's website and privacy policy do not specify how long the company retains camera footage captured inside or outside its vehicles. For years, self-driving companies like Waymo retained large amounts of information collected by their cars for training purposes and to optimize the underlying technology. The company has generally been trending more recently toward deleting data sooner, WIRED previously reported, though it's unclear whether some types may be stored longer than others. Waymo declined to answer questions from WIRED about how many cameras are inside its vehicles, exactly how long footage is retained, and whether the company has ever turned over footage to US federal law enforcement or a branch of the military. Karp did note, however, that the company's engineering team sometimes uses information from sensors, including video footage and other data, to run simulations aimed at improving its technology. She says Waymo also puts limits on both who can access data and how long it's retained. In the company's relatively short time operating in US cities, it has shown a willingness to comply with requests for footage from law enforcement. Officers working for the Mesa Police Department and the Chandler Police Department in Arizona have been requesting and using footage from Waymos for criminal investigations since 2016, or about as long as the vehicles have been in their towns, according to reporting from Phoenix's ABC 15. (The individual pleaded guilty after being charged with disorderly conduct.) As of 2023, Waymo had been issued at least nine search warrants in San Francisco and Arizona's Maricopa County, its primary markets at the time, according to reporting from Bloomberg. While San Francisco police said they couldn't identify a specific Waymo vehicle that was near the crime scene, an officer argued that there was “probable cause” that Waymo vehicles were “driving around the area” and had footage of the victim, possible suspects, and the crime scene, according to a search warrant viewed by Bloomberg. Last year, WIRED reported that Waymo had sued two individuals for allegedly vandalizing its vehicles in San Francisco and had camera footage from the cars of the alleged incidents. All vehicles with self-driving capabilities rely on a combination of lidar, radar, and video data in order to operate. Cruise, the now defunct self-driving-car venture run by General Motors, also reportedly gave camera footage to law enforcement upon request. Private owners of camera-equipped vehicles can also voluntarily turn over camera footage to law enforcement. For example, police in Berkeley, California, have received at least two sets of footage from the owner of a Tesla Cybertruck who said their car was vandalized twice this year, according to documents obtained by WIRED via public record request. Big Story: Bluesky is plotting a total takeover of the social internet The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/06/11/us-governments-vaccine-website-defaced-with-ai-generated-content/'>US government's vaccine website defaced with AI-generated content</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 18:36:03
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>A U.S. government website designed to inform the public about vaccines has been defaced and now hosts apparently AI-generated spam. The domain, which belongs to the U.S. Department of Health and Human Services (HHS), appears to have been hosting the same kind of content — mostly gay-themed and LGBTQ+ posts — since at least May 12, according to an archived version of the site. It's unclear who is responsible for it, or what is the purpose of this defacement other than pushing AI-generated slop spam. Websites hosted on official U.S. government domains have been hijacked in the past to host scam ads and hacking services. On Wednesday, 404 Media reported that the vaccines HHS website is part of a wider spam operation that includes websites owned by NPR, Nvidia, and Stanford University, all of which redirect to “a nonsense SEO spam page,” as 404 Media journalist Sam Cole called it, hosted on wowlazy.com. HHS did not respond to TechCrunch's request for comment. Every weekday and Sunday, you can get the best of TechCrunch's coverage. Every Monday, gets you up to speed on the latest advances in aerospace. Startups are the core of TechCrunch, so get our best coverage delivered weekly. By submitting your email, you agree to our Terms and Privacy Notice.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/pc-components/cpus/amd-zen-5-cpu-has-dropped-to-a-staggeringly-low-price-ryzen-5-9600x-now-just-usd189-even-includes-a-free-512gb-ssd'>AMD Zen 5 CPU has dropped to a staggeringly low price — Ryzen 5 9600X now just $189, even includes a free 512GB SSD</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 18:18:42
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>When you purchase through links on our site, we may earn an affiliate commission. The AMD Zen 5 CPU is available at a low price today, just a few dollars above its all-time low of $185. Check out Newegg's deal and pick up this CPU/SSD combo of AMD's Ryzen 5 9600X processor and a Patriot P300 512GB SSD for just $189. The list price of the 9600X is $279, so you're saving $89 off that price and getting a free SSD worth $31. AMD's Ryzen 5 9600X is built around the Zen 5 architecture and features six cores and 12 threads, with a base clock of 3.9 GHz. The 9600X also features integrated graphics, so if you're building a non-gaming rig or just want to do exceptionally light gaming, this processor does away with the need for a separate graphics card. It features six cores and 12 threads, with a base clock speed of 3.9 GHz. The free Patriot P300 512GB SSD is a PCIe Gen3 x4 M.2 SSD with sequential read and write speeds of 1,700MB/s and 1,100MB/s, respectively. This SSD drive alone retails for $31 at Newegg, and although it has a low capacity for today's uses, it's still large enough to help top up a pre-existing system's total capacity. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Stewart Bendle is a deals and coupon writer at Tom's Hardware. A firm believer in “Bang for the buck” Stewart likes to research the best prices and coupon codes for hardware and build PCs that have a great price for performance ratio. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York,</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/06/11/wikipedia-pauses-ai-generated-summaries-pilot-after-editors-protest/'>Wikipedia pauses AI-generated summaries pilot after editors protest</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 18:01:41
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Wikipedia has reportedly paused an experiment that used AI to summarize articles on its platform after editors pushed back. Wikipedia announced earlier this month it was going to run the experiment for users who have the Wikipedia browser extension installed and chose to opt in, according to 404 Media. AI-generated summaries appeared at the top of every Wikipedia article with a yellow “unverified” label. Editors almost immediately criticized the pilot, raising concerns that it could damage Wikipedia's credibility. Often, the problem with AI-generated summaries is that they contain mistakes, the result of AI “hallucinations.” News publications running similar experiments, like Bloomberg, have been forced to issue corrections and, in some cases, scale back their tests. While Wikipedia has paused its experiment, the platform has indicated it's still interested in AI-generated summaries for use cases like expanding accessibility. Every weekday and Sunday, you can get the best of TechCrunch's coverage. Every Monday, gets you up to speed on the latest advances in aerospace. Startups are the core of TechCrunch, so get our best coverage delivered weekly. By submitting your email, you agree to our Terms and Privacy Notice.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/amazon-is-clearing-out-galaxy-s25-ultra-stock-now-much-cheaper-than-on-the-official-store-2000614347'>Amazon Is Clearing Out Galaxy S25 Ultra Stock, Now Much Cheaper Than on the Official Store</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 17:40:38
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>This article is part of Gizmodo Deals, produced separately from the editorial team. We may earn a commission when you buy through links on the site. Right now, the Galaxy S25 Ultra is Amazon's top-selling phone. It came out in January 2025 and is now seen as the best Android phone – even better than the iPhone 16 Pro according to many. The Galaxy S25 Ultra is regarded by experts as the best phone available currently: Samsung has pushed the boundaries of hardware and software by integrating artificial intelligence at every level to deliver a seamless user experience. The huge 6.8-inch Dynamic AMOLED 2X display offers stunning colors and clear graphics so whether it's work or gaming, everything is a treat. Its battery life is long enough for you to use during a whole day (and maybe even more) of heavy usage without needing to worry that you'll be running out of power, which makes it ideal for business people, influencers, and others who use their phone all day long. The Audio Eraser camera captures stunning video in challenging low-light environments and eliminates background noise. So that you can relive your most cherished moments with less distraction and greater sharpness. The portrait features are optimized to sharpen skin tones while preserving natural texture so that each photo looks professional and stylish. Other than photography, the Galaxy S25 Ultra is also a genius at keeping your day simple through its wide usage of AI: With features like “Multiple Tasks With One Ask,” you can keep your day simple by instructing your phone to perform several tasks all at the same time with a single request. For example, you can have it search for a nearby vegan pet-friendly restaurant and text the results to a friend in one shot. If you're looking for a premium AI smartphone that makes life easier, the Galaxy S25 Ultra is a great pick at its lowest price ever. Get the best tech, science, and culture news in your inbox daily. We may earn a commission when you buy through links on our sites.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/pc-components/gpus/amd-says-zettascale-supercomputers-will-need-half-a-gigawatt-to-operate-enough-for-375-000-homes'>AMD says zettascale supercomputers will need half a gigawatt to operate, enough for 375,000 homes</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 17:36:25
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>When you purchase through links on our site, we may earn an affiliate commission. ComputerBase reports that AMD expects ZettaFLOP-capable supercomputers of the future to require a nuclear power plant's worth of energy to operate. The graph starts between 2010 and 2015, when supercomputers required just 3.2GF/watt to run. The graph then extends (in a straight line) all the way to 2035, when AMD predicts zetta-scale supercomputers will require 2140GF/watt of power, or half a gigawatt of power. The graph assumes a 2x efficiency improvement in AI processor development every 2.2 years. Memory bandwidth and cooling capacity are supposedly the main actors responsible for increasing power consumption to such highly predicted levels. As AI hardware increases in computational power, memory bandwidth, and datacenter cooling systems must increase to keep up. Expounding this problem further is the demand for FP128, FP64, FP16, and FP8 compute capabilities. Thus, future AI accelerators will need to be capable of performing lower precision operations. We are already seeing power consumption skyrocketing with today's latest AI accelerators. By contrast, Nvidia's A100 flagship AI GPU from 5 years ago consumed just 400W of power — less than an RTX 5090. The U.S. government is hoping to rectify this growing energy situation before it becomes a problem with nuclear power plants. Several big companies, such as Microsoft, are also investing heavily in nuclear fusion to solve their datacenter power problems. However, full-blown AI-datacenter farms are now reaching zettaFLOP (Zettascale) performance — with Oracle being the first to provide a zettascale cloud computing cluster, boasting an army of 131,072 Blackwell GPUs (equating to 2.4 zettaFLOPS of performance). Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://techcrunch.com/2025/06/11/whole-foods-tells-staff-cyberattack-at-its-primary-distributor-unfi-will-affect-product-availability/'>Whole Foods warns of shortages after cyberattack at its primary distributor UNFI</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://techcrunch.com', 'title': 'TechCrunch'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 17:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Whole Foods told its employees that the ongoing outages and disruptions at its primary distributor, United Natural Foods (UNFI), may take “several days to resolve.” The Amazon-owned retail giant told staff in an internal communication, seen by TechCrunch, that UNFI was experiencing a “nationwide technology system outage,” which UNFI has for its part described as a cybersecurity incident. When reached by TechCrunch, Whole Foods spokesperson Nathan Cimbala said: “We are working to restock our shelves as quickly as possible and apologize for any inconvenience this may have caused for customers.” Whole Foods did not say how it reached its claim that the situation may resolve in a few days. “As of today, we're gradually bringing our ordering and receiving capabilities back online, with the goal of further increasing our capacity over the coming days.” Since our reporting on Tuesday, TechCrunch has received several reports of empty shelves at some Whole Foods stores and other grocery stores reliant on UNFI. A Whole Foods store visited by this reporter on Tuesday displayed notices in several aisles saying that the store was experiencing an unspecified “temporary out of stock issue” for some products. Much of the downstream real-world impact on grocery stores and their customers may not be seen until later this week. You can securely contact this reporter via encrypted message at zackwhittaker.1337 on Signal. This story was first published on June 10 and has been updated with new information about UNFI's recovery from a spokesperson. TikTok's biggest star was detained by ICE, with help from another influencer</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://news.ycombinator.com/item?id=44219680'>Researchers discover evidence in the mystery of America's 'Lost Colony'</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://news.ycombinator.com', 'title': 'Hacker News'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 16:39:24
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>The issue was that many of the colonists were second sons of relatively wealthy families, and weren't all that familiar with fishing or farming. Poorer families, at the very early stages, weren't sending their sons on these ventures because they needed the labor at home.https://historicjamestowne.org/wp-content/uploads/Subsistenc... John Smith, one of Jamestown's leaders, was not from a wealthy or privileged background. The soil around Jamestown was marshy and brackish, unsuitable for traditional English farming methods. Yes there were lots of fish but they only ran seasonally (sturgeon etc). The soil around Jamestown was marshy and brackish, unsuitable for traditional English farming methods. Yes there were lots of fish but they only ran seasonally (sturgeon etc). Old world politics at the time explain most of this. Some of the english colonies were, ugh, rushed and less well funded than they would have been under ideal situations.This is basically the same reason they didn't look too hard to see what happened to the Roanoke colony. I mean sure, colonists from hundreds of years ago are different than young adults of today.... but the tenuous nature, in general, of people out exploring the world for a new home is unsurprising.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/a-deep-learning-alternative-can-help-ai-agents-gameplay-the-real-world/'>A Deep Learning Alternative Can Help AI Agents Gameplay the Real World</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 16:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Axiom, developed by a software company called Verse AI, is equipped with prior knowledge about the way objects physically interact with each other in the game world. The approach draws inspiration from the free energy principle, a theory that seeks to explain intelligence using principles drawn from math, physics, and information theory as well as biology. The free energy principle was developed by Karl Friston, a renowned neuroscientist who is chief scientist at “cognitive computing” company Verses. Friston told me over video from his home in London that the approach may be especially important for building AI agents. “They have to support the kind of cognition that we see in real brains,” he said. The approach can produce superhuman game-playing algorithms but it requires a great deal of experimentation to work. “The work strikes me as very original, which is great,” he says. Modern AI relies on artificial neural networks that are roughly inspired by the wiring of the brain but work in a fundamentally different way. Over the past decade and a bit, deep learning, an approach that uses neural networks, has enabled computers to do all sorts of impressive things including transcribe speech, recognize faces, and generate images. Most recently, of course, deep learning has led to the large language models that power garrulous and increasingly capable chatbots. Axiom, in theory, promises a more efficient approach to building AI from scratch. It might be especially effective for creating agents that need to learn efficiently from experience, says Gabe René, the CEO of Verses. “It is a new architecture for AI agents that can learn in real time and is more accurate, more efficient, and much smaller,” René says. Hinton was a colleague of Friston's at University College London for years. For more on Friston and the free energy principle, I highly recommend this 2018 WIRED feature article. There's a very simple pattern to Elon Musk's broken promises Big Story: The epic rise and fall of a dark-web psychedelics kingpin WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/okay-apple-vision-pros-spatial-personas-dont-look-like-trash-anymore-2000614401'>Okay, Apple Vision Pro's ‘Spatial Personas' Don't Look Like Trash Anymore</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 15:30:51
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>At WWDC 2025, Apple announced visionOS 26 with a slew of new features, including “spatial Personas,” widgets that you can anchor into your physical space like your walls, and a new spatial web browsing feature that converts 2D photos on websites into 3D ones. As genuinely cutting-edge as these features are (I kept saying “wow,” “whoa,” and “holy sh*t” my entire demo time), it still costs you three-and-a-half grand to experience them. When Apple Vision Pro launched last year and then added spatial Personas—a 3D-generated avatar of yourself that can be used during FaceTime or in a virtual meeting—everybody laughed at them. Literally pointed and (in Nelson Muntz's voice) Ha-Ha-ed at how comically bad they could look. Many people's Personas had holes in the back of people's heads, hair without volume, and skin tones that looked just a bit too bright and clinical. And despite resembling the uncanny valley, there was something just a bit off about them. I'm happy to report that spatial Personas are getting a, um, major facelift (pun intended). Here's what my new spatial Persona looks like in visionOS 26: If you're paying $3,500 for Vision Pro, your virtual self damn well better look more realistic than a console from almost 20 years ago! Apple's not saying what it did specifically to improve the quality of Personas (perhaps using more polygons? The rest of my Vision Pro demo was comparably mind-blowing (as Vision Pro still tends to be), but it also further drove home the fact that you need to have deep pockets in order to enjoy visionOS 26. I saw various widgets—clocks, calendars, a Lady Gaga poster that I could pinch with my fingers to play music, and even a framed window with a #shotoniPhone panorama of Japan's Mount Fuji that I could look “into.” The higher resolution of Vision Pro's displays and the ability for the widgets to stay anchored or pinned on a wall without jittering or moving made them look very convincing as real objects. I also tried out the new spatial browsing in Safari. As I scrolled down a page, 2D photos would convert into 3D ones with depth—Apple calls these “spatial scenes.” And man, do they look good. They look like they're shot with expensive 3D cameras, and unlike most 3D content that has a certain sweet spot for feeling the depth, you can actually view them from different angles. Apple didn't have a demo for using PlayStation VR2 Sense controllers to play games in Vision Pro, but I'm sure I'll get to try that out at some point. I left my visionOS 26 demo impressed at the progress Apple's making. Spatial computing is starting to take a more solid shape. I just wish Apple would drop the price on Vision Pro or hurry up and release a cheaper version so more people could try out this cutting-edge tech. They just look at the price and stop listening, which is a real shame. Get the best tech, science, and culture news in your inbox daily. Apple execs didn't have a super satisfying answer about what went wrong with AI Siri, but they also don't really need one. Blood was spilled, and Intel Macs (among others) paid the price. Follow along with the Gizmodo crew as we unpack everything Apple announces at its annual developer conference in Cupertino, Calif. Apple's Liquid Glass look is clearly a major visual vibe shift for iOS. Visual Intelligence brings Apple's AI features a little bit closer to competitors with the ability to see what you're looking at on your phone. Gaming on iPhone and Mac will now go through a new 'Apple Games' app. Now it just needs games and features players actually want. We may earn a commission when you buy through links on our sites.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.wired.com/story/doge-social-security-administration-benefits/'>Senators Warn DOGE's Social Security Administration Work Could Break Benefits</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.wired.com', 'title': 'WIRED'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 14:32:32
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>In a new letter addressed to SSA commissioner Frank Bisignano, senators Elizabeth Warren and Ron Wyden say that DOGE's plans to “hastily upgrade” SSA IT systems could disrupt the delivery of benefits or result in mass data losses. The move, originally spearheaded by Steve Davis, one of Elon Musk's key lieutenants and a leader at DOGE, could result in total system collapse, experts told WIRED at the time. “Put simply, DOGE has already limited access to benefits by damaging SSA's technological infrastructure—and this rushed IT modernization plan can only exacerbate those problems,” the senators wrote in their letter. Since DOGE infiltrated SSA earlier this year, the agency's website has crashed numerous times, making it difficult for beneficiaries to access their accounts. SSA officials have previously proposed plans to cut its workforce by 12 percent, or around 7,000 jobs. In an interview with the Wall Street Journal earlier this week, Bisignano described plans to revamp SSA into a “digital first” agency, relying heavily on artificial intelligence. “DOGE staffers hacking away Social Security's backend tech with no safeguards is a recipe for disaster, not a serious update of the agency's digital infrastructure,” Warren tells WIRED. “This rush job risks people's private data, creates security gaps, and could result in catastrophic cuts to all benefits. But these modernization plans are often expected to take years, not a few months like DOGE has planned. They also ask for any information related to potential hacks or data leaks that may have occurred as a result of this modernization work. “This is an environment that is held together with bail wire and duct tape,” a former senior SSA technologist working in the office of the chief information officer told WIRED in March. “The leaders need to understand that they're dealing with a house of cards or Jenga. There's a very simple pattern to Elon Musk's broken promises Big Story: The epic rise and fall of a dark-web psychedelics kingpin</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://gizmodo.com/forget-anker-this-4-9-star-rated-dell-6-in-1-docking-station-is-practically-free-only-at-best-buy-2000613947'>Forget Anker, This 4.9-Star Rated Dell 6-in-1 Docking Station Is Practically Free Only at Best Buy</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://gizmodo.com', 'title': 'Gizmodo'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 14:10:42
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>This article is part of Gizmodo Deals, produced separately from the editorial team. We may earn a commission when you buy through links on the site. Pretty much anyone can recognize the value of a good docking station—that goes double for any friends or family member who work from home. Many of us are primarily laptop users. It's comfortable to work from the couch, bring it to a coffee shop, or always have it with you when traveling for either work or leisure. However, a docking station allows you to open up possibilities with a larger work space when needed. And right now, this one from Dell is on sale for $40 off. That brings it down from $100 to just $60. Sure, it feels fine to work just with the laptop alone but we deserve better than fine: sometimes it's better to have a fully robust workstation. This is great for viewing multiple tabs at once or for gaming or video editing. I have a mechanical keyboard with raised, clicky-clacky keys which I find much more satisfying to type on than the flat keypad of my laptop. My laptop simply does not support all of those at once. This Dell DA305 docking station can charge devices and features the following ports. It's also got a patented retractable USB-C cable that you can handily pull out and plug in when it's time to dock your laptop. Charging over the retractable USB-C cable allows for up to 90W power pass for your laptop so you don't even need to plug it in with its own adapter. Additionally, the DisplayPort and HDMI port support up to 4K resolution at 60Hz. Best Buy has reduced the price of this Dell docking station to just $60. Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. We may earn a commission when you buy through links on our sites.</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.tomshardware.com/laptops/gaming-laptops/maingears-new-ultima-18-laptop-pairs-a-4k-screen-with-top-end-rtx-graphics-in-a-clevo-designed-chassis'>Maingear's new Ultima 18 laptop pairs a 4K screen with top-end RTX graphics in a Clevo-designed chassis</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.tomshardware.com', 'title': "Tom's Hardware"}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 12:30:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>This gaming laptop runs up an RTX 5090 and has room for four PCIe SSDs. When you purchase through links on our site, we may earn an affiliate commission. Maingear, a New Jersey-based company known primarily for its boutique gaming desktops, is today launching a new 18-inch gaming laptop that's packed to the gills with performance components.The Ultima 18, which starts at $3,599 on Maingear's website, utilizes an Intel Core Ultra 9 275HX processor. There are two graphics options: Nvidia's GeForce RTX 5080 or 5090 Laptop GPU. Maingear says that the laptop was built with Clevo, which sells barebones systems to resellers that pick components. This type of high-resolution screen is increasingly a rarity, even on the most powerful gaming laptops. While the Titan has a 120 Hz Mini-LED panel, Maingear has opted for a faster 200 Hz display without the fancier technology.Maingear's system also supports up to 192GB of RAM and has room for four M.2 SSDs. The $3,599 starting configuration includes an RTX 5080, 32GB of Team Elite DDR5-4800 RAM, a 2TB T-Force A440 SSD, and Windows 11 Pro. The system features a metal lid and palm rest. And this one is heavy, at 8.8 pounds, though Maingear says that the 330 watt power adapter is "backpack friendly" (and, to be fair, the point of getting a laptop — even one that's 8.8 pounds — is the option to move it from place to place). The company states that there's no "OS-choking bloatware," which is a definite plus; Maingear's Control Center application lets you adjust RGB, fan settings, performance modes, and more. The other big software feature is Nvidia Advanced Optimus, which switches between the integrated and discrete GPU depending on power needs.Maingear is entering a crowded market of expensive, top-of-the-line gaming laptops that includes the likes of Alienware, Gigabyte, Razer, MSI, Asus, and more, putting the RTX 5080 and RTX 5090 in premium designs with plenty of ports, fast screens, and high price tags. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Andrew E. Freedman is a senior editor at Tom's Hardware focusing on laptops, desktops and gaming. He also keeps up with the latest news. A lover of all things gaming and tech, his previous work has shown up in Tom's Guide, Laptop Mag, Kotaku, PCMag and Complex, among others. Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York,</p>
                <br/>
                

                <div id='article_title' class='text-3xl text-gray-300 leading-tight'>
                    <a target='_blank' href='https://www.technologyreview.com/2025/06/11/1118233/amsterdam-fair-welfare-ai-discriminatory-algorithms-failure/'>Inside Amsterdam's high-stakes experiment to create fair welfare AI</a>
                </div>
                <div id='article_attribution'  style='mb-1'>
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Source:</span> {'href': 'https://www.technologyreview.com', 'title': 'MIT Technology Review'}&nbsp;&nbsp;&nbsp;
                    <span class='text-1.5xl font-extrabold text-gray-500 leading-tight'>Published:</span> 2025-06-11 09:00:00
                    <i class="fas fa-chevron-down expand-icon text-gray-500" aria-hidden="true"></i>
                </div>
                <p class='hidden' class='text-gray-500'>Hans de Zwart, a gym teacher turned digital rights advocate, says that when he saw Amsterdam's plan to have an algorithm evaluate every welfare applicant in the city for potential fraud, he nearly fell out of his chair. It was February 2023, and de Zwart, who had served as the executive director of Bits of Freedom, the Netherlands' leading digital rights NGO, had been working as an informal advisor to Amsterdam's city government for nearly two years, reviewing and providing feedback on the AI systems it was developing. According to the city's documentation, this specific AI model—referred to as “Smart Check”—would consider submissions from potential welfare recipients and determine who might have submitted an incorrect application. More than any other project that had come across his desk, this one stood out immediately, he told us—and not in a good way. “There's some very fundamental [and] unfixable problems,” he says, in using this algorithm “on real people.” De Koning, who managed Smart Check's pilot phase, was excited about what he saw as the project's potential to improve efficiency and remove bias from Amsterdam's social benefits system. A team of fraud investigators and data scientists had spent years working on Smart Check, and de Koning believed that promising early results had vindicated their approach. These opposing viewpoints epitomize a global debate about whether algorithms can ever be fair when tasked with making decisions that shape people's lives. Over the past several years of efforts to use artificial intelligence in this way, examples of collateral damage have mounted: nonwhite job applicants weeded out of job application pools in the US, families being wrongly flagged for child abuse investigations in Japan, and low-income residents being denied food subsidies in India. Proponents of these assessment systems argue that they can create more efficient public services by doing more with less and, in the case of welfare systems specifically, reclaim money that is allegedly being lost from the public purse. In practice, many were poorly designed from the start. They sometimes factor in personal characteristics in a way that leads to discrimination, and sometimes they have been deployed without testing for bias or effectiveness. The result has been more than a decade of scandals. Developing and deploying ethical AI is a top priority for the European Union, and the same was true for the US under former president Joe Biden, who released a blueprint for an AI Bill of Rights. That plan was rescinded by the Trump administration, which has removed considerations of equity and fairness, including in technology, at the national level. Nevertheless, systems influenced by these principles are still being tested by leaders in countries, states, provinces, and cities—in and out of the US—that have immense power to make decisions like whom to hire, when to investigate cases of potential child abuse, and which residents should receive services first. Amsterdam indeed thought it was on the right track. City officials in the welfare department believed they could build technology that would prevent fraud while protecting citizens' rights. They followed these emerging best practices and invested a vast amount of time and money in a project that eventually processed live welfare applications. In response to a public records request, the city disclosed multiple versions of the Smart Check algorithm and data on how it evaluated real-world welfare applicants, offering us unique insight into whether, under the best possible conditions, algorithmic systems can deliver on their ambitious promises. The answer to that question is far from simple. For de Koning, Smart Check represented technological progress toward a fairer and more transparent welfare system. For de Zwart, it represented a substantial risk to welfare recipients' rights that no amount of technical tweaking could fix. As this algorithmic experiment unfolded over several years, it called into question the project's central premise: that responsible AI can be more than a thought experiment or corporate selling point—and actually make algorithmic systems fair in the real world. Understanding how Amsterdam found itself conducting a high-stakes endeavor with AI-driven fraud prevention requires going back four decades, to a national scandal around welfare investigations gone too far. In 1984, Albine Grumböck, a divorced single mother of three, had been receiving welfare for several years when she learned that one of her neighbors, an employee at the social service's local office, had been secretly surveilling her life. He documented visits from a male friend, who in theory could have been contributing unreported income to the family. This has helped create an atmosphere of suspicion that leads to problems for both sides, says Marc van Hoof, a lawyer who has helped Dutch welfare recipients navigate the system for decades: “The government doesn't trust its people, and the people don't trust the government.” Making the system work better for beneficiaries, he adds, was a large motivating factor when the city began designing Smart Check in 2019. But he also knew that the Netherlands had become something of a ground zero for problematic welfare AI deployments. The Dutch government's attempts to modernize fraud detection through AI had backfired on a few notorious occasions. In 2019, it was revealed that the national government had been using an algorithm to create risk profiles that it hoped would help spot fraud in the child care benefits system. In Rotterdam, a 2023 investigation by Lighthouse Reports into a system for detecting welfare fraud found it to be biased against women, parents, non-native Dutch speakers, and other vulnerable groups, eventually forcing the city to suspend use of the system. Other cities, like Amsterdam and Leiden, used a system called the Fraud Scorecard, which was first deployed more than 20 years ago and included education, neighborhood, parenthood, and gender as crude risk factors to assess welfare applicants; that program was also discontinued. In the United States, there have been at least 11 cases in which state governments used algorithms to help disperse public benefits, according to the nonprofit Benefits Tech Advocacy Hub, often with troubling results. Michigan, for instance, falsely accused 40,000 people of committing unemployment fraud. And in France, campaigners are taking the national welfare authority to court over an algorithm they claim discriminates against low-income applicants and people with disabilities. This string of scandals, as well as a growing awareness of how racial discrimination can be embedded in algorithmic systems, helped fuel the growing emphasis on responsible AI. It's become “this umbrella term to say that we need to think about not just ethics, but also fairness,” says Jiahao Chen, an ethical-AI consultant who has provided auditing services to both private and local government entities. The approach, based on a set of tools intended to rein in the harms caused by the proliferating technology, has given rise to a rapidly growing field built upon a familiar formula: white papers and frameworks from think tanks and international bodies, and a lucrative consulting industry made up of traditional power players like the Big 5 consultancies, as well as a host of startups and nonprofits. In 2019, for instance, the Organisation for Economic Co-operation and Development, a global economic policy body, published its Principles on Artificial Intelligence as a guide for the development of “trustworthy AI.” Those principles include building explainable systems, consulting public stakeholders, and conducting audits. But the legacy left by decades of algorithmic misconduct has proved hard to shake off, and there is little agreement on where to draw the line between what is fair and what is not. While the Netherlands works to institute reforms shaped by responsible AI at the national level, Algorithm Audit, a Dutch NGO that has provided ethical-AI auditing services to government ministries, has concluded that the technology should be used to profile welfare recipients only under strictly defined conditions, and only if systems avoid taking into account protected characteristics like gender. Meanwhile, Amnesty International, digital rights advocates like de Zwart, and some welfare recipients themselves argue that when it comes to making decisions about people's lives, as in the case of social services, the public sector should not be using AI at all. And this time around, the city wanted to build a system that would “show the people in Amsterdam we do good and we do fair.” If an application looks suspicious, it can be sent to the city's investigations department—which could lead to a rejection, a request to correct paperwork errors, or a recommendation that the candidate receive less money. Investigations can also happen later, once benefits have been dispersed; the outcome may force recipients to pay back funds, and even push some into debt. Officials have broad authority over both applicants and existing welfare recipients. As investigations are carried out—or paperwork errors fixed—much-needed payments may be delayed. In those cases, this can mean that the city has “wrongly harassed people,” Bodaar says. If all went well, the city wrote in its internal documentation, the system would improve on the performance of its human caseworkers, flagging fewer welfare applicants for investigation while identifying a greater proportion of cases with errors. In one document, the city projected that the model would prevent up to 125 individual Amsterdammers from facing debt collection and save €2.4 million annually. Smart Check was an exciting prospect for city officials like de Koning, who would manage the project when it was deployed. He was optimistic, since the city was taking a scientific approach, he says; it would “see if it was going to work” instead of taking the attitude that “this must work, and no matter what, we will continue this.” It was the kind of bold idea that attracted optimistic techies like Loek Berkers, a data scientist who worked on Smart Check in only his second job out of college. Speaking in a cafe tucked behind Amsterdam's city hall, Berkers remembers being impressed at his first contact with the system: “Especially for a project within the municipality,” he says, it “was very much a sort of innovative project that was trying something new.” Smart Check made use of an algorithm called an “explainable boosting machine,” which allows people to more easily understand how AI models produce their predictions. Most other machine-learning models are often regarded as “black boxes” running abstract mathematical processes that are hard to understand for both the employees tasked with using them and the people affected by the results. The Smart Check model would consider 15 characteristics—including whether applicants had previously applied for or received benefits, the sum of their assets, and the number of addresses they had on file—to assign a risk score to each person. It also tried to avoid “proxy” factors—like postal codes—that may not look sensitive on the surface but can become so if, for example, a postal code is statistically associated with a particular ethnic group. With this data, we were able to build a hypothetical welfare recipient to get insight into how an individual applicant would be evaluated by Smart Check. This model was trained on a data set encompassing 3,400 previous investigations of welfare recipients. The problem of using historical data to build the models, he says, is that “we will end up [with] historic biases.” For example, if caseworkers historically made higher rates of mistakes with a specific ethnic group, the model could wrongly learn to predict that this ethnic group commits fraud at higher rates. But how bias should be defined, and hence what it actually means for an algorithm to be fair, is a matter of fierce debate. In other words, they hoped this approach would ensure that welfare applicants of different backgrounds would carry the same burden of being incorrectly investigated at similar rates. It also consulted private organizations, including the consulting firm Deloitte. Anke van der Vliet, now in her 70s, is one longtime member of the council. Their feedback did lead to key changes—including reducing the number of variables the city had initially considered to calculate an applicant's score and excluding variables that could introduce bias, such as age, from the system. “I think it was never going to work that the whole Participation Council was going to stand behind the Smart Check idea,” he says. “There was too much emotion in that group about the whole process of the social benefit system.” He adds, “They were very scared there was going to be another scandal.” The technology could not only make damaging errors but leave them even more difficult to correct—allowing welfare officers to “hide themselves behind digital walls,” says Henk Kroon, an advocate who assists welfare beneficiaries at the Amsterdam Welfare Association, a union established in the 1970s. Such a system could make work “easy for [officials],” he says. The first results were not what they'd hoped for. As the city told us and as our analysis confirmed, the initial model was more likely to wrongly flag non-Dutch applicants. And it was nearly twice as likely to wrongly flag an applicant with a non-Western nationality than one with a Western nationality. The model was also 14% more likely to wrongly flag men for investigation. In essence, they ran a bias test on their own analog system—an important way to benchmark that is rarely done before deploying such systems. What they found in the process led by caseworkers was a strikingly different pattern. Whereas the Smart Check model was more likely to wrongly flag non-Dutch nationals and men, human caseworkers were more likely to wrongly flag Dutch nationals and women. So they turned to a technique from academic research, known as training-data reweighting. In practice, that meant applicants with a non-Western nationality who were deemed to have made meaningful errors in their applications were given less weight in the data, while those with a Western nationality were given more. Eventually, this appeared to solve their problem: As Lighthouse's analysis confirms, once the model was reweighted, Dutch and non-Dutch nationals were equally likely to be wrongly flagged. The model also appeared to be better than caseworkers at identifying applications worthy of extra scrutiny, with internal testing showing a 20% improvement in accuracy. It submitted Smart Check to the Algorithm Register, a government-run transparency initiative meant to keep citizens informed about machine-learning algorithms either in development or already in use by the government. But for de Zwart, those same processes represented a profound misunderstanding: that fairness could be engineered. It might reduce bias against people with a migration background overall, but it wouldn't guarantee fairness across intersecting identities; the model could still discriminate against women with a migration background, for instance. And even if that issue were addressed, he argued, the model might still treat migrant women in certain postal codes unfairly, and so on. “They have a bias test, a human rights assessment; [they have] taken into account automation bias—in short, everything that the responsible-AI world recommends. Nevertheless, the municipality has continued with something that is fundamentally a bad idea.” Ultimately, he told us, it's a question of whether it's legitimate to use data on past behavior to judge “future behavior of your citizens that fundamentally you cannot predict.” Members of Amsterdam's city council were given little warning. In fact, they were only informed the same month—to the disappointment of Elisabeth IJmker, a first-term council member from the Green Party, who balanced her role in municipal government with research on religion and values at Amsterdam's Vrije University. “Reading the words ‘algorithm' and ‘fraud prevention' in one sentence, I think that's worth a discussion,” she told us. Smart Check's performance would be monitored on two key criteria. In other words, could the complex math that made up the algorithm actually detect welfare fraud better and more fairly than human caseworkers? It didn't take long to become clear that the model fell short on both fronts. And it proved no better than a human caseworker at identifying those that actually warranted extra scrutiny. But this time, instead of wrongly flagging non-Dutch people and men as in the initial tests, the model was now more likely to wrongly flag applicants with Dutch nationality and women. Lighthouse's own analysis also revealed other forms of bias unmentioned in the city's documentation, including a greater likelihood that welfare applicants with children would be wrongly flagged for investigation. (Amsterdam officials did not respond to a request for comment about this finding, nor other follow up questions about general critiques of the city's welfare system.) He, and others working on the project, did not believe this was necessarily a reason to scrap Smart Check. They wanted more time—say, “a period of 12 months,” according to de Koning—to continue testing and refining the model. They knew, however, that would be a hard sell. In another council meeting a few months later, he explained why the project was terminated: “I would have found it very difficult to justify, if we were to come up with a pilot … that showed the algorithm contained enormous bias,” he said. Viewed in a certain light, the city had tested out an innovative approach to identifying fraud in a way designed to minimize risks, found that it had not lived up to its promise, and scrapped it before the consequences for real people had a chance to multiply. But for IJmker and some of her city council colleagues focused on social welfare, there was also the question of opportunity cost. But those working in the field say that ambition comes at their expense. City council members were never told exactly how much the effort cost, but in response to questions from MIT Technology Review, Lighthouse, and Trouw on this topic, the city estimated that it had spent some €500,000, plus €35,000 for the contract with Deloitte—but cautioned that the total amount put into the project was only an estimate, given that Smart Check was developed in house by various existing teams and staff members. “But we have always said that [it was discriminatory].” When we spoke to Bodaar in March, a year and a half after the end of the pilot, he was candid in his reflections. “Perhaps it was unfortunate to immediately use one of the most complicated systems,” he said, “and perhaps it is also simply the case that it is not yet … the time to use artificial intelligence for this goal.” In city council meetings she has brought up Smart Check as an example of what not to do. While she was glad that city employees had been thoughtful in their “many protocols,” she worried that the process obscured some of the larger questions of “philosophical” and “political values” that the city had yet to weigh in on as a matter of policy. Questions such as “How do we actually look at profiling?” or “What do we think is justified?”—or even “What is bias?” These questions are, “where politics comes in, or ethics,” she says, “and that's something you cannot put into a checkbox.” “I think a lot of people were just like, ‘Okay, well, we did this. It feels like “a waste,” she adds, “because people worked on this for years.” “Why do we hold AI systems to a higher standard than human agents?” he asks. “There's a high-level thing of Do not discriminate, which I think we can all agree on, but this example highlights some of the complexities of how you translate that [principle].” Ultimately, Chen believes that finding any solution will require trial and error, which by definition usually involves mistakes: “You have to pay that cost.” “Such systems only work when people buy into them,” explains Elissa Redmiles, an assistant professor of computer science at Georgetown University who has studied algorithmic fairness. No matter what the process looks like, these are questions that every government will have to deal with—and urgently—in a future increasingly defined by AI. And, as de Zwart argues, if broader questions are not tackled, even well-intentioned officials deploying systems like Smart Check in cities like Amsterdam will be condemned to learn—or ignore—the same lessons over and over. “We are being seduced by technological solutions for the wrong problems,” he says. Why doesn't the municipality build an algorithm that searches for people who do not apply for social assistance but are entitled to it?” Eileen Guo is the senior reporter for features and investigations at MIT Technology Review. Gabriel Geiger is an investigative reporter at Lighthouse Reports. Justin-Casimir Braun is a data reporter at Lighthouse Reports. You can read a detailed explanation of our technical methodology here. You can read Trouw's companion story, in Dutch, here. Adoption of the tech has civil liberties advocates alarmed, especially as the government vows to expand surveillance of protesters and students. AlphaEvolve uses large language models to find new algorithms that outperform the best human-made solutions for data center management, chip design, and more. New diffusion AI models that make songs from scratch are complicating our definitions of authorship and human creativity. Discover special offers, top stories, upcoming events, and more. Try refreshing this page and updating them one more time.</p>
                <br/>
                

        </div>

        <script>
            // Get all article attribution elements
            const articleAttributions = document.querySelectorAll('#article_attribution');

            // Add an event listener to each attribution element
            articleAttributions.forEach(attribution => {
            attribution.addEventListener('click', () => {
                // Get the next paragraph element (the article text)
                const articleText = attribution.nextElementSibling;

                // Toggle the visibility of the article text
                articleText.classList.toggle('hidden');

                // Toggle the expand icon
                const expandIcon = attribution.querySelector('.expand-icon');
                expandIcon.classList.toggle('fa-chevron-down');
                expandIcon.classList.toggle('fa-chevron-up');
            });
            });    
        </script>

        <footer class="text-center text-sm text-gray-500 mt-12">
            <div class="inline-block align-middle">
                <a href="https://www.youtube.com/@news_n_clues" target="_blank" rel="noopener noreferrer"
                    class="flex items-center gap-2 text-red-600 hover:text-red-700 font-semibold transition duration-300 ease-in-out">
                    Watch Daily <span class="italic">News'n'Clues</span> Podcast on
                    <img src="../images/yt.png" width="16" height="16">
                </a>
            </div>
            <div>
                <a href="mailto:newsnclues@gmail.com?subject=News'n'Clues Aggregator Inquiry">SoftMillennium
                    <script>document.write(new Date().getFullYear());</script>
                </a>
                <!--
            <b>Copyright &copy; <script>document.write(new Date().getFullYear());</script> - <a href='mailto:newsnclues@gmail.com?subject=News Aggregator Inquiry'>News And Clues</a></b>
            -->
            </div>
        </footer>
    </body>
</html>
            